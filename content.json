{"posts":[{"title":"MIT 6.824 Lab2, part1小结","text":"关于MIT 6.824课程的Lab 2, part 1从昨晚在宾馆到今天飞机上，都在过这几个test case，终于最后有点讨巧的过了。 View service是MIT介绍paxos之前用来入门的，它的原理是以时间先后的顺序确定主备服务器，由定期的心跳更新此关系。之所以放在单独的service里获取主备关系，是为了将主备关系从K-V系统中解耦，从而K-V系统不用关心键值操作之外的事情，让更加上层的系统进行处理。 遇到的问题： view service当前的view，和下一个未确认的view在哪些时间点更新？ 怎么认为这个服务器作为主，又怎么确认其为Backup，Idle？ 什么时候能将这些认识更新到服务器当前view上？ 服务器看到的view service的当前view是怎样的？ 不同服务器查询当前view状态，获得的结果是一样的吗？ 我用复杂难看的if-else语句最后实现这些逻辑，但不可否认，实现的漏洞很多，我理想的解决方案还是使用状态机，可以更加清晰容易更新（用图表展现）。","link":"/programming/6.824,%20Lab2,%20part%201/"},{"title":"为PHP7添加新的语法特性","text":"好文章！nikic介绍了如何向PHP添加新的语法特性，原文写的非常精彩，具体是添加in语法功能，使最终实现： 12345678$words = ['hello', 'world', 'foo', 'bar'];var_dump('hello' in $words); // truevar_dump('foo' in $words); // truevar_dump('blub' in $words); // false$string = 'PHP is fun!';var_dump('PHP' in $string); // truevar_dump('Python' in $string); // false 我进行了一下实践，根据PHP7进行了些修改，具体记录下自己的实践过程和心得。 环境准备从Github上下载PHP7的源码，准备好PHP的开发编译环境，我是基于ubuntu，所以运行如下： 12345678910111213141516$ apt-get -y install build-essential autoconf bison re2c下一步，编译PHP7$ cd php-src# create new branch for in operator$ git checkout -b addInOperator# build ./configure script$ ./buildconf# configure PHP in debug mode and with thread safety$ ./configure --disable-all --enable-debug --enable-maintainer-zts# compile (4 is the number of cores you have)$ make -j4#PHP的二进制就是`sapi/cli/php`，可以运行以下命令测试是否编译成功：$ ./sapi/cli/php -v$ ./sapi/cli/php -r 'echo &quot;Hallo World!&quot;;' PHP脚本的生命历程PHP脚本运行经过3个主要阶段： 词法分析 语法分析&amp;编译 执行 词法分析这个阶段是将源码根据规则分解成称为token更小的单元，为后面的语法分析提供材料。修改位于Zend/目录的zend_language_scanner.l，这个文件定义了词法记号规则，为了让PHP能识别in作为关键词。 123&lt;ST_IN_SCRIPTING&gt;&quot;in&quot; { RETURN_TOKEN(T_IN);} 为了让Zend引擎能识别T_IN这个记号，在zend_language_parser.y中加入： 1%token T_IN &quot;in (T_IN)&quot; 重新生成tokenizer系列源文件 12$ cd ext/tokenizer$ ./tokenizer_data_gen.sh 语法分析&amp;编译向语法分析增加in所应用的表达式规则，在zend_language_parser.y加入： 1234expr_without_variable:...| expr T_IN expr { $$ = zend_ast_create_binary_op(ZEND_IN, $1, $3); }... 再设置in的操作符优先级，在语法分析文件中找到以下行，并在行尾加入T_IN： 1%nonassoc '&lt;' T_IS_SMALLER_OR_EQUAL '&gt;' T_IS_GREATER_OR_EQUAL 执行最后在zend_vm_def.h中添加具体的执行逻辑： 1234567891011121314151617ZEND_VM_HANDLER(173, ZEND_IN, CONST|TMPVAR|CV, CONST|TMPVAR|CV){ USE_OPLINE zend_free_op free_op1, free_op2; zval *op1, *op2; SAVE_OPLINE(); op1 = GET_OP1_ZVAL_PTR_UNDEF(BP_VAR_R); op2 = GET_OP2_ZVAL_PTR_UNDEF(BP_VAR_R); // TODO FREE_OP1(); FREE_OP2(); CHECK_EXCEPTION(); ZEND_VM_NEXT_OPCODE();} 在PHP源码根目录运行： 12$ ./sapi/cli/php ./Zend/zend_vm_gen.php$ make -j4 zend_vm_gen.php是根据zend_vm_def.h中的定义生成zend_vm_*.*系列的文件。 之后测试编译后的PHP： 1$ ./sapi/cli/php -r 'var_dump(&quot;s&quot; in &quot;str&quot;);' 会出现Segmentation fault，原因是我之前在zend_language_parser.y中加入的操作是zend_ast_create_binary_op，所以在编译时会调用位于zend_opcode.c的get_binary_op方法，这个方法返回一个函数指针，用于处理expr in expr这个操作两个参数的语句。 12binary_op_type op = get_binary_op(ast-&gt;attr);ret = op(result, &amp;op1, &amp;op2); 所以之后我在3处添加代码： zend_opcode.c中添加查询ZEND_IN操作函数的代码 12345678ZEND_API binary_op_type get_binary_op(int opcode){ switch (opcode) { ... case ZEND_IN: return (binary_op_type) in_function; ...} zend_operators.c中添加处理函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344ZEND_API int ZEND_FASTCALL in_function(zval *result, zval *op1, zval *op2){ zval op1_copy, op2_copy; int use_copy1 = 0, use_copy2 = 0; switch (Z_TYPE_P(op2)) { case IS_STRING: use_copy1 = zend_make_printable_zval(op1, &amp;op1_copy); if (use_copy1) { op1 = &amp;op1_copy; } if (Z_STRLEN_P(op1) == 0) { ZVAL_TRUE(result); } else { const char *found = zend_memnstr_ex( Z_STRVAL_P(op2), Z_STRVAL_P(op1), Z_STRLEN_P(op1), Z_STRVAL_P(op2) + Z_STRLEN_P(op2) ); ZVAL_BOOL(result, found != NULL); } break; case IS_ARRAY: use_copy1 = zend_make_printable_zval(op1, &amp;op1_copy); if (use_copy1) { op1 = &amp;op1_copy; } ZVAL_BOOL(result, zend_hash_exists(Z_ARR_P(op2), Z_STR_P(op1))); break; default: zend_error(E_EXCEPTION | E_ERROR, &quot;Unsupported operand types&quot;); ZVAL_FALSE(result); return FAILURE; } if (use_copy1) { zval_dtor(&amp;op1_copy); } return SUCCESS;} zend_vm_def.h中补上TODO该做的处理逻辑 12345678910111213141516171819// 我在系统里的最新操作号是173，根据实际为准ZEND_VM_HANDLER(173, ZEND_IN, CONST|TMPVAR|CV, CONST|TMPVAR|CV){ USE_OPLINE zend_free_op free_op1, free_op2; zval *op1, *op2; SAVE_OPLINE(); op1 = GET_OP1_ZVAL_PTR_UNDEF(BP_VAR_R); op2 = GET_OP2_ZVAL_PTR_UNDEF(BP_VAR_R); in_function(EX_VAR(opline-&gt;result.var), op1, op2); ZVAL_TRUE(EX_VAR(opline-&gt;result.var)); FREE_OP1(); FREE_OP2(); CHECK_EXCEPTION(); ZEND_VM_NEXT_OPCODE();} 依旧是在PHP源码根目录下运行： 12$ ./sapi/cli/php ./Zend/zend_vm_gen.php$ make -j4 现在再测试生成的php，应该会满足： 12345678910$ ./sapi/cli/php -r 'var_dump(&quot;s&quot; in &quot;str&quot;);'bool(true)$ ./sapi/cli/php -r 'var_dump(&quot;s&quot; in &quot;xxx&quot;);'bool(false)$ ./sapi/cli/php -r 'var_dump(&quot;a&quot; in [&quot;a&quot;=&gt;1]);'bool(true)$ ./sapi/cli/php -r 'var_dump(&quot;a&quot; in [&quot;b&quot;=&gt;1]);'bool(false)$ ./sapi/cli/php -r 'var_dump(&quot;s&quot; in 123);'Fatal error: Unsupported operand types in Command line code on line 1 小结实际进行了一番操作之后，对于PHP的运行机制会落实到更小的单位，从解释阶段到文件级别。PHP7版本引入了ast这个中间结构，比起nikic那个版本来说，需要修改更多文件，下一步要剖析一下ast在PHP7中的作用和在执行过程中的角色。","link":"/programming/add-syntactic-feature-to-php7/"},{"title":"Ansible常用命令","text":"动机记录一下常用的Ansible指令、模块，方便检索。 Ansible主机/etc/ansible/hosts中，server是目标服务器列表名，包含两个服务器 123[spark]10.0.0.110.0.0.2 Ansible Shell模块12# 在spark集群上执行ls指令$ ansible spark -m shell -a 'ls' Ansible-playbook执行较大的复杂任务时，以YAML的声明语法来配置，并且可以放置一些模板类文件和资源文件等。 123456789101112131415161718.|___python.yml|___python.host|___roles| |___python| | |___defaults # 默认变量| | |___handlers # 可以被任务使用或者任何该任务之外的地方| | |___files| | |___vars # 其它变量| | |___templates # 模板| | |___meta # 元数据| | |___tasks # 主要任务列表| | | |___main.yml| |___airflow| | |___tasks| | | |___main.yml| | | |___templates| | | | |___airflow.cfg 变量12345---# 声明变量foo: field1: one field2: two 1234# 使用base_path- hosts: app_servers vars: app_path: { { base_path } }/22 12345# 在命令行传入变量$ ansible-playbook release.yml --extra-vars &quot;hosts=vipers user=starbuck&quot;# 文件形式$ ansible-playbook release.yml --extra-vars &quot;@some_file.json&quot; 更多的变量还有3类作用域等，以后用到再加。 文件123# 将files目录下的conf/发送到目标节点- name: copy conf files copy: src=conf/agent.conf dest=/opt/flume/apache-flume-1.7.0-bin/conf/ handler12345# handlers/main.yml---- name: start telegraf service: name=telegraf state=started 12345# 拷贝docker配置文件并触发docker重启- name: copy docker conf to dest host copy: src=conf/docker.conf dest=/etc/telegraf/telegraf.d/ when: &quot;'docker' in group_names&quot; notify: restart telegraf 任务roles目录就是按具体安装的功能模块划分，比如python模块、jdk模块、spark模块等等，他们相互不重复，并且可以有依赖关系，比如jdk -&gt; spark，通过多个role的组合搭配出各种环境的配置方法。 12345678910111213# python/tasks/main.yml- name: Install pip apt: name: python-pip state: present become: false- name: Upgrade pip pip: name: pip extra_args: --upgrade -i https://pypi.doubanio.com/simple/ become: false 12345678910111213141516171819202122232425262728293031323334353637383940# airflow/tasks/main.yml- name: Create virtualenv for airflow command: virtualenv /home/ubuntu/airflow_v creates=/home/ubuntu/airflow_v become: false- name: Install mysqlclient apt: name: libmysqlclient-dev state: present become: true- name: Install airflow within `airflow_v` pip: name: apache-airflow[all] extra_args: -i https://pypi.doubanio.com/simple/ virtualenv: /home/ubuntu/airflow_v- name: Install celery[redis] within `airflow_v` pip: name: celery[redis] extra_args: -i https://pypi.doubanio.com/simple/ virtualenv: /home/ubuntu/airflow_v - name: Create Airflow home file: path: /home/ubuntu/airflow state: directory mode: 0755- name: Initialize airflow database command: source /home/ubuntu/airflow_v/bin/activate &amp;&amp; /home/ubuntu/airflow_v/bin/airflow initdb creates=/home/ubuntu/airflow/airflow.cfg environment: AIRFLOW_HOME: /home/ubuntu/airflow - name: Syncronize airflow configuration synchronize: src: templates/airflow.cfg dest: /home/ubuntu/airflow/airflow.cfg become: false 入口文件python.yml是ansible入口文件，包含目标host、运行的具体任务。 123456---# 发布机上的代码初始化- hosts: spark roles: - python - airflow 下面是任务中常用的模块 apt安装模块123456# 安装pip- name: Install pip apt: name: python-pip state: present become: false Pip安装模块12345678910111213# 使用pip升级pip- name: Upgrade pip pip: name: pip extra_args: --upgrade -i https://pypi.doubanio.com/simple/ become: false # 使用指定虚拟环境安装依赖- name: Install celery[redis] within `airflow_v` pip: name: celery[redis] extra_args: -i https://pypi.doubanio.com/simple/ virtualenv: /home/ubuntu/airflow_v Shell模块1234- name: Initialize airflow database command: source /home/ubuntu/airflow_v/bin/activate &amp;&amp; /home/ubuntu/airflow_v/bin/airflow initdb creates=/home/ubuntu/airflow/airflow.cfg environment: AIRFLOW_HOME: /home/ubuntu/airflow File模块123456# 在目标服务器上创建权限为0755的目录- name: Create Airflow home file: path: /home/ubuntu/airflow state: directory mode: 0755 Synchronize模块123456# 将本地文件同步到目标服务器上- name: Syncronize airflow configuration synchronize: src: templates/airflow.cfg dest: /home/ubuntu/airflow/airflow.cfg become: false 下载模块123456# 从指定URL下载文件到目标服务器指定目录- name: Download the Go tarball get_url: url: &quot;{{ go_download_location }}&quot; dest: /usr/local/src/{{ go_tarball }} checksum: &quot;{{ go_tarball_checksum }}&quot; Systemd模块1234567# 重启docker服务- name: restart docker systemd: state: restarted daemon_reload: yes name: docker become: true Template模块12345# 将templates目录下的docker.my模板复制到目标服务器的/etc/default/docker目录- name: Configure docker mirror registry to Aliyun template: src=docker.my dest=/etc/default/docker notify: restart docker become: true Blockinfile模块1234567891011# 一个官网例子，在/etc/hosts里添加映射- name: Add mappings to /etc/hosts blockinfile: path: /etc/hosts block: | {{ item.ip }} {{ item.name }} marker: &quot;# {mark} ANSIBLE MANAGED BLOCK {{ item.name }}&quot; with_items: - { name: host1, ip: 10.10.1.10 } - { name: host2, ip: 10.10.1.11 } - { name: host3, ip: 10.10.1.12 } Copy模块123# 拷贝zookeeper配置文件- name: copy zookeeper conf to dest host copy: src=conf/zookeeper.conf dest=/etc/zookeeper/conf/ Service模块12- name: restart telegraf service: name=telegraf state=restarted 引用 http://docs.ansible.com/ansible/latest/","link":"/operation/ansible-frequently-used-commands/"},{"title":"基于gitlab搭建CI","text":"自建gitlab自建gitlab一大优势是快，之前用翻墙走github速度也在1M/s左右，可是自建的gitlab提升了建立连接的速度，可以说体验又上升了一个档次。 用docker起一个gitlab服务尤其方便 GitLab Docker images 选择CI如果有人像我一样对Jenkins的UI和配置不感兴趣，那么Gitlab CI runner是不错的选择，配置用YAML，清晰明了，UI集成在gitlab中。对比jenkins，上手容易，颜值更高。 Gitlab CI runner 找一台ubuntu 16.04，机器配置按照项目大小和数量决定 安装CI runner1$ sudo apt-get install gitlab-ci-multi-runner 将runner注册到gitlab1$ sudo gitlab-ci-multi-runner register 调整配置1234567891011121314concurrent = 16 # 更新并发build数量到16[[runners]] name = &quot;10-1-10-10&quot; # 这里是runner的名字 url = &quot;https://gitlab.com&quot; # 所属gitlab的url token = &quot;please-input-gitlab-register-token&quot; # gitlab注册token executor = &quot;docker&quot; # 以docker形式运行每次CI任务 [runners.docker] tls_verify = false image = &quot;ruby:2.1&quot; # docker运行的默认镜像 privileged = false # 不授权 volumes = [&quot;/cache&quot;, &quot;/var/run/docker.sock:/var/run/docker.sock&quot;] extra_hosts = [&quot;gitlab.com:some-internal-ip&quot;] [runners.cache] 调整concurrent，最大限度利用机器若镜像中使用docker push等命令，将宿主机docker.sock挂载到容器中若gitlab与runner在同网络，设置docker容器内的gitlab host为内网IP，最小化网络延迟 Pipelinepipeline指CI的流程可以当做管道，只有上一个操作成功之后，下一个操作继续执行，如利用gitlab ci构建build-&gt;test-&gt;release-&gt;deployment的流程，当代码push到master时，CI触发，如下 配置CI.gitlab-ci.yml是gitlab项目的CI配置文件，文件形式是yaml。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687image: ruby:2.1stages:- test- build- release- deploymentvariables: DOCKER_DRIVER: overlay SERVICE_NAME: test CONTAINER_TEST_IMAGE: $IMAGE_NAME:$CI_BUILD_REF_NAME CONTAINER_RELEASE_IMAGE: $IMAGE_NAME:latesttest:1.8.0: image: golang:1.8 stage: test before_script: - echo &quot;before test&quot; script: - echo &quot;test&quot;build:test: image: golang:1.8 stage: build before_script: - echo &quot;before script&quot; script: - mkdir -p binaries/ - touch binaries/test artifacts: paths: - binaries/ expire_in: 1h only: - testbuild:prod: image: golang:1.8 stage: build before_script: - echo &quot;before build&quot; script: - mkdir -p binaries/ - touch binaries/prod artifacts: paths: - binaries/ expire_in: 1h only: - masterrelease:test: stage: release script: - echo &quot;release $CONTAINER_TEST_IMAGE&quot; only: - test dependencies: - build:testrelease:prod: stage: release script: - echo &quot;release $CONTAINER_RELEASE_IMAGE&quot; only: - master dependencies: - build:proddeployment:test: stage: deployment image: kroniak/ssh-client before_script: - echo &quot;before script&quot; script: - echo &quot;deployment test&quot; only: - testdeployment:stage: stage: deployment image: kroniak/ssh-client before_script: - echo &quot;before script&quot; only: - master script: - echo &quot;deployment stage&quot; stage声明管道有几个阶段，stage的顺序代表着前置stage完成后才能进入下一个stage，每个stage有1个或多个可执行点 每个可执行操作里有一些配置 stage当前执行所属阶段，就是文件开头声明的stages列表中一个 image当前命令执行的基础镜像 before_script脚本执行前初始化命令 only限制执行的分支，例如只有test分支能触发deployment:test script(required)执行的脚本 artifacts这次执行将产生的需要传递给下一个stage的文件 dependencies当前执行依赖的前置执行，不仅是声明一种依赖关系，同时上个执行生成的artifacts也会传递过来 在gitlab项目中设置CI/CD Pipelines的variables，可以为docker容器注入隐秘变量，如docker仓库秘钥，这些变量可以在CI运行脚本时被解析。 小结Gitlab CI能自定义阶段和在容器里执行脚本，并且水平拓容便捷，和Gitlab的集成好。","link":"/operation/build-ci-with-gitlab/"},{"title":"Setup OAuth2 client for Django in 5 minutes","text":"This article explains how to setup OAuth2 client for Django in 5 minutes, it’s used for Web service which requires user to login by OAuth2, especially for those who are familiar with OAuth2.0 but unfamiliar with Django. If you have no idea about OAuth2.0 workflow, please visit OAuth2 net The example here introduces Web service implements the OAuth2 workflow, the user must login first then he can see the web content. The Web Framework is Python Django, the OAuth library we use is Authlib==0.14.1. It supports user session persistence and auto-refresh access_token. OAuth configurationIf we consider using Github as the authorization server, first you need to register a new OAuth application in Github, then you’ll have you credentials.Secondly, setup OAuth settings in settings.py. 123456789101112131415161718# OAuth SettingsOAUTH_URL_WHITELISTS = []OAUTH_CLIENT_NAME = 'github'OAUTH_CLIENT = { 'client_id': '', 'client_secret': '', 'access_token_url': 'https://github.com/login/oauth/access_token', 'authorize_url': 'https://github.com/login/oauth/authorize', 'api_base_url': 'https://api.github.com/', 'redirect_uri': 'https://songrgg.com/oauth/callback', 'client_kwargs': { 'scope': 'profile email', 'token_placement': 'header' }, 'userinfo_endpoint': 'user',} Pay attention, the redirect uri should be the one your server will use to fetch access token and setup user session, I setup /oauth/callback here, the logic will be introduced in middleware. MiddlewareFor each user request, the server will check user’s session and see if user has logined, in both Django or other Web frameworks, we could setup middleware to handle user requests. Initialize the OAuth clientUse the OAuth configuration to initialize the OAuth client. 1234567def update_token(token, refresh_token, access_token): request.session['token'] = token return Nonesso_client = self.oauth.register( settings.OAUTH_CLIENT_NAME, overwrite=True, **settings.OAUTH_CLIENT, update_token=update_token) update_token parameter is used to refresh the access_token when it’s expired. Process OAuth callbackAfter the user authorizes the login, our server should fetch the access_token from the authorization server and store it in user session. 12345678if request.path.startswith('/oauth/callback'): self.clear_session(request) request.session['token'] = sso_client.authorize_access_token(request) if self.get_current_user(sso_client, request) is not None: redirect_uri = request.session.pop('redirect_uri', None) if redirect_uri is not None: return redirect(redirect_uri) return redirect(views.index) Fetch user infoAfter the access_token is ready, fetch the user info from the resource API, otherwise redirect user to the authorization page. 1234567891011121314151617@staticmethoddef get_current_user(sso_client, request): token = request.session.get('token', None) if token is None or 'access_token' not in token: return None if not OAuth2Token.from_dict(token).is_expired() and 'user' in request.session: return request.session['user'] try: res = sso_client.get(settings.OAUTH_CLIENT['userinfo_endpoint'], token=OAuth2Token(token)) if res.ok: request.session['user'] = res.json() return res.json() except OAuthError as e: print(e) return None Put the middlewareIn settings.py, put the middleware class to the array. 123MIDDLEWARE = [ 'oauth_demo.middleware.oauth.OAuthMiddleware'] SessionWe use Django session module and the default storage is sqlite3, you can simply change it to other backends like redis by modifying settings.py. ConclusionThe full repository is located at https://github.com/songrgg/oauth-demo.","link":"/programming/django-oauth-client-setup/"},{"title":"Anatomy of envoy proxy: the architecture of envoy and how it works","text":"Envoy has become more and more popular, the basic functionality is quite similar to Nginx, working as a high performace Web server, proxy. But Enovy imported a lot of features that was related to SOA or Microservice like Service Discovery, Circuit Breaker, Rate limiting and so on. A lot of developers know the roles envoy plays, and the basic functionality it will implement, but don’t know how it organize the architecture and how we understand its configuration well. For me, it’s not easy to understand envoy’s architecture and its configuration since it has a lot of terminology, but if the developer knew how the user traffic goes, he could understand the design of envoy. Envoy In ServicemeshRecently, more and more companies take Service Mesh to solve the communication problem among backend services, it’s a typical use case for envoy to work as a basic component for building a service mesh, envoy plays an important role and one of the service mesh solution Istio uses Envoy as the core of the networking. As pictured, the Envoy is deployed beside every application, this kind of application we call it Sidecar. Let’s analyze how the user traffic moves. The user hits the website and the browser tries to query an API, the api gateway receives the user request. The API Gateway redirects the request to the backend server1 (in Kubernetes, it can be a Pod) The envoy on backend server receives this HTTP request and resolves it to the destination server and forwards the request to the local destination port which APP1 listens at. The APP1 receives requests, processes the bussiness logic and tries to call a dependent RPC service in APP2, the request first is sent to local envoy. The local envoy resolves the RPC service APP2’s IP address and port according to the management server and sends the RPC request to APP2 server. The server where the RPC service located at recevies the request, to be clear, it’s the envoy receiving the request, after the same logic like step 3. The APP2 processes the request and returns. The envoy forwards the response to server1. There’re two forwards being ignored, envoy(1) to APP1, APP1 to envoy(1). Then the envoy(1) returns the reponse to API gateway. The API gateway returns to the user. The management server is responsible for telling envoy how to process the requests and where to forward.Service discovery is where applications register themselves. Ingress and EgressAs you can see, there’re two kinds of traffic within a server: ingress and egress. Any traffic sent to server, it’s ingress. Any traffic sent from server, it’s egress. How to implement this transparently? Setup IPtables to redirect any traffic to this server to the envoy service first, then envoy redirects the traffic to the real application on this server. Setup IPtables to redirect any traffic from this server to the envoy service first and envoy resolves the destination service using Service Discovery, redirects the request to the destination server. By intercepting the inbound and outbound traffic, envoy can implement the service discovery, circuit breaker, rate limiting, monitoring transparently, the developers don’t need to care about the details or integrate libraries. Anatomy of envoy proxy configurationThe first important role of envoy in the service mesh is proxy, it receives requests and forwards requests.To see the components that make the proxy work, we can start with a request flow. A request reaches a port on the server which envoy listens at, we call this part listener. Envoy receives the request and tries to process this request according to some rule, the rule is route. The route processes the request based on the request’s metadata and tries to request the specific backend servers, the backend servers are called cluster. The concrete server IP:port behind cluster is called endpoint. It’s the main part of envoy components in most of the proxy cases, similar to Nginx, you’re allowed to setup all the configuration by static files. Here is an example with static configuration, you can see the full file here. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960static_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 80 filter_chains: - filters: - name: envoy.http_connection_manager typed_config: &quot;@type&quot;: type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager codec_type: auto stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: backend domains: - &quot;*&quot; routes: - match: prefix: &quot;/service/1&quot; route: cluster: service1 - match: prefix: &quot;/service/2&quot; route: cluster: service2 http_filters: - name: envoy.router typed_config: {} clusters: - name: service1 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} load_assignment: cluster_name: service1 endpoints: - lb_endpoints: - endpoint: address: socket_address: address: service1 port_value: 80 - name: service2 connect_timeout: 0.25s type: strict_dns lb_policy: round_robin http2_protocol_options: {} load_assignment: cluster_name: service2 endpoints: - lb_endpoints: - endpoint: address: socket_address: address: service2 port_value: 80 Don’t be nervous, you can understand this configuration easily by order: listenerIt says the envoy listens at 0.0.0.0:80 routeIt tells how to process request, there’s a route named local_route, a rule matching wildcard domain and forwards the request to service1 cluster if the request path’s prefix matches /service/1, else forwards to service2 cluster if the request path’s prefix matches service/2. clusterFinally, the service1 cluster memtioned before is resolved to several endpoints, it’s the real address of the server, the address is service1:80. So the basic structure of the configuration is quite straightforward and easy to understand, if you want to manipulate the configuration and don’t know where to start, you can refer to this sample and every data model inside can be found in envoy documentation. But most importantly, it’s allowed to use dynamic configuration which is mostly used in SOA and Microservice, it fits the situations where the service’s endpoints and route rules may change anytime. For using dynamic resources, envoy supports setting an API server and divides the above components into different APIs or different resources within an API. LDS: listenerThe listener discovery service (LDS) is an optional API that Envoy will call to dynamically fetch listeners. Envoy will reconcile the API response and add, modify, or remove known listeners depending on what is required. RDS: routeThe route discovery service (RDS) API is an optional API that Envoy will call to dynamically fetch route configurations. A route configuration includes both HTTP header modifications, virtual hosts, and the individual route entries contained within each virtual host. CDS: clusterThe cluster discovery service (CDS) is an optional API that Envoy will call to dynamically fetch cluster manager members. Envoy will reconcile the API response and add, modify, or remove known clusters depending on what is required. EDS: endpointThe endpoint discovery service is a xDS management server based on gRPC or REST-JSON API server used by Envoy to fetch cluster members. The cluster members are called “endpoint” in Envoy terminology. For each cluster, Envoy fetch the endpoints from the discovery service. The concept is equal to the static configuration, you can initialize the configuration by 123456789101112131415161718192021222324252627282930313233343536373839admin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 127.0.0.1, port_value: 9901 }dynamic_resources: lds_config: api_config_source: api_type: GRPC grpc_services: envoy_grpc: cluster_name: xds_cluster cds_config: api_config_source: api_type: GRPC grpc_services: envoy_grpc: cluster_name: xds_clusterstatic_resources: clusters: - name: xds_cluster connect_timeout: 0.25s type: STATIC lb_policy: ROUND_ROBIN http2_protocol_options: {} upstream_connection_options: # configure a TCP keep-alive to detect and reconnect to the admin # server in the event of a TCP socket half open connection tcp_keepalive: {} load_assignment: cluster_name: xds_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: 127.0.0.1 port_value: 5678 The only static resource is the xds_cluster which is the management server cluster which provides a GRPC streaming API answering the LDS, CDS, EDS, RDS configuration.As you notice, there’re only lds_config and cds_config inside the config file, that’s because rds is included in lds and eds is included in cds. In Service Mesh architecture, the management server is the most important module, it always connects to a distributed service discovery system which can be Etcd, Zookeeper, Consul, Eureka or the Kubernetes(Kubernetes often used Etcd as their service discovery component), and provide some interfaces to manipulate the configuration to implement near-realtime configuration change. ConclustionUnderstanding how the user traffic flows makes it easier for me to understand the component design of envoy and have a first glimpse of the envoy configuration, then you can start with more features later. Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesImplement zero downtime HTTP service rollout on Kubernetes","link":"/architecture/deeper-understanding-to-envoy/"},{"title":"etcd生产环境实践","text":"生产环境搭建etcd以搭建3节点高可用ETCD集群为例，分别在三台主机上初始化ETCD1,ETCD2,ETCD3作为机器IP地址。 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ ETCD1=http://10.0.0.1$ ETCD2=http://10.0.0.2$ ETCD2=http://10.0.0.3# 节点1为4核4GB的机器$ etcd --name etcd1 \\--initial-advertise-peer-urls $ETCD1:2380 \\--listen-peer-urls $ETCD1:2380 \\--listen-client-urls $ETCD1:2379,http://127.0.0.1:2379 \\--advertise-client-urls $ETCD1:2379 \\--initial-cluster-token etcd-cluster \\--initial-cluster etcd1=$ETCD1:2380,etcd2=$ETCD2:2380,etcd3=$ETCD3:2380 \\--auto-compaction-retention=1 \\--quota-backend-bytes=$((4*1024*1024*1024)) \\--initial-cluster-state new# 节点2为4核4GB的机器$ etcd --name etcd2 \\--initial-advertise-peer-urls $ETCD2:2380 \\--listen-peer-urls $ETCD2:2380 \\--listen-client-urls $ETCD2:2379,http://127.0.0.1:2379 \\--advertise-client-urls $ETCD2:2379 \\--initial-cluster-token etcd-cluster \\--initial-cluster etcd1=$ETCD1:2380,etcd2=$ETCD2:2380,etcd3=$ETCD3:2380 \\--auto-compaction-retention=1 \\--quota-backend-bytes=$((4*1024*1024*1024)) \\--initial-cluster-state new# 节点3为4核4GB的机器$ etcd --name etcd3 \\--initial-advertise-peer-urls $ETCD3:2380 \\--listen-peer-urls $ETCD3:2380 \\--listen-client-urls $ETCD3:2379,http://127.0.0.1:2379 \\--advertise-client-urls $ETCD3:2379 \\--initial-cluster-token etcd-cluster \\--initial-cluster etcd1=$ETCD1:2380,etcd2=$ETCD2:2380,etcd3=$ETCD3:2380 \\--auto-compaction-retention=1 \\--quota-backend-bytes=$((4*1024*1024*1024)) \\--initial-cluster-state new# 检查集群是否正确启动$ etcdctl --endpoints=$ETCD1:2379 member list400938f1eaf1d0ed: name=etcd3 peerURLs=http://10.0.0.1:2380 clientURLs=http://10.0.0.1:2379 isLeader=falsebd0ff97b33ac1165: name=etcd2 peerURLs=http://10.0.0.2:2380 clientURLs=http://10.0.0.2:2379 isLeader=falsebe62429afec4445f: name=etcd1 peerURLs=http://10.0.0.3:2380 clientURLs=http://10.0.0.3:2379 isLeader=true 历史记录压缩如果将etcd用作服务发现，每次服务注册和更新都可以看做一条新数据，日积月累，这些数据的量会导致etcd占用内存越来越大，直到etcd到达空间配额限制的时候，etcd的写入将会被静止，影响线上服务，定期删除历史记录就是避免这种情况。 12# 只保留一个小时的历史数据$ etcd --auto-compaction-retention=1 磁盘去碎片化etcd官方是说在进行compaction操作之后，旧的revision被压缩，会产生内部的碎片，内部碎片是指空闲状态的，能被后端使用但是仍然消耗存储空间的磁盘空间。去碎片化实际上是将存储空间还给文件系统。 1$ etcdctl defrag 空间配额空间配额用来保障集群可靠地进行操作。如果没有限制配额，当键空间变大之后，直到用光了磁盘空间，它就会影响etcd集群的表现。当任意节点超出空间配额， 那么它将进入维护状态，只接受读/删操作。只有释放了足够空间、去碎片化了后端数据库并且清理了空间配额之后，集群才能继续正常操作。 默认限制是2GB，可以通过--quota-backend-bytes配置，最高上限为8GB。 123# 显式配置配额为16MB$ etcd --quota-backend-bytes=$((16*1024*1024))$ ETCDCTL_API=3 etcdctl --write-out=table endpoint status 如果遇到空间配额不足的情况，那么需要对etcd进行操作。 12345678910# 获取当前版本号$ rev=$(ETCDCTL_API=3 etcdctl --endpoints=:2379 endpoint status --write-out=&quot;json&quot; | egrep -o '&quot;revision&quot;:[0-9]*' | egrep -o '[0-9]*'）# 压缩所有旧版本$ ETCDCTL_API=3 etcdctl compact $rev# 去碎片化$ ETCDCTL_API=3 etcdctl defrag# 取消警报$ ETCDCTL_API=3 etcdctl alarm disarm# 测试通过$ ETCDCTL_API=3 etcdctl put newkey 123 快照备份1234567$ etcdctl snapshot save backup.db$ etcdctl --write-out=table snapshot status backup.db+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| fe01cf57 | 10 | 7 | 2.1 MB |+----------+----------+------------+------------+ 调优跨数据中心时，需要调整心跳间隔和选举超时时间默认的心跳时间是100ms，建议为round trip时间默认选举超时是1000ms 参考 详细配置 etcd运维 Recommended Posts(Driven byHexo Recommended Posts plugin)Istio Version Control On Kubernetes微服务实践四: 配置管理","link":"/operation/etcd-for-production/"},{"title":"Kubernetes DNS拓展","text":"Kubernetes DNS在内部服务与外部服务交互，内部服务与内部服务，内部服务与云托管服务交互的工具，拓展DNS可以在内部服务访问集群外服务时像访问集群内服务一样，通过DNS映射将统一风格的域名映射到可访问的IP，而不需要影响内部服务的运行，这里介绍如何使用Consul来拓展DNS。 自定义域名解析拓展DNS的方法就是为特定规则的域名指定DNS服务器，在ConfigMap中设置指定域名相对的dns server，如consul.local结尾的域名使用10.150.0.1来解析。 ConfigMap12345678910apiVersion: v1kind: ConfigMapmetadata: name: kube-dns namespace: kube-systemdata: stubDomains: | {&quot;consul.local&quot;: [&quot;10.150.0.1&quot;]} upstreamNameservers: | [&quot;172.16.0.1&quot;] 自定义规则不对dnsPolicy为Default或None的Pod起作用，只有当ClusterFirst时，域名解析会按照stubDomains和upstreamNameservers来解析。无自定义配置: 任何不匹配集群域名后缀的请求，被转发给节点的dns.自定义: 如果stub和upstream配置，按照如下顺序 带集群后缀的，请求转发给kube-dns stub后缀的，转发给指定的dns 其它的转发给upstream dns Consul as a DNSConsul是Golang实现的服务发现工具，同时支持DNS解析，通过HTTP API动态添加服务发现节点实现动态DNS解析。 注册Redis1到redis dns.json1234567891011121314{ &quot;ID&quot;: &quot;redis1&quot;, &quot;Name&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;127.0.0.1&quot;, &quot;Port&quot;: 8000, &quot;Meta&quot;: { &quot;redis_version&quot;: &quot;4.0&quot; }, &quot;EnableTagOverride&quot;: false} 1$ curl -XPUT http://localhost:8500/v1/agent/service/register -d @dns.json 使用DNS查询 dig redis service1234567891011121314151617181920$ dig @127.0.0.1 -p 8600 redis.service.consul SRV; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 8600 redis.service.consul SRV; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 6823;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 3;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;redis.service.consul. IN SRV;; ANSWER SECTION:redis.service.consul. 0 IN SRV 1 1 8000 srjiangs-MacBook-Pro.local.node.dc1.consul.;; ADDITIONAL SECTION:srjiangs-MacBook-Pro.local.node.dc1.consul. 0 IN A 127.0.0.1srjiangs-MacBook-Pro.local.node.dc1.consul. 0 IN TXT &quot;consul-network-segment=&quot; 注册多个Redis dns.json1234567891011121314{ &quot;ID&quot;: &quot;redis2&quot;, &quot;Name&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;127.0.0.1&quot;, &quot;Port&quot;: 8000, &quot;Meta&quot;: { &quot;redis_version&quot;: &quot;4.0&quot; }, &quot;EnableTagOverride&quot;: false} register1$ curl -XPUT http://localhost:8500/v1/agent/service/register -d @dns.json DNS查询 dig1234567891011121314151617181920212223242526272829$ dig @127.0.0.1 -p 8600 redis.service.consul SRV; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 8600 redis.service.consul SRV; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 11920;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 5;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;redis.service.consul. IN SRV;; ANSWER SECTION:redis.service.consul. 0 IN SRV 1 1 9000 srjiangs-MacBook-Pro.local.node.dc1.consul.redis.service.consul. 0 IN SRV 1 1 8000 srjiangs-MacBook-Pro.local.node.dc1.consul.;; ADDITIONAL SECTION:srjiangs-MacBook-Pro.local.node.dc1.consul. 0 IN A 127.0.0.1srjiangs-MacBook-Pro.local.node.dc1.consul. 0 IN TXT &quot;consul-network-segment=&quot;srjiangs-MacBook-Pro.local.node.dc1.consul. 0 IN A 127.0.0.1srjiangs-MacBook-Pro.local.node.dc1.consul. 0 IN TXT &quot;consul-network-segment=&quot;;; Query time: 0 msec;; SERVER: 127.0.0.1#8600(127.0.0.1);; WHEN: Thu Aug 16 16:47:43 CST 2018;; MSG SIZE rcvd: 277 Consul Service 1234567891011121314151617181920212223242526272829$ curl http://localhost:8500/v1/agent/services{ &quot;redis1&quot;: { &quot;ID&quot;: &quot;redis1&quot;, &quot;Service&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;127.0.0.1&quot;, &quot;Port&quot;: 8000, &quot;EnableTagOverride&quot;: false, &quot;CreateIndex&quot;: 0, &quot;ModifyIndex&quot;: 0 }, &quot;redis2&quot;: { &quot;ID&quot;: &quot;redis2&quot;, &quot;Service&quot;: &quot;redis&quot;, &quot;Tags&quot;: [ &quot;primary&quot;, &quot;v1&quot; ], &quot;Address&quot;: &quot;127.0.0.1&quot;, &quot;Port&quot;: 9000, &quot;EnableTagOverride&quot;: false, &quot;CreateIndex&quot;: 0, &quot;ModifyIndex&quot;: 0 }} 更多细节 根据集群DC返回该数据中心能访问到的DNS 动态添加域名和IP映射 引用 Customizing DNS Service - Kubernetes GitHub - skynetservices/skydns: DNS service discovery for etcd Adding entries to Pod /etc/hosts with HostAliases - Kubernetes","link":"/operation/extend-kubernetes-dns/"},{"title":"拒绝二手资料","text":"不扯远了, 缘起是我玩了这么久计算机, 做了这么久的web开发, 竟然tm的没有真正看过http的文档, 不仅是http, 其它的一些, 像写sheme的时候竟然也没有看过scheme的规格文档, 真是服了自己, 这些关于http,scheme,java,php等都是通过间接从别人消化了之后的陈述获得, 这样速度固然快, 但是难免不能完整的经历一个技术的发展思想, 缘何来, 向何处去, 这样子的不完整性是非常不利于深入领域, 钻研核心的. 所以我给自己定下规矩, 凡事准备严肃对待的事物都要去学习它的第一手资料, 了解它们来龙去脉, 不然绝不说自己是真正会这些东西. 今天是张国荣先生离开第12年, 感谢他和他的作品给我带来的那么多的正能量.","link":"/life/first-hand-doc/"},{"title":"初次使用golang channel","text":"goroutine之间的同步goroutine是golang中在语言级别实现的轻量级线程，仅仅利用go就能立刻起一个新线程。多线程会引入线程之间的同步问题，经典的同步问题如生产者-消费者问题，在c，java级别需要使用锁、信号量进行共享资源的互斥使用和多线程之间的时序控制，而在golang中有channel作为同步的工具。 1. channel实现两个goroutine之间的通信123456789101112131415161718192021222324package main import &quot;strconv&quot;import &quot;fmt&quot;func main() { taskChan := make(chan string, 3) doneChan := make(chan int, 1) for i := 0; i &lt; 3; i++ { taskChan &lt;- strconv.Itoa(i) fmt.Println(&quot;send: &quot;, i) } go func() { for i := 0; i &lt; 3; i++ { task := &lt;-taskChan fmt.Println(&quot;received: &quot;, task) } doneChan &lt;- 1 }() &lt;-doneChan} 创建一个channel，make(chan TYPE {, NUM}), TYPE指的是channel中传输的数据类型，第二个参数是可选的，指的是channel的容量大小。 向channel传入数据，CHAN &lt;- DATA, CHAN指的是目的channel即收集数据的一方，DATA则是要传的数据。 启动一个goroutine接收main routine向channel发送的数据，go func(){ BODY }()新建一个线程运行一个匿名函数。 从channel读取数据，DATA := &lt;-CHAN，和向channel传入数据相反，在数据输送箭头的右侧的是channel，形象地展现了数据从‘隧道’流出到变量里。 通知主线程任务执行结束，doneChan的作用是为了让main routine等待这个刚起的goroutine结束，这里显示了channel的另一个特性，如果从empty channel中读取数据，则会阻塞当前routine，直到有数据可以读取。 上面这个程序就是main routine向另一个routine发送了3条int类型的数据，当3条数据被接收到后，主线程也从阻塞状态恢复运行，随后结束。 2. 不要陷入“死锁”我一开始用channel的时候有报过*”fatal error: all goroutines are asleep - deadlock! “*的错误，真实的代码是下面这样的: 12345678910111213package mainimport &quot;fmt&quot;func main() { ch := make(chan int) ch &lt;- 1 // I'm blocked because there is no channel read yet. fmt.Println(&quot;send&quot;) go func() { &lt;-ch // I will never be called for the main routine is blocked! fmt.Println(&quot;received&quot;) }() fmt.Println(&quot;over&quot;)} 我的本意是从main routine发送给另一个routine一个int型的数据，但是运行出了上述的错误，原因有2个: 当前routine向channel发送/接收数据时，如果另一端没有相应地接收/发送，那么当前routine则会进行休眠。 这个程序的main routine先行在ch &lt;- 1进入休眠状态，程序的余下部分根本来不及运行，那么channel里的数据永远不会被读出，也就不能唤醒main routine，进入“死锁”。 解决这个“死锁”的方法可是是设置channel的容量大小大于1，那么channel就不会因为数据输入而阻塞主程; 或者将数据输入channel的语句置于启动新的goroutine之后。 3. channel作为状态转移的信号源我跟着MIT的分布式计算课程做了原型为Map-Reduce的课后练习，目的是实现一个Master向Worker分派任务的功能：Master服务器去等待Worker服务器连接注册，Master先将Map任务和Reduce任务分派给这些注册Worker，等待Map任务全部完成，然后将Reduce任务再分配出去，等待全部完成。 123456789101112131415161718192021222324252627282930313233343536373839// Initialize a channel which records the process of the map jobs.mapJobChannel := make(chan string) // Start a goroutine to send the nMap(the number of the Map jobs) tasks to the main routine.go func() { for m := 0; m &lt; nMap; m++ { // Send the &quot;start a Map job &lt;m&gt;&quot; to the receiver. mapJobChannel &lt;- &quot;start, &quot; + strconv.Itoa(m) }}()// Main routine listens on this mapJobChannel for the Map job task information.nMapJobs := nMap// Process the nMap Map tasks util they're all done.for nMapJobs &gt; 0 { // Receive the Map tasks from the channel. taskInfo := strings.Split(&lt;-mapJobChannel, &quot;,&quot;) state, mapNo := taskInfo[0], taskInfo[1] if state == &quot;start&quot; { // Assign a Map task with number mapNo to a worker. go func() { // Wait for a worker to finish the Map task. ok, workNo := assignJobToWorker(&quot;Map&quot;, mapNo) if ok { // Send a task finish signal and set the worker's state to idle. mapJobChannel &lt;- &quot;end, &quot; + mapNo setWorkerState(workNo, &quot;idle&quot;) } else { // Restart this task and set the worker's state to finished. mapJobChannel &lt;- &quot;start, &quot; + mapNo setWorkerState(workNo, &quot;finished&quot;) } }() } else { nMapJobs-- }} 以上是我截取自己写的代码，关于用channel来传递当前Map任务的进度信息，用类似信号的方式标注当前的任务执行状态。 当从channel中读取到”start, {NUM}”时找一个空闲的Worker去执行Map任务，并且等待它的完成，完成成功则向channel中发送”end, {NUM}”信号，表明任务完成，如果失败，就重发”start, {NUM}”信号。 从channel中读取到”end, {NUM}”时，把剩余任务数减1。这种信号触发的方式，触发Master的状态转移，并且可以通过增加信号以及信号处理的方式，拓展业务处理的情况，暂时还能处理这个需求情景。 代码文件github地址MIT分布式系统课程","link":"/programming/golang-channel/"},{"title":"再见，micro: 迁移go-micro到纯gRPC框架","text":"micro是基于golang的微服务框架，之前华尔街见闻架构升级中谈到了我们是基于go-micro的后端架构，随着我们对服务网格的调研、测试和实施，为了打通不同语言之间的服务调用，我们选择了gRPC作为服务内部的通用协议。 go-micro框架的架构非常具有拓展性，它拥有自己的RPC框架，通过抽象codec,transport,selector等微服务组件，你既可以使用官方实现的各种插件go-plugins进行组装，又可以根据实际的情况实现自己的组件。然而，我们打算利用服务网格的优势，将微服务的基础组件下沉到基础设施中去，将组件代码从代码库中剥离开来。 这样一来，我们相当于只需要最简的RPC框架，只需要服务之间有统一、稳定、高效的通信协议，由于micro在我们新架构中略显臃肿，于是我们选择逐渐剥除micro。还有一个重要原因，我们选择的服务网格方案是istio，它的代理原生支持gRPC，而micro只是将gRPC当做transport层，相当于复写了gRPC的服务路由逻辑，这样有损于istio的一些特性，譬如流量监控等功能。 出于这些考虑，第一步需要将micro改成纯gRPC模式，这里的改造部分我们考虑只应该去更改基础库的代码，而尽量不要使业务代码更改，减少对已有逻辑的影响，和一些软性的譬如开发人员的工作量。 服务发现模块istio的服务发现支持etcd、consul等，我们需要将其改成使用kubernetes的服务名进行访问。通过实现istio selector的方式，我们将RPC调用的服务名与k8s的服务名端口做映射。 register.go1234567891011121314151617181920212223242526272829303132333435363738package istioimport ( &quot;fmt&quot; &quot;github.com/micro/go-micro/registry&quot; &quot;github.com/micro/go-micro/selector&quot;)const ( svcPort = 10088)var ( serviceHostMapping = map[string]string{ &quot;payment&quot;: &quot;payment.common&quot;, &quot;content&quot;: &quot;content.ns1&quot;, &quot;user&quot;: &quot;user.ns1&quot;, })type istio struct{}func (r *istio) Select(service string, opts ...selector.SelectOption) (selector.Next, error) { if host, exist := serviceHostMapping[service]; exist { node := &amp;registry.Node{ Id: service, Address: host, Port: svcPort, } return func() (*registry.Node, error) { return node, nil }, nil } return nil, fmt.Errorf(&quot;service %s(%s) not found&quot;, service, svc)}... 这里注意的是由于服务之间调用需要指定service name和端口，所以在这里我们把端口设置了一个magic number。 传输模块go-micro采用各种协议作为传输报文的工具，可以从transport中了解到，有http/grpc/tcp/utp等，我们曾先后使用过tcp、utp、gRPC，经过测试gRPC是其中最为稳定的。之前提到过gRPC只是作为传输信道，micro定义了自己的RPC接口，实现了RPC的路由、传输、重试等功能，通过自定义了protobuf的插件生成符合micro标准的proto文件。 为了向后兼容，在使用grpc替换原RPC时，我也需要根据protobuf文件生成新的golang代码，其中包括client端、server端代码的变更，实际上我的更新是针对protoc-gen-go，fork之后修改grpc/grpc.go部分，在func generateService()中对Client端代码进行micro的适配。 Client1func (c *someClient) DoSomething(ctx context.Context, in *SomeRequest, opts ...grpc.CallOption) (*SomeResponse, error) ServerServer端代码的更新除了接口的适配，有一个关注点是micro的接口设计是 1DoSomething(context.Context, *SomeRequest, *SomeResponse) error 而gRPC是 1DoSomething(context.Context, *SomeRequest) (*SomeResponse, error) micro接口设计的好处是支持服务端缓存的应用，当服务端handler触发时，cache interceptor可以将response缓存，当缓存命中时可以将其返回。而由于gRPC的版本将response放在了返回值中，运行时无法将譬如redis中字符串格式的response解码成SomeResponse，而micro版本由于将其放在了参数位置，所以可以通过 1json.Unmarshall([]byte(&quot;&quot;), &amp;SomeResponse{}) 从字符串恢复response。 我们的做法是不改变接口的设计，而是在自动生成的golang代码中，在interceptor运行之前将response object塞入context。 123456// Serverctx = ctx.WithValue(&quot;IstioResponseBody&quot;, &amp;SomeResponse{})// Interceptorobj := ctx.Value(&quot;IstioResponseBody&quot;)json.Unmarshall([]byte(&quot;&quot;), obj) Interceptor模块一些interceptor的迁移工作，例如logger、cache、recover等。 注意事项 向后兼容更新的过程开发人员实际上并不是特别感兴趣，我们SRE能做的是对他们基本上无改动，第一个版本不要让开发感知。虽然实际上测试时由于我们的变动，测试环境不断受到稳定性质疑。 micro和gRPC的context传递不同gRPC是基于http2，metadata是基于HTTP header传递，Client利用metadata.NewOutgoingContext(ctx, MD)携带metadata，而Server端利用metadata.FromIncomingContext(ctx)提取metadata。而micro则用context.WithValue(metaKey{}, MD)传递。 使用环境变量做一些特殊处理我们通过环境变量在基础库里做了一些判断，在新老架构并存下进行一些特殊处理，譬如刚才说到的metadata获取逻辑。 关于proto自动生成我们通过在CI中配置istio版本镜像的编译打包逻辑和原有逻辑共存，在编译前我们运行proto编译工具，生成gRPC的golang文件，从而打出新的镜像。 总结micro陪伴了我们一年多的时间，期间它的架构设计给了我们很大的弹性可以去适配一些我们的架构，可以选择很多好的开源工具，譬如zipkin、prometheus、etcd等，适合于刚上微服务不久的项目进行技术摸索和选择，而这一年的时间，我们的架构也逐渐明晰和稳定，我们更倾向于一个精简的基础库，并且由于在线项目的不断增加，运维成本可以通过下沉基础组件进行降低。 Recommended Posts(Driven byHexo Recommended Posts plugin)Anatomy of envoy proxy: the architecture of envoy and how it worksIstio Version Control On KubernetesgRPC Golang Client Connection Test微服务实践四: 配置管理","link":"/microservice/goodbye-micro/"},{"title":"gRPC Golang Client Connection Test","text":"gRPC is a well-known RPC protocol and a lot of companies adopted it as an internal communication protocol because of its robustness and stability. To use it more efficiently, I’ve done some experiments about how to maximize the gRPC client concurrency. 3 Tests One connection per requestCreate a connection when request is made. Only one client, one connectionUse a common client by all requests. Fixed-size Connection poolIf connection pool has enough connections, take it from pool, otherwise create a new connection.The connection pool has fixed max capacity, release unused connection to pool when it’s not full. Performance ComparisonHardwareMacBook Pro (15-inch, 2016)Processor 2.7 GHz Intel Core i7Memory 16GB 2133 MHz LPDDR3 Press tool, client and server run on the same machine When server just says hello to client Only one client, one connection 123456789$ wrk -t2 -c100 -d10s http://localhost:10099/performanceRunning 10s test @ http://localhost:10099/performance 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 2.85ms 1.24ms 22.24ms 80.47% Req/Sec 15.86k 2.86k 21.58k 76.50% 316674 requests in 10.04s, 38.66MB readRequests/sec: 31532.58Transfer/sec: 3.85MB Fixed-size Connection poolTake connection from pool first, otherwise create a new connection. 123456789$ wrk -t2 -c100 -d10s http://localhost:10099/performanceRunning 10s test @ http://localhost:10099/performance 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 36.86ms 34.35ms 334.72ms 83.30% Req/Sec 1.58k 660.95 3.89k 67.86% 31127 requests in 10.04s, 3.80MB readRequests/sec: 3100.89Transfer/sec: 387.61KB One connection per requestClose to the second result, about 3000 Requests/sec When server sleeps 0.5s and says hello to client Only one client, one connection 123456789$ wrk -t2 -c100 -d10s http://localhost:10099/performanceRunning 10s test @ http://localhost:10099/performance 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 503.43ms 1.95ms 511.62ms 64.35% Req/Sec 130.60 119.12 485.00 80.85% 2000 requests in 10.10s, 250.00KB readRequests/sec: 198.04Transfer/sec: 24.75KB Fixed-size Connection pool 123456789$ wrk -t2 -c100 -d10s http://localhost:10099/performanceRunning 10s test @ http://localhost:10099/performance 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 504.44ms 5.14ms 529.46ms 92.58% Req/Sec 119.40 95.83 470.00 85.11% 1901 requests in 10.07s, 237.62KB readRequests/sec: 188.80Transfer/sec: 23.60KB one connection per request 123456789$ wrk -t2 -c100 -d10s http://localhost:10099/performanceRunning 10s test @ http://localhost:10099/performance 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 509.30ms 4.85ms 529.65ms 73.74% Req/Sec 188.81 188.12 494.00 71.23% 1900 requests in 10.06s, 237.50KB readRequests/sec: 188.89Transfer/sec: 23.61KB Sample CodeFind sample code Some ResultsIn gereral, use shared grpc client rather than use a customized connection pool. Referencegrpcpooling-grpc-connections Recommended Posts(Driven byHexo Recommended Posts plugin)再见，micro: 迁移go-micro到纯gRPC框架","link":"/programming/grpc-go-client-performance-test/"},{"title":"My first Hackathon: bring Spinnaker to my company","text":"I’ve joined my first Hackathon and worked on a project about using Spinnaker as CI/CD tool within company. The biggest challenge is to install Spinnaker on CentOS 7 with docker-compose. Why Spinnaker? Spinnaker is dedicated to deploy services across multiple cloud providers and the integration with AWS, GCP, Azure is out of box. It’s focused on deploy stably, support full control of workflow, developers can customize the deployment flow to improve the quality of deployment, also it’s automatic. You will have a Web UI. How to use Spinnaker?Spinnaker can integrate with VCS like Github, Gitlab, Gitbucket, docker registry.For example the workflow of deploy golang github project to Kubernetes cluster. A developer pushes code to Github repository, it triggers the Travis-CI that starts the test, build, package stages, a docker image is pushed to Docker repository finally. Spinnaker has a trigger on the Docker repository and starts to deploy that image to a Kubernetes cluster in some cloud provider. The deployment workflow includes Deploy to staging, Canary Deployment, Deploy to production. Details of this hackathonOur goal is to start a Spinnaker cluster and create a pipeline that can deploy the Git project to AWS and our own Kubernetes cluster.There’re some contraints for this hackathon, I’m still new to the company so maybe it’s because of my limited information. I don’t have a Kuberntes cluster with API server’s access right, that means I can’t deploy Spinnaker cluster to Kubernetes. I don’t have a Debian/Ubuntu machine, that means I can’t start Spinnaker on a bare metal with Halyard. The Spinnaker need to be deployed within company’s infrastructure which is not any cloud provider, otherwise it can’t communicate with other infrastructure. And what does Spinnaker need? It needs Halyard to configure the Spinnaker cluster, it will control where Spinnaker cluster is deployed, as we know Spinnaker itself is composed of microservices, hard to deploy on bare metal. Especially for its cloud provider settings, so Spinnaker relies on Halyard heavily, and every time the Spinnaker cluster update and upgrade can be managed safely. Sadly, Halyard is only used for deploying Spinnaker itself to Kubernetes or some cloud providers and Debian/Ubuntu single machine. Then according to our constraints above, we lose every chance to use that tool. Maybe it’s time to give up :( We didn’t give up, choose to install Spinnaker cluster the hardest way. Install Spinnaker cluster on Bare metal machine which is CentOS 7, for running the cluster easier, we use Docker to start each components. My colleague found docker-compose.yml in Spinnaker, but the latest modification time was 4 years ago. After trying so many times we still couldn’t make it, I tried to give up actually, and also it was far away from quickstart and wasted so much time. Actually I didn’t give up, I returned home and thought if the Spinnaker cluster can be deployed in Kubernetes, it can be deployed with Docker compose, since they didn’t have any difference. So I chose the most stupid method, I created a Spinnaker cluster on GCP with far less time and rewrote the docker-compose.yml according to: Check each Kubernetes Pod’s yaml file to get the environment variables, image name, arguments… Login to the containers to fetch the mounted configuration files like clouddriver.yml, front50.yml, gate.yml… Yes, they’re generated by Halyard where the comment says DONT'T EDIT THIS FILE, IT'S AUTOGENERATED BY.... After another X hours, without ignoring every details, I migrated all the Kubernetes configuration to docker-compose.yml, the second when I ran the docker-compose up -d and every microservice was labeled DONE and http://localhost:9000 worked. Reflection of this hackathonYes, it’s not a happy ending for my first hackathon, our team didn’t reach our goal, there’s some experience for this time. Gain The hardest way to know how the Spinnaker configuration works, how it deploys. I felt some joy when the installation was successful :) Some knowledge about the GCP (first time to use GCP) Improvements next time Team member can be closer, whatever the seat we sit or the communication channel we talked, time is presious, we need check our progress frequently. Fail fast, although this time I worked out the installation, but it was not a great result, next time we should help the team member to tackle the most important task. Prepare enough, time is precious, if you want to make it in the Hackathon project, do some preparations, it’s not cheat! Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesImplement zero downtime HTTP service rollout on Kubernetes","link":"/operation/hackathon-spinnaker/"},{"title":"Hive on spark实践","text":"配置利用Cloudera的CDH套件搭建好Hadoop 2.6，可CDH中的Hive版本不高，于是独立安装Hive 2.3，由于Hive的执行引擎默认是Spark，根据Hive官网上的Hive on Spark教程开始配置。 Standalone配置了Spark Standalone模式之后，配置Hive遇到一些困难。 由于使用的是pre-build版本的Spark，遇到报错： 12java.lang.NoSuchFieldError: SPARK_RPC_CLIENT_CONNECT_TIMEOUT at org.apache.hive.spark.client.rpc.RpcConfiguration.&lt;clinit&gt;(RpcConfiguration.java:46) 接下来要自己编译Spark without Hive，可是一直没有解决，报错为： 1234[info] 'compiler-interface' not yet compiled for Scala 2.11.8. Compiling...error: scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found. at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:17) at scala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:18) 非常曲折，最后选择尝试已经配置好的Yarn模式的Spark。 YarnYarn模式的Spark集群已经就绪，在Hive中配置Spark 123456set hive.execution.engine=spark;set spark.master=yarn;set spark.executor.memory=512m;set spark.serializer=org.apache.spark.serializer.KryoSerializer;set spark.eventLog.enabled=true;set spark.eventLog.dir=&lt;Spark event log folder (must exist)&gt; 上传Spark应用所需的Spark依赖jar包到Hdfs中，例如放到Hdfs中的/spark-jars目录，并配置hive-site.xml 1234&lt;property&gt; &lt;name&gt;spark.yarn.jars&lt;/name&gt; &lt;value&gt;hdfs://xxxx:8020/spark-jars/*&lt;/value&gt;&lt;/property&gt; 注意这里需要保证使用Hive时的用户能够有权限访问Hdfs上指定的目录。 用户数据导入源数据当源数据是JSON并位于Hdfs的/tmp目录，如 1234567{ &quot;time&quot;: &quot;1515682813526&quot;, &quot;uid&quot;: 1, &quot;ip&quot;: &quot;1.1.1.1&quot;, &quot;path&quot;: &quot;https://github.com&quot;, &quot;referer&quot;: &quot;https://google.com&quot;} 创建Hive外部表12345678910CREATE EXTERNAL TABLE IF NOT EXISTS tmp.pageview( time BIGINT, uid BIGINT, ip STRING, path STRING, referer STRING)ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'STORED AS TEXTFILELOCATION '/tmp'; 导入ORC表ORC表是Hive的一种高效表，在查询上更加快速，可以从外部表将数据导入ORC表。 123456789101112CREATE TABLE IF NOT EXISTS orc.pageview( time BIGINT, uid BIGINT, ip STRING, path STRING, referer STRING)STORED AS ORC;FROM tmp.pageview pvINSERT INTO TABLE orc.pageviewSELECT time, uid, ip, path, referer; Recommended Posts(Driven byHexo Recommended Posts plugin)Spark on Hive实现APP渠道分析Hive环境搭建(Ubuntu)","link":"/programming/hive-on-spark-practice/"},{"title":"Hive环境搭建(Ubuntu)","text":"Hive是什么？基于Hadoop的数据查询工具，可以使用类SQL进行数据查询。 Hadoop安装 参考了Hadoop安装 注意到etc/hadoop/hadoop-env.sh里设置JAVA_HOME，不然运行./start_dfs.sh会报没设置JAVA_HOME 运行程序时，输出目录不能存在 配置文件 hdfs的输出目录需要保证空间充足，我在虚拟机里玩，分配的磁盘空间太小，另外加了空间并挂载，重新设置了输出目录。 Hive安装 参考了安装hive MetaStore用了MySQL，结果安装的驱动版本不正确，遇到MetaStoreClient lost connection. Attempting to reconnect。 实践后感真正装环境的坑还是很多的，开始分配的虚拟机磁盘空间太小了，基本运行不了，jdbc驱动版本不正确都会导致Hive客户端开启后时常崩溃，配置文件的一些常用选项需要了解，如果出现问题可以有一个解决思路，当然Google还是常用些，实在解决不了如很难想到是jdbc驱动的原因，我就下了一份源码，定位到报错，通过代码推测了发生问题的地方。 引用 安装Hadoop 安装hive Recommended Posts(Driven byHexo Recommended Posts plugin)Spark on Hive实现APP渠道分析Hive on spark实践","link":"/programming/hive-environment-settings/"},{"title":"How to host Swagger documentation using yaml&#x2F;json configuration files?","text":"Maintain the swagger documentation by Swagger Editor and then you can use the yaml files to generate online swagger documentation easily with Spring boot. Workflow for Swagger documentation Update swagger documentation with Swagger Editor, export the yaml files Update the yaml files in Spring boot project Redeploy the Spring boot project How to setup in Spring boot?Swagger provides swagger-ui and some jars to host a documentation, you can use Java annotations or yaml files to autogenerate the swagger documentation. The example below is using static yaml files to generate documentation. Demo project: https://github.com/songrgg/swaggerdemo The static yaml file is fetched from Swagger Editor, put it under the resources directory. src/main/resources/static/swagger-apis/api1/swagger.yaml1234567891011swagger: &quot;2.0&quot;info: description: &quot;This is a sample server Petstore server. You can find out more about Swagger at [http://swagger.io](http://swagger.io) or on [irc.freenode.net, #swagger](http://swagger.io/irc/). For this sample, you can use the api key `special-key` to test the authorization filters.&quot; version: &quot;1.0.0&quot; title: &quot;Swagger Petstore&quot; termsOfService: &quot;http://swagger.io/terms/&quot; contact: email: &quot;apiteam@swagger.io&quot; license: name: &quot;Apache 2.0&quot; url: &quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot; Application.java123456789101112131415161718@EnableSwagger2@SpringBootApplicationpublic class SwaggerDemoApplication { public static void main(String[] args) { SpringApplication.run(SwaggerDemoApplication.class, args); } @Bean public Docket swagger() { return new Docket(SWAGGER_2) .select() .apis(RequestHandlerSelectors.any()) .paths(PathSelectors.any()) .build(); }} Read the static yaml files:src/main/resources/swagger-apis/api1/swagger.yaml and src/main/resources/swagger-apis/api2/swagger.yaml. SwaggerSpecConfig.java1234567891011121314151617181920212223@Configurationpublic class SwaggerSpecConfig { @Primary @Bean public SwaggerResourcesProvider swaggerResourcesProvider( InMemorySwaggerResourcesProvider defaultResourcesProvider) { return () -&gt; { List&lt;SwaggerResource&gt; resources = new ArrayList&lt;&gt;(); Arrays.asList(&quot;api1&quot;, &quot;api2&quot;) .forEach(resourceName -&gt; resources.add(loadResource(resourceName))); return resources; }; } private SwaggerResource loadResource(String resource) { SwaggerResource wsResource = new SwaggerResource(); wsResource.setName(resource); wsResource.setSwaggerVersion(&quot;2.0&quot;); wsResource.setLocation(&quot;/swagger-apis/&quot; + resource + &quot;/swagger.yaml&quot;); return wsResource; }} pom.xml1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-common&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; Run the spring boot server and access &lt;hostname&gt;/swagger-ui.html to see the documentation. Recommended Posts(Driven byHexo Recommended Posts plugin)Spring data redis的一个bugSpring boot实现数据库读写分离解决Jedis数据读取乱码问题","link":"/operation/host-swagger-documentation-with-yaml-json-files/"},{"title":"How does Prometheus query work? - Part 1, Step, Query and Range","text":"Prometheus is an opensource time series database, commonly used to gather and calculate monitoring metrics, this article explains how the query works with /query_range API. Start a PrometheusAccording to the Prometheus doc, a prometheus server has been started and listens at localhost:9090. Prometheus browser is a WEB UI that is used to query the metrics for testing, the path is http://localhost:9090/graph, there are two APIs used to query the metrics, the first one is /query (which is used to see the metric value in a specified time point, the other one is /query_range used to query the metric during a period. Metric typesBefore we start analyzing a query, we need to know the metric types Prometheus provides: CounterIndicates an cumulative number of the observable, for example the total http request number. This metric value is increasing monotonically increasing. For instance, http_request_total records the total count of HTTP requests the server serves. GaugeA gauge is a metric that represents a single numerical value that can arbitrarily go up and down. For instance, the instance’s CPU usage, it’s changeable all the time. HistogramA histogram samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values.The Prometheus Histograms gives an example, http_request_duration_seconds, consider we have an SLO requirement, 95% requests’ response time is within 300ms, so the straightforward way is to divide the bucket into several segments, for example, 300ms, 1s, 5s, 10s, +inf, and each bucket contains its counter metric which counts the total number of the requests within this bucket, 123456http_request_duration_bucket[0 - 300ms][0 - 1s][0 - 5s][0 - 10s][0 - +inf] You’ll see the bucket is divided and each segment starts from 0 seconds, it means [0 - 1s] will include the [0 - 300ms]. As we said, each bucket metric is of counter type, it records the total requests number within that response time and there’s a total count metric called http_request_duration_seconds_count. To calculate how much does 300ms occupy, we can calculate with http_request_duration_seconds_bucket{le=&quot;0.3&quot;}/http_request_duration_seconds_count, it calculates the instant value of the moment, but if we only use the instant value of that moment, the data will be not smooth and the graph might be spiky, so we’d better use duration to gather more data, sum(rate(http_request_duration_seconds_bucket{le=&quot;0.3&quot;}[5m]))/sum(rate(http_request_duration_seconds_count[5m])). SummarySimilar to a histogram, a summary samples observations (usually things like request durations and response sizes). While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window. How does query work?When we use Prometheus to calculate a query, normally we’re using the query_range functionality, after a time range and step is specified, the query will be applied to every step and a point will be put into the results. Let’s see an entire query API:http://localhost:9090/api/v1/query_range?query=prometheus_target_interval_length_seconds&amp;start=1590830727.588&amp;end=1590834327.588&amp;step=14 it might be a little messy, let’s break it down into parameters, queryquery is the metric formula that needs to be calculated Time range (start and end)It is easy to understand, we can specify when should the metrics start and end, I want to see the last 30 minutes or last 7 days, it’s set by the start and end parameters, their format is unix timestamp. stepIt is used to decide how many data points we need by setting each data points’ interval and its format is second. The result is too much, I would only paste part of it: 12345678910111213141516171819{ &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: { &quot;resultType&quot;: &quot;matrix&quot;, &quot;result&quot;: [{ &quot;metric&quot;: { &quot;__name__&quot;: &quot;prometheus_target_interval_length_seconds&quot;, &quot;instance&quot;: &quot;localhost:9090&quot;, &quot;interval&quot;: &quot;15s&quot;, &quot;job&quot;: &quot;prometheus&quot;, &quot;quantile&quot;: &quot;0.01&quot; }, &quot;values&quot;: [ [1590830727.588, &quot;14.996369259&quot;], [1590830741.588, &quot;14.996369259”] ] }] }} status marks the calculation is successful or not result.metric shows the original metric result.values shows the actual data points we need, the left value is the timestamp, the right one is the metric value.Each result value’s interval is exactly the step we set, in the previous example, it’s 14 seconds. It means every 14 second, the query will be calculated and one data point is generated. ConclusionI explored the query_range API a bit in this article, in the next article, I’ll explore some frequent Prometheus functions like rate, irate, histogram_percentile, etc. Reference Prometheus - Metric types Prometheus - Get started Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesHow to check and monitor SSL certificates expiration with TelegrafGenerate monitoring dashboards & alertings using Grafana API","link":"/operation/how-does-prometheus-query-works/"},{"title":"How to alert for Pod Restart &amp; OOMKilled in Kubernetes","text":"This article introduces how to set up alerts for monitoring Kubernetes Pod restarts and more importantly, when the Pods are OOMKilled we can be notified. ContextRecently, we noticed some containers’ restart counts were high, and found they were caused by OOMKill (the process is out of memory and the operating system kills it). No existing alerts are reporting the container restarts and OOMKills so far. Although some OOMs may not affect the SLIs of the applications, it may still cause some requests to be interrupted, more severely, when some of the Pods were down the capacity of the application will be under expected, it might cause cascading resource fatigue. Data sourcecadvisor &amp; kube-state-metrics expose the k8s metrics, Prometheus and other metric collection system will scrape the metrics from them. Here’s the list of cadvisor k8s metrics when using Prometheus. Container Restart MetricFor monitoring the container restarts, kube-state-metrics exposes the metrics to Prometheus as kube_pod_container_status_restarts_total → CountThe number of container restarts per container. We can use the increase of Pod container restart count in the last 1h to track the restarts. 1increase(kube_pod_container_status_restarts_total[1h]) OOMKilled MetricWhen the containers were killed because of OOMKilled, the container’s exit reason will be populated as OOMKilled and meanwhile it will emit a gauge kube_pod_container_status_last_terminated_reason { reason: &quot;OOMKilled&quot;, container: &quot;some-container&quot; } , kube_pod_container_status_last_terminated_reason → GaugeDescribes the last reason the container was in the terminated state. In Prometheus, we can use kube_pod_container_status_last_terminated_reason{reason=&quot;OOMKilled&quot;} to filter the OOMKilled metrics and build the graph. However, as Guide to OOMKill Alerting in Kubernetes Clusters said, this metric will not be emitted when the OOMKill comes from the child process instead of the main process, so a more reliable way is to listen to the Kubernetes OOMKill events and build metrics based on that. Fortunately, cadvisor provides such container_oom_events_total which represents “Count of out of memory events observed for the container” after v0.39.1 container_oom_events_total → counterDescribes the container’s OOM events. cadvisor notices logs started with invoked oom-killer: from /dev/kmsg and emits the metric. The kernel will oomkill the container when free memory is under the low limit memory fragment, when allocating memory greater than and there is no contiguous memory available. AlertingWe want to get notified when the service is below capacity or restarted unexpectedly so the team can start to find the root cause. low-capacity alertsThis alert notifies when the capacity of your application is below the threshold. The threshold is related to the service and its total pod count. For example, if an application has 10 pods and 8 of them can hold the normal traffic, 80% can be an appropriate threshold. In another case, if the total pod count is low, the alert can be how many pods should be alive. 12# Use Prometheus as data sourcekube_deployment_status_replicas_available{namespace=&quot;$PROJECT&quot;} / kube_deployment_spec_replicas{namespace=&quot;$PROJECT&quot;} This alert can be highly critical when your service is critical and out of capacity. Pod container restart rate too highThis alert triggers when your pod’s container restarts frequently. It can be critical when several pods restart at the same time so that not enough pods are handling the requests. This alert can be low urgent for the applications which have a proper retry mechanism and fault tolerance. When a request is interrupted by pod restart, it will be retried later. Otherwise, this can be critical to the application. We can use the pod container restart count in the last 1h and set the alert when it exceeds the threshold. 12# prometheusincrease(kube_pod_container_status_restarts_total{namespace=&quot;$PROJECT&quot;, pod=~&quot;.*$APP.*&quot;}[1h]) For this alert, it can be low critical and sent to the development channel for the team on-call to check. OOMEventsOOMEvents is a useful metric for complementing the pod container restart alert, it’s clear and straightforward, currently we can get the OOMEvents from kube_pod_container_status_last_terminated_reason exposed by cadvisor.` 123456# prometheus, fetch the counter of the containers OOM events.container_oom_events_total{name=&quot;&lt;some-container&gt;&quot;}# OR if your cadvisor is below v3.9.1# prometheus, fetch the gauge of the containers terminated by OOMKilled in the specific namespace.kube_pod_container_status_last_terminated_reason{reason=&quot;OOMKilled&quot;,namespace=&quot;$PROJECT&quot;} For this alert, it can be low critical and sent to the development channel for the team on-call to check. ConclusionBy using these metrics you will have a better understanding of your k8s applications, a good idea will be to create a grafana template dashboard of these metrics, any team can fork this dashboard and build their own. Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesImplement zero downtime HTTP service rollout on KubernetesHow does Prometheus query work? - Part 1, Step, Query and Range","link":"/operation/how-to-alert-for-Pod-Restart-OOMKilled-in-Kubernetes/"},{"title":"how to build the smallest docker image as fast as you can","text":"I’ll use an example to introduce how to build the smallest docker image to your best, a light image will accelerate image rollout and the fast build process will speed up your development cycle. A Golang sampleIt’s quite common that we use golang to implement microservice, for example tens of golang service docker images are deployed to the Kubernetes. Consider we need to make our first golang service image, it runs the following code as an HTTP server. 123456789101112131415package mainimport ( &quot;net/http&quot;)func main() { http.HandleFunc(&quot;/&quot;, test) http.ListenAndServe(&quot;:8080&quot;, nil)}func test(w http.ResponseWriter, r *http.Request) { w.Header().Set(&quot;Service&quot;, &quot;Test&quot;) w.WriteHeader(200)} Docker it!The intuitive solution is to run go run main.go in the docker, so let’s use golang image as a base image. 12345FROM golang:1.14.0-alpineWORKDIR /go/src/github.com/songrgg/testservice/COPY main.go .CMD [ &quot;go&quot;, &quot;run&quot;, &quot;main.go&quot; ] Run docker build . and it shows 12345678910111213141516Sending build context to Docker daemon 3.072kBStep 1/4 : FROM golang:1.14.0-alpine ---&gt; 51e47ee4db58Step 2/4 : WORKDIR /go/src/github.com/songrgg/testservice/ ---&gt; Using cache ---&gt; 8dc325ca7ca6Step 3/4 : COPY main.go . ---&gt; Using cache ---&gt; c46c5f5bfda8Step 4/4 : CMD [ &quot;go&quot;, &quot;run&quot;, &quot;main.go&quot; ] ---&gt; Using cache ---&gt; acc5a6d462f5Successfully built acc5a6d462f5$ docker image inspect acc5a6d462f5 --format='{{.Size}}'369193951 It’s 369193951 bytes near 370MB. Reduce the unnecessary filesGolang is a compilation language which can be packed into a binary, we can reduce the size by only putting the binary into the image. We know that the latest docker supports multi-stage builds which can eliminate the intermediate layers effectively, the revised dockerfile looks like this: 1234567891011FROM golang:1.14.0-alpineWORKDIR /go/src/github.com/songrgg/testservice/COPY main.go .RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROM alpine:latestRUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=0 /go/src/github.com/songrgg/testservice/app .CMD [&quot;./app&quot;] Let’s see the layers it generates. 12345678910111213141516171819202122232425262728293031323334353637383940$ docker build .Sending build context to Docker daemon 3.072kBStep 1/9 : FROM golang:1.14.0-alpine ---&gt; 51e47ee4db58Step 2/9 : WORKDIR /go/src/github.com/songrgg/testservice/ ---&gt; Using cache ---&gt; 8dc325ca7ca6Step 3/9 : COPY main.go . ---&gt; Using cache ---&gt; c46c5f5bfda8Step 4/9 : RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . ---&gt; Running in be3bdce1ec48Removing intermediate container be3bdce1ec48 ---&gt; 9c3470f9e73dStep 5/9 : FROM alpine:latest ---&gt; 053cde6e8953Step 6/9 : RUN apk --no-cache add ca-certificates ---&gt; Running in 6acd18e2d2bafetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gzfetch http://dl-cdn.alpinelinux.org/alpine/v3.6/community/x86_64/APKINDEX.tar.gz(1/1) Installing ca-certificates (20161130-r3)Executing busybox-1.26.2-r7.triggerExecuting ca-certificates-20161130-r3.triggerOK: 5 MiB in 12 packagesRemoving intermediate container 6acd18e2d2ba ---&gt; 7b4ee3222013Step 7/9 : WORKDIR /root/ ---&gt; Running in 336a8a2115a6Removing intermediate container 336a8a2115a6 ---&gt; 77fa1196ab2fStep 8/9 : COPY --from=0 /go/src/github.com/songrgg/testservice/app . ---&gt; c6bc47f614afStep 9/9 : CMD [&quot;./app&quot;] ---&gt; Running in f77053027c4bRemoving intermediate container f77053027c4b ---&gt; d327f978f1d8Successfully built d327f978f1d8$ docker inspect d327f978f1d8 --format='{{.Size}}'11924648 We could notice there are several intermediate containers removed, they’re layers from the first stage and prepare the executable for the second stage. Most importantly, the docker image size is below 12MB. The most important factor is we changed the FROM image to alpine which is only 4.2MB, so the extra size is almost the size of the golang process. From scratch?Yeah, the base image could be smaller, FROM scratch is a base image that will make the next command to be the first layer in your image. I changed the FROM alpine:latest to FROM scratch, the image size is 7MB now, but I would suggest using alpine because it’ll be hard for you in scratch if you want to debug within the container. So you’ll need a balance between the image size and functionality :) Some pitfalls you may faceUnnecessary large build contextDocker build will send the build context to docker daemon at first, the context is default to the current directory, so please be sure the files in the current directory is necessary or is small enough. If the file size is big, it will affect docker build speed terribly. Wrong command orderJust remember to put the stable layers before the changeable layers, because docker will cache the layers if they are not changed, it’s calculated by the hash value of their content. 1234FROM ubuntuCOPY changeable.txt .RUN apt-get update &amp;&amp; apt-get install curlRUN ... In the above example, every time the changeable.txt is changed, it will rerun every commands after it and waste time doing the things it could prevent. Just turn to the following form. 1234FROM ubuntuRUN apt-get update &amp;&amp; apt-get install curlRUN ...COPY changeable.txt . Reference Best practises for writing Dockerfile Recommended Posts(Driven byHexo Recommended Posts plugin)How to install Spinnaker on CentOS 7Raspberry 3安装docker","link":"/operation/how-to-build-a-smallest-docker-image/"},{"title":"How to check and monitor SSL certificates expiration with Telegraf","text":"As a developer or operator of a Website, the certificate expiration could happen and make the services not work. I’ll introduce how to monitor certificates like SSL,JKS,P12 using Telegraf. Certificates are broadly used for security reasons, they can be used within internal service or public service communication. The most common certificate is TLS used for verifying the identity of the HTTPS service. To increase security, the certificate will not be always valid because of expiration. To prevent the certificate expiry, we should rotate them periodically and meanwhile monitor them and alert if expired. Telegraf is a popular metric collecting tool to implement this. Overview for certificate types .csrCertificate Signing Request used to request a certificate from the certificate authority. .pemThis is a container format that may include just the public certificate or may include an entire certificate chain including public key, private key, and root certificates. Confusingly, it may also encode a CSR (e.g. as used here) as the PKCS10 format can be translated into PEM. .keyThis is a PEM formatted file containing just the private-key of a specific certificate and is merely a conventional name and not a standardized one. .pkcs12 .pfx .p12This is a passworded container format that contains both public and private certificate pairs. Unlike .pem files, this container is fully encrypted. Openssl can turn this into a .pem file with both public and private keys. .cert .cer .crtA .pem (or rarely .der) formatted file with a different extension, one that is recognized by Windows Explorer as a certificate, which .pem is not. .jksA Java KeyStore (JKS) is a repository of security certificates – either authorization certificates or public key certificates – plus corresponding private keys, used for instance in SSL encryption. Check certificate expiry time check the JKS expiry time check_jks.sh12# to check keystore.jks expiry timekeytool -list -v -keystore keystore.jks -storepass &quot;pass&quot; | grep until check the PKCS#12 expiry time check_p12.sh12# to check certicate.p12 expiry timeopenssl pkcs12 -in certicate.p12 -nokeys | openssl x509 -noout -enddate Customize telegraf pluginIn this case, we can use a bash script to collect the metrics and output it as influxDB line protocol, it does not need you to use influxDB, you can use any kind of monitoring backend that can read from telegraf, for example, Prometheus. Telegraf is a daemon that can be running on servers to collect system metrics, it supports multiple input plugins to collect metrics. intput.exec is an input plugin which will run the specified script, the output of the script will be treated as a data point. Bash script to generate the metricWe can write a bash script to generate an influxDB line formatted metric, the script will use openssl to resolve the certificate. This is a script used to resolve PKCS#12 files. generate_p12_metric.sh12345678910111213141516#!/bin/bashFILE_PATH=&quot;path-to-pkcs#12-cert&quot;P12_UNTIL=$(openssl pkcs12 -in $FILE_PATH -nokeys 2&gt;/dev/null | openssl x509 -text -noout 2&gt;/dev/null | grep After | sed 's/.*After : //' )# return 1 year when there's no existing fileif [ -z &quot;$P12_UNTIL&quot; ]then echo &quot;$((360*24*60*60))&quot; exit 0fiP12_UNTIL_EPOCH=$(date +%s --date=&quot;$P12_UNTIL&quot;)NOW_EPOCH=$(date +%s)echo &quot;pkcs12_cert,source=$FILE_PATH expiry=$(($P12_UNTIL_EPOCH-$NOW_EPOCH)) ${NOW_EPOCH}000000000&quot; Another script to resolve the JKS file generate_jks_metric.sh1234567891011121314FILE_PATH=&quot;path-to-jks-cert&quot;KEYSTORE_UNTIL=$(echo 'dummydummy' | keytool -list -v -keystore $FILE_PATH 2&gt;/dev/null | grep -i Until | sed 's/.*until: //')# This may be caused by unexistent file, return 1 year to skip checking.if [ -z &quot;$KEYSTORE_UNTIL&quot; ]then echo &quot;$((360*24*60*60))&quot; exit 0fiKEYSTORE_UNTIL_EPOCH=$(date +%s --date=&quot;$KEYSTORE_UNTIL&quot;)NOW_EPOCH=$(date +%s)echo &quot;jks_cert,source=$FILE_PATH expiry=$(($KEYSTORE_UNTIL_EPOCH-$NOW_EPOCH)) ${NOW_EPOCH}000000000&quot; X509 CertThere’s an X509 Cert Input Plugin already there. Telegraf configurationPut the jks_cert.conf under the telegraf’s configuration folder, restart telegraf and it will take effect. jks_cert.conf1234[[inputs.exec]] commands = [ &quot;/usr/local/bin/jks_certificate_metric.sh&quot; ] data_format = &quot;influx&quot; What’s nextConnect the data Telegraf collected to Time series database like Prometheus, InfluxDB, Graphite, and show them with Grafana. Reference What is a pem file and how does it differ from other OpenSSL generated key files? OpenSSL check p12 expiration date What is a PEM file and how does it differ from other OpenSSL generated key file pkcs#12 OpenSSL essentials Java keytool essentials Java KeyStore Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesHow does Prometheus query work? - Part 1, Step, Query and RangeGenerate monitoring dashboards & alertings using Grafana API","link":"/operation/how-to-check-and-monitor-tls-jks-certificates-with-telegraf/"},{"title":"How to set up a reasonable memory limit for Java applications in Kubernetes","text":"This article introduces some discovery of the Java memory usage in Kubernetes and how to set up a reasonable memory request/limit based on the Java heap requirement and the memory usage of the application. ContextThe trigger for me to look up the memory usage of Java applications in Kubernets is the increased OOM events in production, after investigation, it was not caused by JVM heap shortage, so I need to investigate where the non-heap memory goes.(If you’re not familiar with the OOM events, you can check the article “How to alert for Pod Restart &amp; OOMKilled in Kubernetes “) Container MetricsThere are several metrics for memory usage in Kubernetes, container_memory_rss (cadvisor)The amount of anonymous and swap cache memory (includes transparent hugepages). container_memory_working_set_bytes (cadvisor)The amount of working set memory, this includes recently accessed memory, dirty memory, and kernel memory. Working set is &lt;= “usage” and it equals to Usage minus total_inactive_file. resident set sizeIt is container_memory_rss + file_mapped (file_mapped is accounted only when the memory CGroup is owner of page cache) For Kubernetes, it depends on container_memory_working_set_bytes to oom-kill the container which exceeds the memory limit, we’ll use this metric in the following sections. Heap Usage &lt;&lt; Memory LimitAfter we noticed several OOMs in the production, it’s time to figure out the root cause. According to the JVM metrics, I found the heap size was way less than the Kubernetes memory usage, let’s check an sample, its memory usage upper limit is as high as 90% of the total memory size. initial and max heap size is 1.5G Set by XX:InitialHeapSize=1536m -XX:MaxHeapSize=1536m -XX:MaxGCPauseMillis=50Kubernetes request and memory limit is 3G set by deployment.yaml, 12345resources: limits: memory: 3Gi requests: memory: 3Gi In the monitoring dashboard, Kubernetes memory usage is close to 90% of the memory limit which is 2.7G In other words, the non-heap memory took 2.7G-1.5G = 1.2G. A close-up on Java memoryJVM contains heap and non-heap memory, let’s take a sample. The data is from an application with XX:InitialHeapSize=1536m -XX:MaxHeapSize=1536m -XX:MaxGCPauseMillis=50 HeapIf we set the max size of the heap, we can consider the upper limit is fixed and heap size is 1.5G, we can divide the heap memory into Eden, Survivor, Old regions if we were using G1 as our GC algorithm. We can check heap info by jcmd, as we can see the used heap 593M is way less than the committed size 1.5G, so we are good with the heap usage. 1234567bash-4.2$ jcmd 99 GC.heap_info99: garbage-first heap total 1572864K, used 608214K [0x00000000a0000000, 0x0000000100000000) region size 1024K, 255 young (261120K), 1 survivors (1024K) Metaspace used 82997K, capacity 85390K, committed 89168K, reserved 1126400K class space used 9600K, capacity 10640K, committed 11980K, reserved 1048576KNon-Heap Analysis with NMT To get more debug information, I enabled the native memory tracking (switch on Native memory tracking by adding XX:NativeMemoryTracking=[off | summary | detail] to the Java options) in the app, I chose detail to show more details of the memory usage. After the application ran for 5 days, the memory usage was increasing slowly to 78.29% of the memory limit (3G), namely 2.35G. Let’s use jcmd to show where the memory went, 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475bash-4.2$ jcmd 99 VM.native_memory99:Native Memory Tracking:Total: reserved=3597274KB, committed=2291546KB- Java Heap (reserved=1572864KB, committed=1572864KB) (mmap: reserved=1572864KB, committed=1572864KB)- Class (reserved=1128971KB, committed=91739KB) (classes #13083) ( instance classes #12390, array classes #693) (malloc=2571KB #40364) (mmap: reserved=1126400KB, committed=89168KB) ( Metadata: ) ( reserved=77824KB, committed=77188KB) ( used=73396KB) ( free=3792KB) ( waste=0KB =0.00%) ( Class space:) ( reserved=1048576KB, committed=11980KB) ( used=9601KB) ( free=2379KB) ( waste=0KB =0.00%)- Thread (reserved=73325KB, committed=7621KB) (thread #71) (stack: reserved=72960KB, committed=7256KB) (malloc=252KB #428) (arena=113KB #140)- Code (reserved=250907KB, committed=48115KB) (malloc=3219KB #15038) (mmap: reserved=247688KB, committed=44896KB)- GC (reserved=131736KB, committed=131736KB) (malloc=40392KB #703266) (mmap: reserved=91344KB, committed=91344KB)- Compiler (reserved=965KB, committed=965KB) (malloc=832KB #1061) (arena=133KB #5)- Internal (reserved=371646KB, committed=371646KB) (malloc=371614KB #2010417) (mmap: reserved=32KB, committed=32KB)- Other (reserved=899KB, committed=899KB) (malloc=899KB #70)- Symbol (reserved=18735KB, committed=18735KB) (malloc=16042KB #163964) (arena=2693KB #1)- Native Memory Tracking (reserved=46557KB, committed=46557KB) (malloc=513KB #7270) (tracking overhead=46044KB)- Arena Chunk (reserved=178KB, committed=178KB) (malloc=178KB)- Logging (reserved=4KB, committed=4KB) (malloc=4KB #187)- Arguments (reserved=24KB, committed=24KB) (malloc=24KB #493)- Module (reserved=208KB, committed=208KB) (malloc=208KB #1919)- Synchronizer (reserved=246KB, committed=246KB) (malloc=246KB #2070)- Safepoint (reserved=8KB, committed=8KB) (mmap: reserved=8KB, committed=8KB) As the output said, the total committed memory is 2.18G, committed heap size is the same as we specified, 1.5G. For the other sections, Internal took 362M (371646KB), GC took 128M (131736KB), Class took 89M (91739KB), Code took 47M (48115KB), Symbol took 18M (18735KB), Thread took 7M, … (Didn’t take Native Memory Tracking into account because it was the overhead of the tracing, not the real situation in production.) Since I set an NMT baseline early, we can run jcmd &lt;process-id&gt; VM.native_memory detail.diff to know which method consumed the memory. As the output below (full output), I omitted some sections which didn’t increase a lot. Compared to the baseline, the total committed memory increased 437M (448201KB), the Internal section increased the most, 361M (373425KB). 123456789101112131415161718192021222324252627282930313233Native Memory Tracking:Total: reserved=3599409KB +443893KB, committed=2293681KB +448201KB...- Code (reserved=250907KB +1KB, committed=48115KB +4321KB) (malloc=3219KB +1KB #15038 +444) (mmap: reserved=247688KB, committed=44896KB +4320KB)- GC (reserved=131889KB +31893KB, committed=131889KB +31893KB) (malloc=40545KB +31893KB #706519 +672802) (mmap: reserved=91344KB, committed=91344KB)...- Internal (reserved=373425KB +369869KB, committed=373425KB +369869KB) (malloc=373393KB +369869KB #2020176 +2003461) (mmap: reserved=32KB, committed=32KB) ...[0x00007ff2b5ab0cb1] GCNotifier::pushNotification(GCMemoryManager*, char const*, char const*)+0x71[0x00007ff2b5e099ce] GCMemoryManager::gc_end(bool, bool, bool, bool, GCCause::Cause, bool)+0x27e[0x00007ff2b5e0b95a] TraceMemoryManagerStats::~TraceMemoryManagerStats()+0x2a[0x00007ff2b5a537f8] G1CollectedHeap::do_collection_pause_at_safepoint(double)+0x8a8 (malloc=31508KB type=Internal +31303KB #672175 +667804)[0x00007ff2b5e091b9] GCStatInfo::GCStatInfo(int)+0x29[0x00007ff2b5ab0c8d] GCNotifier::pushNotification(GCMemoryManager*, char const*, char const*)+0x4d[0x00007ff2b5e099ce] GCMemoryManager::gc_end(bool, bool, bool, bool, GCCause::Cause, bool)+0x27e[0x00007ff2b5e0b95a] TraceMemoryManagerStats::~TraceMemoryManagerStats()+0x2a (malloc=168044KB type=Internal +166951KB #672175 +667804)... You can find out the memory in Internal was consumed by TraceMemoryManagerStats::~TraceMemoryManagerStats() which is related to GC, so it seems GC will create some GC data and the data size is slowly increasing. GC+Internal consumed 493M. So now we know where the non-heap memory goes. G1 Tuning?Java 11 uses G1 as the default GC algorithm, CMS (Concurrent Mark Sweep) is deprecated and Java 11 mentioned The general recommendation is to use G1 with its default settings, eventually giving it a different pause-time goal and setting a maximum Java heap size by using -Xmx if desired. So that means by using G1, a complicated configuration is not that necessary, you just need to make a wish and G1 will try its best to implement it. It also indicates a bit why G1 will consume more and more memory, it might gather some information about the memory behavior to optimize the memory allocation. To fit a Java application to the Kubernetes, we need to specify several things: InitialHeapSize and MaxHeapSizeSetting this is to limit the heap memory, setting them to the same value will reduce the heap resizing. You can either set the MaxHeapSize to a static value based on the usage or to a ratio of the memory limit like 50%, 60% depending on your real usage. So these two goals will conflict, in most cases, we set our goal of the pause time and G1 will configure based on the goal. MaxGCPauseMillisThe default value of it is 200, G1 will try to balance the throughout and the pause time based on this value. There are two directions in GC tuning:“Increase the throughout” means reducing the overall GC time.“Improve the GC pause time” means doing GC more frequently, for example, it will reduce the size of Young region (eden, survivor region) to trigger the young GC more often. Calculate the required memory based on monitoringAs the memory analysis showed, the required memory = the max heap size + JVM non-heap (GC, Metaspace, Code, etc.), considering the application might need more native memory when using JNI APIs, like java.util.zip.Inflater will allocate some native memory for (de)compression. It’s hard to give an exact memory limit at first, we can always start with a loose limit and leave more room for the non-heap. After the application runs in the production for some time and the metrics are in place, we can adjust the memory limit based on the monitoring data. To help the developers to realize if the memory limit is reasonable, we can set some thresholds for the application resource usage, if the app falls into these holes, we will generate some warnings to the developers. Memory Request is too high = 0.6 &gt;= mem_usage(p95) / mem_requestMemory Request is too low = 0.9 &lt;= mem_usage(p95) / mem_requestMemory Limit is too low = 0.85 &lt;= mem_usage(p95) / mem_limit The prometheus we use is memory usage P95 in last 7 days. 1234quantile_over_time( 0.95, max by (namespace, container) (container_memory_working_set_bytes{namespace=~&quot;&lt;namespace&gt;.*&quot;, container=&quot;&lt;container&gt;&quot;, pod=~&quot;&lt;pod&gt;-.+&quot;})[7d:]) You can put the metrics on the monitoring dashboard and trigger alerts of warning level to the responsive team and iterate the memory limit accordingly, when you have more data I think this can also be automated. ConclusionFor Java applications, we recommend set the max heap with either a static value or a reasonable ratio (40% ~ 60%) based on the heap usage, make sure to leave enough space for GC and other native memory usage. For other applications, we can set up the required memory based on the monitoring data, we should always give enough free memory for the application, a good start is the three limitations we set above. Recommended Posts(Driven byHexo Recommended Posts plugin)How to alert for Pod Restart & OOMKilled in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesImplement zero downtime HTTP service rollout on KubernetesHow does Prometheus query work? - Part 1, Step, Query and Range","link":"/operation/how-to-setup-java-application-memory-limit-in-kubernetes/"},{"title":"支持iOS9 Universal links遇到的问题","text":"记录为iOS9上的APP支持Universal links遇到的一些问题。 在Web服务器上传apple-app-site-association文件 必须支持HTTPS获取配置文件 文件名后不加.json后缀 必须在根目录下，例如官网地址为www.site.com，那么必须通过https://www.site.com/apple-app-site-association访问到该文件 details为列表，列表元素中的paths是数组，且下标越小匹配优先级越高 paths可以使用*匹配任何字符串，?来匹配单个字符 appID格式为{teamId}.{bundleId}12345678910111213141516171819{ &quot;applinks&quot;:{ &quot;apps&quot;:[], &quot;details&quot;:[ { &quot;appID&quot;:&quot;U9JEY66N6A.com.ss.iphone.article.News&quot;, &quot;paths&quot;:[&quot;/m/detail/*&quot;,&quot;/&quot;] }, { &quot;appID&quot;:&quot;U9JEY66N6A.com.ss.iphone.article.Explore&quot;, &quot;paths&quot;:[&quot;/m/detail/*&quot;,&quot;/&quot;] }, { &quot;appID&quot;:&quot;U9JEY66N6A.com.ss.iphone.article.NewsSocial&quot;, &quot;paths&quot;:[&quot;/m/detail/*&quot;,&quot;/&quot;] } ] }} 应用增加applinks支持 测试效果 Safari端可以长按链接，选择使用Web browser打开，或者使用Native APP打开，iOS会记录用户记录，下次用户点击链接，默认使用APP打开，反之，如果用户选择使用浏览器打开链接，下次点击链接默认用浏览器打开。所以，Safari会根据用户最近行为决定如何打开Universal Links，记住用户习惯。 微信内置浏览器不支持长按链接选择打开方式，默认直接使用APP打开。 引用 Support Universal Links iOS 9学习系列：打通 iOS 9 的通用链接（Universal Links）","link":"/operation/iOS9-universal-links/"},{"title":"How to install Spinnaker on CentOS 7","text":"Spinnaker doesn’t support installation on CentOS machine, this article introduces how to use Docker to install Spinnaker components on CentOS directly. As we know, according to the spinnaker’s official documentation, spinnaker provides a tool for installing the spinnaker cluster in Kubernetes cluster or debian/ubuntu bare metal machine, but there’s not an option to install it on CentOS or other common operating systems. Althogh the spinnaker provides a way to start Spinnaker with Docker Compose, but it’s out of date. So I created a new docker-compose project to quickstart a spinnaker cluster on any kind of os. QuickstartProvision a machine with at least 16GB memory and 4 cores, it may need more than this since my macbook pro(4cores, 16GB) was stuck when I started spinnaker. 12345sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composegit clone https://github.com/songrgg/spinnaker-composecd spinnaker-composedocker-compose up -d To access the spinnakerVisit http://localhost:9000 in browser if you run spinnaker on local machine. Otherwise you can use ssh to create tunnel on local machine. Spinnaker exposes two ports, 9000 for web, 8084 for api gateway. 12ssh -L 8084:localhost:8084 &lt;remote-host&gt;ssh -L 9000:localhost:9000 &lt;remote-host&gt; Add clouddriverYou need to edit config/clouddriver.yml to open multiple clouddrivers, it’s not easy to integrate since we don’t have the halyard to do this stuff, but if you already have the clouddriver.yml, it’s easier to make it work. Since clouddriver will need credentials, you need to modify the docker-compose.yml to mount the credential files on the docker containers. Recommended Posts(Driven byHexo Recommended Posts plugin)how to build the smallest docker image as fast as you canMy first Hackathon: bring Spinnaker to my companyRaspberry 3安装docker","link":"/operation/install-spinnaker-on-centos/"},{"title":"InfluxDB command cheatsheet","text":"This article is InfluxDB command cheatsheet about how to interact with influxDB server and query the metrics. The InfluxDB version I tested is v1.7.10 Connect &amp; StartConnect to InfluxDB server and select the database. 12345678910111213$ influx -host 127.0.0.1 -port 8086&gt; SHOW DATABASES;name: databasesname----_internal&gt; CREATE DATABASE test;&gt; USE test;Using database test&gt; --- fill the database with some points&gt; INSERT temperature,machine=unit42,type=assembly external=25,internal=37&gt; INSERT temperature,machine=unit43,type=assembly external=25,internal=37&gt; INSERT temperature,machine=unit43,type=not_assembly external=25,internal=37 Show everythingShow is a helpful command that will help you find all the schemas you may use. Show common information1234567891011121314&gt; --- list all databases&gt; SHOW DATABASES&gt; --- show all measurements&gt; SHOW MEASUREMENTS&gt; --- show measurements where machine tag = 'unit42'&gt; SHOW MEASUREMENTS WHERE &quot;machine&quot; = 'unit42'&gt; --- show measurements that start with 'temp'&gt; SHOW MEASUREMENTS WITH MEASUREMENT =~ /temp.*/&gt; --- show all running queries&gt; SHOW QUERIES&gt; --- show all retention policies on a database&gt; SHOW RETENTION POLICIES ON &quot;test&quot;&gt; --- show all users in InfluxDB&gt; SHOW USERS Show series12345678910111213141516171819&gt; --- show all series&gt; show series from temperaturekey---temperature,machine=unit42,type=assemblytemperature,machine=unit43,type=assemblytemperature,machine=unit43,type=not_assembly&gt; --- show series from machine unit42&gt; SHOW SERIES FROM temperature WHERE machine = 'unit42'key---temperature,machine=unit42,type=assembly&gt; -- show estimated cardinality of the series on current database&gt; SHOW SERIES CARDINALITY-- show estimated cardinality of the series on specified database&gt; SHOW SERIES CARDINALITY ON mydbcardinality estimation----------------------3 Show tag1234567891011121314151617181920&gt; --- show tag keys&gt; SHOW TAG KEYSname: temperaturetagKey------machinetype&gt; --- show all tag keys from the temperature measurement&gt; SHOW TAG KEYS FROM &quot;temperature&quot;&gt; --- show all tag keys where the machine key = 'unit42'&gt; SHOW TAG KEYS WHERE &quot;machine&quot; = 'unit42'&gt; --- show all tag values across all measurements for the machine tag&gt; SHOW TAG VALUES WITH KEY = &quot;machine&quot;name: temperaturekey value--- -----machine unit42machine unit43&gt; --- show tag values for a specific database and measurement&gt; SHOW TAG VALUES ON test FROM temperature WITH KEY = &quot;machine&quot; Show field123456&gt; SHOW FIELD KEYS ON testname: temperaturefieldKey fieldType-------- ---------external floatinternal float Show cardinality12345678&gt; SHOW MEASUREMENT CARDINALITYcardinality estimation----------------------1&gt; SHOW MEASUREMENT EXACT CARDINALITY ON testcount-----1 Measurement12345678910&gt; --- create a temperature point&gt; INSERT temperature,machine=unit42,type=assembly external=26,internal=38&gt; --- select temperature for unit42&gt; SELECT * FROM temperature WHERE &quot;machine&quot; = 'unit42'&gt; --- select specific fields and tags from measurement, NOTE: at least one field must be included&gt; SELECT &quot;internal&quot;::field, &quot;machine&quot;::tag FROM temperature WHERE &quot;machine&quot; = 'unit42'&gt; --- delete metrics from temperature measurement&gt; DELETE FROM &quot;temperature&quot; WHERE time &lt; '2000-01-01T00:00:00Z'&gt; --- drop the temperature measurement&gt; DROP MEASUREMENT &quot;temperature&quot; Query analysis123456789101112&gt; --- explain the logic behind the query&gt; EXPLAIN SELECT * FROM temperatureQUERY PLAN----------EXPRESSION: &lt;nil&gt;AUXILIARY FIELDS: external::float, internal::float, machine::tag, type::tagNUMBER OF SHARDS: 1NUMBER OF SERIES: 3CACHED VALUES: 0NUMBER OF FILES: 6NUMBER OF BLOCKS: 6SIZE OF BLOCKS: 204 Reference InfluxDB 1.7 Query language","link":"/operation/influxdb-command-cheatsheet/"},{"title":"解决Jedis数据读取乱码问题","text":"现象同一套代码，同一个数据源，不同的操作系统，在OSX上数据提取编码正常，而Ubuntu上拉取数据乱码，数据拉取代码如下。 123456789101112131415161718192021222324252627@Overridepublic List&lt;String&gt; mget(String... fields) { List&lt;byte[]&gt; arrFields = new ArrayList&lt;&gt;(); for (String field : fields) { arrFields.add(field.getBytes()); } List&lt;byte[]&gt; results = redisTemplate.execute( (RedisCallback&lt;List&lt;byte[]&gt;&gt;) connection -&gt; connection.mGet(arrFields.toArray(new byte[arrFields.size()][])) ); if (results == null) { return new ArrayList&lt;&gt;(); } final List&lt;String&gt; ret = new ArrayList&lt;&gt;(); results.forEach(result -&gt; { if (result != null) { try { ret.add(new String(result)); } catch (UnsupportedEncodingException e) { ret.add(null); } } else { ret.add(null); } }); return ret;} 检查了Redis存储的数据都是UTF-8之后，怀疑是不同OS的编码有所不同，所以对提取的数据指定了编码格式，即new String(data, &quot;UTF8&quot;);，解决了问题。 Recommended Posts(Driven byHexo Recommended Posts plugin)How to host Swagger documentation using yaml/json configuration files?Spring data redis的一个bugSpring boot实现数据库读写分离","link":"/programming/jedis-utf8/"},{"title":"Istio Version Control On Kubernetes","text":"Istio has been adopted as a common implementation of service mesh, since more and more companies want to bring Istio into production, the version control of Istio seems a significant problem to solve. Version control is necessary as Istio components can be treated as the equivalent RPC services like our business services, we need to have an understanding of which version we are using now and what does the next version bring. And some Istio components can cooperate with the others, if we need to upgrade one component we need to upgrade the other components too. Although the Istio community provides the Istio upgrade method, we don’t actually want to upgrade such a whole thing in one move, it influences so much that we don’t want to risk. HelmHelm is package control system for Kubernetes, it’s more like the complicated version of brew on Mac. It uses Yaml and Golang templates to formulate a complete application configuration on Kubernetes. A lot of applications are packaged as Helm charts, we can build a complicated application by declaring the dependencies between these charts. Istio + HelmIstio project contains the Helm configuration itself, I recommend the users to extract the Helm brought by Istio into a standalone Git repository. Like the other services, we use Git to control each component’s configuration and track every modification. CI/CDAs more and more people advocate infrastructure as code, we not only store the Istio Helm as a Git repository, we bring the CI/CD to the infrastructure. If three Kubernetes clusters: test, stage, production exist, we can setup CI/CD to test cluster based on the specified Git test branch, each commit will trigger the generation of the latest Istio K8S Yaml config and apply them to the test cluster, after the tests on test environment, we brought the Istio updates to the stage and production in order. Still worried about Production?Although we have tested in the test environment, we still worry if it works in the production and a method to let you know what actually changes is to diff the K8S configuration between the latest config and the current K8S config.I recommend the kubediff tool to distinguish the differences. Conclusion Every updates to Istio configuration need to be tracked by Git. Like the common services, deploy the change to test environment as soon as possible to fail fast. Use kubediff to show the changes. Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesImplement zero downtime HTTP service rollout on Kubernetes","link":"/operation/istio-version-control-on-k8s/"},{"title":"levelDB里缓存实现","text":"leveldb的缓存机制 leveldb采用LRU机制, 利用键的哈希值前n位作为索引, 将要插入的键值对分派到指定的缓存区, 当缓存区的使用率大于总容量后, 优先淘汰最近最少使用的缓存, 独立的缓存区总量为2^n . 初始化ShardedLRUCache 设置初始缓存容量, 并设置16个子分区的容量.123456789static const int kNumShardBits = 4;static const int kNumShards = 1 &lt;&lt; kNumShardBits;explicit ShardedLRUCache(size_t capacity) : last_id_(0) { const size_t per_shard = (capacity + (kNumShards - 1)) / kNumShards; for (int s = 0; s &lt; kNumShards; s++) { shard_[s].SetCapacity(per_shard); }} 新建缓存 由key的hash值的前kNumShardBits位作为子缓存区索引, 默认为前4位的索引位, 索引到指定的区, 子缓存区LRUCache接管新建缓存的任务.1234567class ShardedLRUCache : public Cache { ... virtual Handle* Insert(const Slice&amp; key, void* value, size_t charge, void (*deleter)(const Slice&amp; key, void* value)) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } LRUCache结构1234567891011121314151617181920212223242526272829303132333435// A single shard of sharded cache.class LRUCache { public: LRUCache(); ~LRUCache(); // Separate from constructor so caller can easily make an array of LRUCache void SetCapacity(size_t capacity) { capacity_ = capacity; } // Like Cache methods, but with an extra &quot;hash&quot; parameter. Cache::Handle* Insert(const Slice&amp; key, uint32_t hash, void* value, size_t charge, void (*deleter)(const Slice&amp; key, void* value)); Cache::Handle* Lookup(const Slice&amp; key, uint32_t hash); void Release(Cache::Handle* handle); void Erase(const Slice&amp; key, uint32_t hash); private: void LRU_Remove(LRUHandle* e); void LRU_Append(LRUHandle* e); void Unref(LRUHandle* e); // Initialized before use. size_t capacity_; // mutex_ protects the following state. port::Mutex mutex_; size_t usage_; // Dummy head of LRU list. // lru.prev is newest entry, lru.next is oldest entry. LRUHandle lru_; HandleTable table_;}; LRUCache才是真正具有缓存功能的结构, capacity_表示它的最大容量, mutex_规范各线程互斥地访问, usage_标志缓存中已用容量, lru_作为哑节点, 它的前节点是最新的缓存对象, 它的后节点是最老的缓存对象, table_是用来统计对象是否被缓存的哈希表. LRUCache缓存生成 缓存的基础节点是LRUHandle, 储存节点的(键, 值, 哈希值, next, prev节点, 销毁处理函数等)信息. 综合上面的LRUCache结构也看到, 实际上, 缓存节点是双向列表存储, LRUHandle lru_这个哑节点用来分隔最常更新的节点和最不常更新节点. 当要将缓存节点插入缓存区, 先由哈希表判断缓存是否已存在, 若存在, 将其更新至最常更新节点; 若不存在, 则插入为最常更新节点. 同时更新哈希表HandleTable table_. 这里的HandleTable实现并没特殊之处, 我看是采用键哈希策略进行哈希, 如果键冲突则以链表进行存储. 利用二级指针对链表进行插入. 123456789101112131415LRUHandle* Insert(LRUHandle* h) { LRUHandle** ptr = FindPointer(h-&gt;key(), h-&gt;hash); LRUHandle* old = *ptr; h-&gt;next_hash = (old == NULL ? NULL : old-&gt;next_hash); *ptr = h; if (old == NULL) { ++elems_; if (elems_ &gt; length_) { // Since each cache entry is fairly large, we aim for a small // average linked list length (&lt;= 1). Resize(); } } return old;} 二级指针在HandleTable这个哈希表实现里, 有个FindPointer方法用来查找此对象是否已存在, 并返回LRUHandle**, 代码如下: 1234567891011// Return a pointer to slot that points to a cache entry that// matches key/hash. If there is no such cache entry, return a// pointer to the trailing slot in the corresponding linked list.LRUHandle** FindPointer(const Slice&amp; key, uint32_t hash) { LRUHandle** ptr = &amp;list_[hash &amp; (length_ - 1)]; while (*ptr != NULL &amp;&amp; ((*ptr)-&gt;hash != hash || key != (*ptr)-&gt;key())) { ptr = &amp;(*ptr)-&gt;next_hash; } return ptr;} 可见, 如果已存在节点, 则返回这个节点的LRUHandle**; 如果不存在, 返回的是可以保存这个LRUHandle*的地址. LRUCache缓存查找 如果缓存存在, 则将其放置到哑节点lru_的prev位置, 即最近使用节点, 相当于提升它的优先级, 便于下次快速查找; 如果不存在这个缓存, 返回NULL.小结看完leveldb的缓存实现, 在实现思路上, 是传统的lru算法, 使用哈希表判重, 根据缓存的请求时间提升缓存的优先级. 里面的细节例如使用哈希值的前n位进行路由, 路由到2^n个独立的缓存区, 各个缓存区维护自己的mutex进行并发控制; 哈希表在插入节点时判断空间使用率, 并进行自动扩容, 保证查找效率在O(1). Recommended Posts(Driven byHexo Recommended Posts plugin)levelDB里integer编码levelDB里自定义的状态levelDB里skiplist的实现代码赏析","link":"/programming/levelDB-cache/"},{"title":"PHP面试+怀旧","text":"面试今天去面试了一家公司, 跟自己说面完要写个小结, 记录这次面试的前后历程. 这次面试的缘起是因为团队有较大变动, 也开始重新审视自己, 正是在这个时候, 发现自己所处的环境还是非常“危险”的, 是指的自己处于一种不温不火的酱油状态, 整天游离在完成工作和无聊消沉的独处中, 并且成为了温吞吞的人, 于是发现现在的环境实在不是特别适合自己当初要进来的初衷, 突然渴望那种每天都在蜕变, 每天都在痛并快乐着的成长日子, 所以准备找面试机会, 看看是否能找到适合自己想法的机会. XX公司给了个面试机会, 自己还是非常重视的, 恰逢清明节不回家, 于是在上海的小房子里进行一波准备. 准备工作由于面的是php岗位, 网站是日pv千万级的电商, 所以准备的东西还是围绕高性能web开发. 我的背景做过的都是企业级的web开发, 所以没有特别多的性能优化, 项目中用过C#, JAVA, PHP进行实际开发, PHP开发经验是半年, 但是胜在基础好, 大学里尽写c了, 也学过scheme, 所以基本上就触类旁通, 语言这些东西都不是大问题, 但一些PHP的细节, 可能不太清楚, api不太熟等. PHP熟悉PHP开发中与Web开发相关的细节, 主要是和HTTP关联的一些内容(如$_GET, $POST, $_FILES变量的意义等), 一些常用的数据结构实现(如array的实现机制, 如何保证哈希和链表特性), 数据库操作相关函数(mysql_query, mysql_connect等). MySQL数据库是常见的性能瓶颈, 所以对于数据库的深入理解是必要的, 《高性能MySQL》是本入门好书, 专门看了下它的第[1,5,6,7]章, 分别是MySQL的概览, 表的创建, 索引建立, 查询解析原理等, 我比较关注innodb引擎, 所以关注了它的实现机制, 什么是聚簇索引, 它和MyISAM的存储方式有何不同, 什么情况下会使用覆盖索引, 什么时候索引会被使用, 排序是否能享受索引……等等. 最主要的是理解innodb的存储结构B+树之后, 就会懂得为什么索引在遇见范围查询后就失效, 而复合索引的顺序也能成为区分索引的一个元素, explain是如何帮助分析查询的效能, 太多以前没注意的点在这个时候突然醒悟. Javascript说实话, 自己对前端一直是属于能上手, 但费劲, JS就够喝一壶了, 不是说难, 是没把它严肃对待, 可能和自己年少时觉得前端太没意思, 属于下乘有关, 呵呵, 年轻的时候, 这种傻傻的思想还是挺多的… 这两天看了下this的意义, prototype和__proto__的关系这些基础的知识, 然后在leetcode上做题也用JS提交, 这两天做水题, 所以代码也不是特别多. 当然项目经验里也有jQuery, AngularJS这些工具的使用, 但认真写JS还是从AngularJS开始, 当初用jQuery的时候大多时间为了赶进度, 所以没有将很多代码模块化起来, 继承jQuery做成公共库, 现在想想真是蠢, 为了这么点进度, 放下自己的节操… AngularJS还是对前端开发的一大进步, 它强迫你用它的种种特性, 如双向绑定, service, directive等, 让你站在抽象层上考虑实现, 而不是整天想着用jQuery操作这个那个dom. 所以在微信端开发的时候, 比较开心. HTML+CSS这个没怎么看. 性能优化方面性能优化, 说实话, 最重要的, 第一是经验, 第二是思路, 第一我不具备, 但思路可以站在前辈们的肩膀上, 网站的性能优化, 如果要到骨子里, 必须把自己想象成数据, 想象成报文, 从客户端(浏览器)出发, 从生成请求, 经过网络到达服务器(服务器可能反向代理), 读取请求, 分析请求, 生成报文, 返回到客户端. 每个位置都可以进行优化, 在浏览器端可以通过HTTP报文的Last-Modified-Since, Etag, Expired属性进行缓存协商, 使用浏览器缓存, 将请求量减少甚至消除请求; 服务器读取请求, 涉及到服务器模型, 是基于epoll,kqueue还是select, 最大支持连接数, 并发量大时, nginx作反向代理, 负载均衡, 分摊服务器压力; 分析请求是服务器和后端脚本语言共同完成, 如果是请求静态文件是不是直接就返回, 如果是动态内容, 会解释脚本, 在解释脚本时是否考虑使用opcode缓存优化, 在解释时尽量少引入不需要的文件, 造成额外的解释压力; 分析请求里就有关于动态内容, 对于数据库和其它资源文件的访问优化, 以及最后的动态内容缓存和过期配置. 其实优化就是找到其中的某一环节, 一一解决, 细节只有在真正遇到, 并解决才会有真正的收获. 今天的面试体验今天和lead聊了聊, 基本聊的就是以上的内容, 数据库确实是非常重要的, 特别是面对这样的访问量以及查询场景, 经常要查询销量前十的产品, 根据用户搜索进行内容查询, 这种情况下索引要建的好. 不过数据大多是多在用户的购买记录吧, 估计得分库分表了. 最后lead语重心长地道出了”项目到后来要思考的都是哲学问题啊”, 这点我还是挺赞同的, 在我有限的设计程序结构的经验上, 我最分裂的情况都是如果要拓展应该怎么设计架构, 才能易于拓展, 我这代码写的够健壮吗? 那些奇葩的scenario都覆盖了吗? 用的人会觉得好用吗? 我自己哪? 这个产品的思想是什么? 我对这个思想是报什么态度的, 有对错吗? 之后还有家!!公司的面试, 想着还是尽快面了吧, 自己的初衷是为了能认真的提高自己, 不管如何还是要坚定这一想法, 勿忘初衷啊!!!!!!!!!!!!!!!!!!!!!!!!!!!! 插曲前两天看到躺在自家的ipv, 想着和它的快乐日子, 于是决定拯救它于水火之中, 不就是硬盘坏了吗, 换一个就是了, 看到网上有人卖的ssd, 就入了一块, 今天货到了, 就给它换上, 可惜充电线没有, 得到公司才能看到效果了, 拆机似难实易, 换好还是挺高兴的. 上纪念图. Recommended Posts(Driven byHexo Recommended Posts plugin)笔试面试带来什么？","link":"/life/interview-and-my-ipv/"},{"title":"levelDB里skiplist的实现代码赏析","text":"跳表的原理就是利用随机性建立索引，加速搜索，并且简化代码实现难度。具体的跳表原理不再赘述，主要是看了levelDB有一些实现细节的东西，凸显自己写的实现不足之处。 去除冗余的key 1234567891011121314151617181920212223242526272829303132333435template&lt;typename Key, class Comparator&gt;struct SkipList&lt;Key,Comparator&gt;::Node { explicit Node(const Key&amp; k) : key(k) { } Key const key; // Accessors/mutators for links. Wrapped in methods so we can // add the appropriate barriers as necessary. Node* Next(int n) { assert(n &gt;= 0); // Use an 'acquire load' so that we observe a fully initialized // version of the returned Node. return reinterpret_cast&lt;Node*&gt;(next_[n].Acquire_Load()); } void SetNext(int n, Node* x) { assert(n &gt;= 0); // Use a 'release store' so that anybody who reads through this // pointer observes a fully initialized version of the inserted node. next_[n].Release_Store(x); } // No-barrier variants that can be safely used in a few locations. Node* NoBarrier_Next(int n) { assert(n &gt;= 0); return reinterpret_cast&lt;Node*&gt;(next_[n].NoBarrier_Load()); } void NoBarrier_SetNext(int n, Node* x) { assert(n &gt;= 0); next_[n].NoBarrier_Store(x); } private: // Array of length equal to the node height. next_[0] is lowest level link. port::AtomicPointer next_[1];}; 这里使用一个Node节点表示所有相同key，不同高度的节点集合，仅保留了key和不同高度的向右指针，并且使用NewNode来动态分配随即高度的向右指针集合，而next_就指向这指针集合。这也是c/c++ tricky的地方。 12345678910111213141516#include &lt;stdio.h&gt;struct Node { char str[1];};int main() { char* mem = new char[4]; for (int i = 0; i &lt; 4; i++) { mem[i] = i + '0'; } Node* node = (Node*)mem; char* const pstr = node-&gt;str; for (int i = 0; i &lt; 4; i++) { printf(&quot;%c&quot;, pstr[i]); } return 0;} 就像上面这个简单的sample，成员str可以作为指针指向从数组下标0开始的元素，并且不受申明时的限制，不局限于大小1，索引至分配的最大的内存地址。 简易随机数生成 123456789101112131415161718192021uint32_t Next() { static const uint32_t M = 2147483647L; // 2^31-1 static const uint64_t A = 16807; // bits 14, 8, 7, 5, 2, 1, 0 // We are computing // seed_ = (seed_ * A) % M, where M = 2^31-1 // // seed_ must not be zero or M, or else all subsequent computed values // will be zero or M respectively. For all other values, seed_ will end // up cycling through every number in [1,M-1] uint64_t product = seed_ * A; // Compute (product % M) using the fact that ((x &lt;&lt; 31) % M) == x. seed_ = static_cast&lt;uint32_t&gt;((product &gt;&gt; 31) + (product &amp; M)); // The first reduction may overflow by 1 bit, so we may need to // repeat. mod == M is not possible; using &gt; allows the faster // sign-bit-based test. if (seed_ &gt; M) { seed_ -= M; } return seed_;} 可以看到，他使用A和M对种子进行运算，达到一定数据范围内不会重复的数集，而里面对于(product % M)，使用(product &gt;&gt; 31) + (product &amp; M)进行运算优化，考虑右移和与操作的代价远小于取余操作。 简洁清晰的私有帮助方法，帮助寻找小于指定key的节点1234567891011121314151617181920template&lt;typename Key, class Comparator&gt;typename SkipList&lt;Key,Comparator&gt;::Node*SkipList&lt;Key,Comparator&gt;::FindLessThan(const Key&amp; key) const { Node* x = head_; int level = GetMaxHeight() - 1; while (true) { assert(x == head_ || compare_(x-&gt;key, key) &lt; 0); Node* next = x-&gt;Next(level); if (next == NULL || compare_(next-&gt;key, key) &gt;= 0) { if (level == 0) { return x; } else { // Switch to next list level--; } } else { x = next; } }} Recommended Posts(Driven byHexo Recommended Posts plugin)levelDB里缓存实现levelDB里integer编码levelDB里自定义的状态","link":"/programming/levelDB-skiplist/"},{"title":"levelDB里integer编码","text":"leveldb的数字编码方法 EncodeFixed32按照机器的字节存储顺序(大端, 小端), 原样存储到一块32位内存 EncodeFixed64按照机器的字节存储顺序(大端, 小端), 原样存储到一块64位内存 PutFixed32将EncodeFixed32处理过的内存块追加到目的地址 PutFixed64同PutFixed32 EncodeVarint32将32位的无符号整数以7个有效位, 1个标志位的形式编码. 对于Varint这种整数编码, 是为了减短数值较小的整数所占的存储空间. 实现方法是每8位, 低7位存储有效数值和高1位存储标记位(用来标记是否还有后续的数值), 以127和128这两个数为例:127 =&gt; 0111 1111第一个字节低7位是有效数值127, 最高位位0, 所以表示没有后续的数值, 于是算出它所代表的数值是127. 128 =&gt; 1111 1111 0000 0001第一个字节低7位是有效数值127, 最高位位1, 所以表示有后续的数值, 相同计算方法算出后续值1, 最高位标志0表示结束, 127 + (1&lt;&lt;7) = 128, 因为每8位实际表示的是7位, 所以要将1左移7位, 若有更多位, 计算方式相同. leveldb里的32位编码实现:123456789101112131415161718192021222324252627char* EncodeVarint32(char* dst, uint32_t v) { // Operate on characters as unsigneds unsigned char* ptr = reinterpret_cast&lt;unsigned char*&gt;(dst); static const int B = 128; if (v &lt; (1&lt;&lt;7)) { *(ptr++) = v; } else if (v &lt; (1&lt;&lt;14)) { *(ptr++) = v | B; *(ptr++) = v&gt;&gt;7; } else if (v &lt; (1&lt;&lt;21)) { *(ptr++) = v | B; *(ptr++) = (v&gt;&gt;7) | B; *(ptr++) = v&gt;&gt;14; } else if (v &lt; (1&lt;&lt;28)) { *(ptr++) = v | B; *(ptr++) = (v&gt;&gt;7) | B; *(ptr++) = (v&gt;&gt;14) | B; *(ptr++) = v&gt;&gt;21; } else { *(ptr++) = v | B; *(ptr++) = (v&gt;&gt;7) | B; *(ptr++) = (v&gt;&gt;14) | B; *(ptr++) = (v&gt;&gt;21) | B; *(ptr++) = v&gt;&gt;28; } return reinterpret_cast&lt;char*&gt;(ptr);} 用了5个if来判断这个数落在哪个数值区间里, 32位的数值使用变长编码能使用1-5位来表示 leveldb里的64位编码1234567891011121314151617181920212223// leveldb implementationchar* EncodeVarint64(char* dst, uint64_t v) { static const int B = 128; unsigned char* ptr = reinterpret_cast&lt;unsigned char*&gt;(dst); while (v &gt;= B) { *(ptr++) = (v &amp; (B-1)) | B; v &gt;&gt;= 7; } *(ptr++) = static_cast&lt;unsigned char&gt;(v); return reinterpret_cast&lt;char*&gt;(ptr);}// My own implementationchar* EncodeVarint64(char* dst, uint64_t v) { unsigned char* ptr = reinterpret_cast&lt;unsigned char*&gt;(dst); static const int B = 128; int flag = 1; for (int offset = 0; flag &gt; 0 &amp;&amp; offset &lt; 8; offset++) { *(ptr++) = ((v &gt;&gt; (offset * 7)) &amp; 127) | B; flag = v &gt;&gt; (offset * 7); } return reinterpret_cast&lt;char*&gt;(ptr);} 比较了我和leveldb的实现之后, 发现leveldb更简洁高效, 是因为我的实现维护了一个offset来记录当前的偏移, flag用来记录是否继续计算, 每次都需要维护这个offset和flag, leveldb直接使用v作为变量, 通过每次对它的修改, 来记录是否继续, 同时它只需要用低7位, 省去了记录偏移的麻烦, 简洁有力! Recommended Posts(Driven byHexo Recommended Posts plugin)levelDB里缓存实现levelDB里自定义的状态levelDB里skiplist的实现代码赏析","link":"/programming/leveldb-integer-coding/"},{"title":"Linux namespace in Go - Part 1, UTS and PID","text":"This article starts some Golang experiments on Linux namespace and provides context for Container technology. Linux namespace is an important foundation of container technology, it provides lightweight isolation between processes with Linux kernel support, therefore, different services can share the same machine with better resource utilization, great security. The series of Linux namespace in Go: Linux namespace in Go - Part 1, UTS and PID Linux namespace in Go - Part 2, UID and Mount Linux namespace in Go - Part 3, Cgroups resource limit Linux namespaceThere’s a definition from Linux manual introducing Linux namespace: A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. So, Linux namespace is the key that we can control the resources the processes can access. Namespace typesWhat kind of isolation could we control is decided by the namespace types. UTSHostname and NIS domain name CgroupControls the system resources (like CPU, Memory…) the process can use. IPCPOSIX message queues NetworkNetwork devices, stacks, ports, etc. MountMount points PIDProcess IDs TimeBoot and monotonic clocks UserUser and group IDs “Go” through these typesNote that Linux namespace is only available in Linux distributions, I use Ubuntu 20.04 and Golang 1.14.2 here to run the experiments. If you’re using other OS, you might find the Linux namespace libraries missing, go and find a Linux machine and Ubuntu is recommended. Note: The experiments code can be found in https://github.com/songrgg/namespace-demo UTS NamespaceUTS will isolate the hostname for the forked process from its caller. 12345678910111213141516171819202122232425// folder v1package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;os/exec&quot; &quot;syscall&quot;)func main() { exec.Command(&quot;/bin/bash&quot;) cmd := exec.Cmd{ Path: &quot;/bin/bash&quot;, Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, SysProcAttr: &amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS, }, } if err := cmd.Run(); err != nil { fmt.Println(err) }} This script needs sudo permission, run sudo go run main.go and it will create a new bash process with a new UTS namespace, you could modify hostname within this namespace and it won’t change the outside’s hostname. 123456789101112[sojiang@ namespace-demo]$ hostnamesojiang.local[sojiang@ namespace-demo]$ sudo go run exercise01/main.go[root@ namespace-demo]# hostnamesojiang.local[root@ namespace-demo]# hostname test.local[root@ namespace-demo]# hostnametest.local[root@ namespace-demo]# exitexit[sojiang@ namespace-demo]$ hostnamesojiang.local PID namespacePID namespace would create a new namespace for the process where the process ID is the same as the parent process, but note that you can only operate the processes under your namespace and can’t operate the processes in the parent namespace, in the opposite, the parent namespace has permission to operate the processes under the child namespaces. Create a PID namespace simply by adding a CLONE_NEWPID flag: 123SysProcAttr: &amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID,}, Run the process again, 123456[sojiang@ namespace-demo]$ sudo go run exercise02/main.go[root@ namespace-demo]$ ps -efsojiang 6820 4062 0 06:44 ? /bin/zsh -iroot 7561 ... sudo go run exercise02/main.go[root@ namespace-demo]$ kill -9 6820bash: kill: (6820) - No such process In the ps -ef output, we could see zsh which runs in the parent namespace and go run exercise02/main.go is running in the process’s namespace. We call the parent namespace P and the child namespace C, if we run sleep 100 in P, use ps -ef to get the process id and run kill -9 &lt;process-id&gt; in C, it will output “process not exist”. In the opposite, we could kill the process in C, that’s because the process visibility is in a single direction, only parent namespace could see all the processes in both P and C. Like the following picture, pid 1 is in the parent namespace of pid Namespace x, so pid 1 could see all the processes, pid 3 could only see pid 3, pid 5 and pid 5. What’s next?Here I did experiments on Linux UTS and PID namespaces, we know the isolation mechanism of them. I still have several questions, How to run the program as other user instead of root? I can still see the process list by ps -ef in the child namespace, however, most of the processes are in the parent namespace, there’s no need for me to see them, how to hide them or have my own process list? The answer is in the Linux namespace in Go - Part 2, UID and Mount. Reference Linux Namespaces by Ed King Linux Programmer’s Manual Namespaces(7) Golang exec package Recommended Posts(Driven byHexo Recommended Posts plugin)Linux namespace in Go - Part 3, Cgroups resource limitLinux namespace in Go - Part 2, UID and Mount","link":"/programming/linux-namespace-part01-uts-pid/"},{"title":"levelDB里自定义的状态","text":"LevelDB状态码LevelDB里专门定义了Status类，自定义了常见的状态码，用于函数的返回。 状态码类型在Status类中以枚举类型定义Code： 0 =&gt; 处理成功1 =&gt; 未找到2 =&gt; 崩溃3 =&gt; 不支持4 =&gt; 无效的参数5 =&gt; IO错误 并且有6个静态方法用来生成对应状态码的Status对象, 和4个判断方法来方便确认状态。 存储方式状态码包括状态信息都是利用私有字符串变量state_存储，存储方式如下： 0 ~ 4字节：消息长度(消息1的长度len1+分割符的长度+消息2的长度len2)4 ~ 5字节：状态码5 ~ 5+len1：消息1的长度(若消息2不为空)5+len1 ~ 7+len1：分隔符的长度(分隔符为2个字符，分别为冒号和空格)7+len1 ~ 7+len1+len2：消息2的长度 这种存储方式，另得所有操作都是基于这个state_变量的下标索引操作。 小结定义异常在其它代码遇到错误时可以抛出适当的异常, 这样的实现方式我认为好处是初始化，销毁和复制操作简单，并且这些内容在内存中的位置紧密，有利于缓存，不知道是不是这样考虑才这么做的，但分成3个变量也无伤大雅。 Recommended Posts(Driven byHexo Recommended Posts plugin)levelDB里缓存实现levelDB里integer编码levelDB里skiplist的实现代码赏析","link":"/programming/leveldb-status/"},{"title":"Linux namespace in Go - Part 2, UID and Mount","text":"In the previous article, I did two experiments on what isolation it brings with PID and UTS, this article explains UID and Mount namespace. The series of Linux namespace in Go: Linux namespace in Go - Part 1, UTS and PID Linux namespace in Go - Part 2, UID and Mount Linux namespace in Go - Part 3, Cgroups resource limit UID namespaceAs Linux man page described, User namespaces isolate security-related identifiers and attributes,in particular, user IDs and group IDs (see credentials(7)), the root directory, keys (see keyrings(7)), and capabilities (see capabilities(7)). A process’s user and group IDs can be different inside and outside a user namespace. In particular, a process can have a normal unprivileged user ID outside a user namespace while at the same time having a user ID of 0 inside the namespace; in other words, the process has full privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace. The key point is that the unprivileged user outside the namespace can be mapped to the root-user inside the new namespace by creating a UID namespace with UID mappings. Let’s take an example, on my Ubuntu I want to create a user namespace and use non-root user to run the process, as tested in my previous article, I can’t run the Go program without root permission. But with user namespace, I can map non-root user, in this case, 12$ iduid=1000(srjiang) gid=1000(srjiang) groups=1000(srjiang), to the root user in the container, 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;os/exec&quot; &quot;syscall&quot;)func main() { cmd := exec.Command(&quot;/bin/bash&quot;) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr cmd.SysProcAttr = &amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWUSER, UidMappings: []syscall.SysProcIDMap{ { ContainerID: 0, HostID: os.Getuid(), Size: 1, }, }, GidMappings: []syscall.SysProcIDMap{ { ContainerID: 0, HostID: os.Getgid(), Size: 1, }, }, } if err := cmd.Run(); err != nil { fmt.Printf(&quot;Error running the exec.Command - %s\\n&quot;, err) os.Exit(1) }} UidMappings implements the user mapping between host and container, it will map the user id in the host to the user id in the container, the size parameter indicates it’s a contiguous range mapping. If size is 10 and containerID is 0, HostID is 1000, it means 1000-1010 will be mapped to 0-10. GidMappings is the same mechanism as UidMappings, it represents Group id. Now, we’re root in the container but non-root in the host, how does the permission look like in the container? 1234(container) # iduid=0(root) gid=0(root) groups=0(root),65534(nogroup)(container) # touch /testmypermissiontouch: cannot touch '/testmypermission': Permission denied You can find out that although I’m the root user in the container, I still don’t have the permission tocreate a file under root directory, because to the host, I’m actually a non-root user srjiang, I can only manipulate the files that srjiang can. If I run the golang program with sudo, I can operate on the root directory as I wish. What about ps -ef?Remember in the previous article, I left a question how to list the processes only visible within this namespace, here comes the answer: mount a new /proc. The proc filesystem is a pseudo-filesystem which provides an interface to kernel data structures. It is commonly mounted at /proc.As name explained, the process information is stored under /proc folder, most of the files in the proc filesystem are read-only, they’re dynamic and stored in memory. By default, everybody may access all /proc/[pid] directories, besides process information you can also update the process configuration. To isolate the container’s process list from the host, we need to mount a new /proc directory instead of sharing the host’s /proc, we can implement this by 1(container) # mount -t proc proc /proc After mounting the /proc, 1234(container) # ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 09:08 pts/1 00:00:00 /bin/bashroot 74 1 0 09:56 pts/1 00:00:00 ps -ef You’ll only see the processes within the container. One step forwardFor now, we have our own /proc directory that supports standalone process information, but we still share other filesystem with the host, if we want to have a totally fresh filesystem, we need to prepare a new root filesystem and replace the default root filesystem with the new one. Download the alpine root filesystemAlpine OS is popular and secure, tiny OS, I choose it as this experiment’s OS, the files (mini root filesystem) can be downloaded from https://alpinelinux.org/downloads/, after that we need to consider how to let the process use this root filesystem, we’ll pivot_root here to change the root filesystem.pivot_root takes two main parameters, the first one is new_root which is where your new root filesystem is, in this case, ~/Downloads/alpine_root/; the second one put_old which is the location you want to put your current root filesystem. 12345(container) # mount -B ~/Downloads/alpine_root/ ~/Downloads/alpine_root/(container) # pivot_root ~/Downloads/alpine_root/ ~/Downloads/alpine_root/old_root(container) # cd /(container) # lsbin brook dev etc home lib media mnt old_root opt proc root run sbin srv sys tmp usr var The reader might be asking “why do we need to mount the new root filesystem again?”, read the pivot_root man page, you will see new_root must be a path to a mount point, but can’t be “/“, so mounting itself ensures it’s a mount point. After you run the commands in the container, you have set the alpine root filesystem now. What about old_root?We almost forget old_root, it’s the previous root filesystem, we don’t want to see it in the container, so let’s umount it. 12(container) # umount /old_rootumount: can't unmount /old_root: Resource busy Who’s using old_root now? I remember, the shell we use now is still under /old_root, we need to use alpine’s shell, so, the final workflow of the program is: mount alpine root filesystem mount /proc use pivot_root to use alpine chdir to the root directory umount the old root filesystem run the alpine’s shell (or whatever you like) For the Golang implementation, it’s here: https://github.com/songrgg/namespace-demo#mount-a-new-root-filesystem What’s Next?Until now, we have setup an Alpine container that has separate UTS, UID, PID namespaces, we’ll create the networking namespace in the next experiment.Also, in the future we can test how to limit the container resource. ReferenceLinux Manual - procLinux /proc explainedPivot_rootPivot_root2 Recommended Posts(Driven byHexo Recommended Posts plugin)Linux namespace in Go - Part 3, Cgroups resource limitLinux namespace in Go - Part 1, UTS and PID","link":"/programming/linux-namespace-part02-uid-mount/"},{"title":"Linux namespace in Go - Part 3, Cgroups resource limit","text":"In the previous article, I did two experiments on what isolation it brings with UID and Mount, this article explains how to limit the container’s resource by using Cgroups, for instance, CPU, memory resources. The series of Linux namespace in Go: Linux namespace in Go - Part 1, UTS and PID Linux namespace in Go - Part 2, UID and Mount Linux namespace in Go - Part 3, Cgroups resource limit Cgroups Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored. The kernel’s cgroup interface is provided through a pseudo-filesystem called cgroupfs. Grouping is implemented in the core cgroup kernel code, while resource tracking and limits are implemented in a set of per-resource-type subsystems (memory, CPU, and so on). We can use cgroups to control the container’s resource usage, it’s necessary when we have many containers running in a host machine, it prevents some container from consuming too much resources therefore the other containers would run out of CPU, memory, etc. The interface we setup the resource limit is Linux I/O interface, you can simply write to the cgroups configuration files and it will take effect immediately. How to setup cgroup limit?Cgroup configuration is organized by file system hierachy, for convention, the cgroup directory is mounted under /sys/fs/cgroup, the separate resource configuration directories are placed under some paths like /sys/fs/cgroup/cpu/user/user1, this is the configuration for user1’s processes.Cgroup configuration is applied to the processes, if the parent process’s resource is limited, its child processes are also automatically limited according to its parent cgroup limit. The process list is stored under /sys/fs/cgroup/cpu/user/user1/cgroup.procs, after you add the process ID to the file, the processes it spawns will be added to the file automatically. Resource TypesThere are different resource types that you can specify for your process, they’re called controllers. cpuYou’re allowed to setup both soft and hard limits to the CPU shares your processes can use, soft means if the CPU is not busy, it would specify more CPU shares to the process otherwise it would not. Hard means no matter the CPU is busy or not, the process could not use more that the specified limit. cpuacctThe CPU accounting controller is used to group tasks using cgroups andaccount the CPU usage of these groups of tasks. The CPU accounting controller supports multi-hierarchy groups. An accountinggroup accumulates the CPU usage of all of its child groups and the tasksdirectly present in its group. cpusetThis cgroup can be used to bind the processes in a cgroup to a specified set of CPUs and NUMA nodes. memoryThe memory controller supports reporting and limiting of process memory, kernel memory, and swap used by cgroups. devicesThis supports controlling which processes may create (mknod)devices as well as open them for reading or writing. Thepolicies may be specified as allow-lists and deny-lists.Hierarchy is enforced, so new rules must not violate existingrules for the target or ancestor cgroups. freezerThe freezer cgroup can suspend and restore (resume) all processes in a cgroup. Freezing a cgroup /A also causes itschildren, for example, processes in /A/B, to be frozen. net_clsThis places a classid, specified for the cgroup, on networkpackets created by a cgroup. These classids can then be usedin firewall rules, as well as used to shape traffic usingtc(8). This applies only to packets leaving the cgroup, notto traffic arriving at the cgroup. blkioThe blkio cgroup controls and limits access to specified block devices by applying IO control in the form of throttling and upper limits against leaf nodes and intermediate nodes in the storage hierarchy. perf_eventThis controller allows perf monitoring of the set of processes grouped in a cgroup. net_prioThis allows priorities to be specified, per network interface, for cgroups. hugetlbThis supports limiting the use of huge pages by cgroups. pidsThis controller permits limiting the number of process that may be created in a cgroup. rdmaThe RDMA controller permits limiting the use of RDMA/IB-specific resources per cgroup. I would take CPU and memory controllers as example in the following exercises. CPU controllerThis introduces how to setup CPU limits for the process, in this case, I wanna limit the CPU hard limit to 0.5 cores. First, we need to create an isolated group for this CPU limit, as I said before, the configuration is usually under /sys/fs/cgroup, let’s create a new folder for this, we call this /sys/fs/cgroup/cpu/mycontainer. 1sudo mkdir -p /sys/fs/cgroup/cpu/mycontainer Then, we set the CPU hard limit to 0.5 cores, there are two parameters cpu.cfs_period_usthe total available run-time within a period (in microseconds) cpu.cfs_quota_usthe length of a period (in microseconds) The actual schedule run-time of the process will be cpu.cfs_quota_us microseconds of cpu.cfs_period_us microsends, so to use only 0.5 cores, we can specify 5000 out of 10000. 123sudo suecho 10000 &gt; /sys/fs/cgroup/cpu/mycontainer/cpu.cfs_period_usecho 5000 &gt; /sys/fs/cgroup/cpu/mycontainer/cpu.cfs_quota_us Finally, we put the process of a Bash script to the cgroup.procs file, 12bashecho $$ &gt; /sys/fs/cgroup/cpu/mycontainer/cgroup.procs You can test the CPU usage with yes &gt; /dev/null and use htop to monitor the current CPU usage, it will be around 0.5 core used by the yes command. Memory ControllerSimilar to the CPU controller, let’s have a look at the memory cgroup configurations. tasks # attach a task(thread) and show list of threads cgroup.procs # show list of processes cgroup.event_control # an interface for event_fd() memory.usage_in_bytes # show current usage for memory memory.memsw.usage_in_bytes # show current usage for memory+Swap memory.limit_in_bytes # set/show limit of memory usage memory.memsw.limit_in_bytes # set/show limit of memory+Swap usage … For more details, check Memory Cgroup To setup the memory hard limit for the process, first we create a cgroup folder for this and write to the memory.limit_in_bytes. After that, we added the process ID to the cgroup.procs. 1234sudo mkdir -p /sys/fs/cgroup/memory/mycontainersudo suecho 10000000 &gt; /sys/fs/cgroup/memory/mycontainer/memory.limit_in_bytesecho &lt;process-id&gt; &gt; /sys/fs/cgroup/memory/mycontainer/cgroup.procs Cgroups in GolangCgroup in Golang is equivalent to the commands I have executed before, the example code you can access in the exercise05. cgroup cpu & memory setup123456789101112131415161718192021222324func addProcessToCgroup(filepath string, pid int) { file, err := os.OpenFile(filepath, os.O_WRONLY, 0644) if err != nil { fmt.Println(err) os.Exit(1) } defer file.Close() if _, err := file.WriteString(fmt.Sprintf(&quot;%d&quot;, pid)); err != nil { fmt.Println(&quot;failed to setup cgroup for the container: &quot;, err) os.Exit(1) }}func cgroupSetup(pid int) { for _, c := range []string{&quot;cpu&quot;, &quot;memory&quot;} { cpath := fmt.Sprintf(&quot;/sys/fs/cgroup/%s/mycontainer/&quot;, c) if err := os.MkdirAll(cpath, 0644); err != nil { fmt.Println(&quot;failed to create cpu cgroup for my container: &quot;, err) os.Exit(1) } addProcessToCgroup(cpath+&quot;cgroup.procs&quot;, pid) }} The update to cgroup directories needs root permission, we not only need to use sudo to run the program, but also need to modify the SysProcIDMap, use the command line arguments to setup the UID and GID mapping to the root user &amp; group in the container, in my case it’s the current non-root user I use, so I use -uid=1000 -gid=1000. 1sudo NEWROOT=/home/srjiang/Downloads/alpine_root go run exercise05/main.go -uid=1000 -gid=1000 After you run the program, you’ll find the process of the program in the /sys/fs/cgroup/cpu/mycontainer/cgroup.procs, it’s up to you to modify the cgroup configuration like CPU limits and memory limits on your own. What’s Next?We’ve had a container with its own file system, isolated user namespace and PID namespace, and we can limit the resource of this container, next, we want to bring the network to this container. Reference Cgroups Linux Manual Drill Set the CPU limit for the Service Memory Cgroup Recommended Posts(Driven byHexo Recommended Posts plugin)Linux namespace in Go - Part 2, UID and MountLinux namespace in Go - Part 1, UTS and PID","link":"/programming/linux-namespace-part03-cgroups/"},{"title":"微服务实践一: 架构图谱","text":"目录 服务拆分与服务发现 微服务框架选择 服务间通信 服务编排 配置管理 服务容错与降级 监控 API监控 服务调用链 服务负载 基础依赖监控 日志分析 Monolithic vs Microservice Monolithic Microservice 开发测试 Java类语言项目越大，运行调试需要越多的编译时间，本地调试有较多依赖，况且业务复杂后不易新人上手 只有部分功能的代码，运行更快速，根据业务划分，方便新人上手 部署 更新整个项目 更新一个微服务 生产调试 日志集中，调试方便 日志分散，服务依赖复杂；拓容简便 这两种架构视乎业务的复杂程度和代码量的大小，复杂程度低于一定程度，还是单一应用开发更快速，部署更加容易，因为服务拆分和线上的调试这些都是需要成本，反之，当业务不断增加，代码不断膨胀，服务拆分显得逐渐重要，公共功能能抽出来做成基础服务，各个业务所需的轮子也不需要重新造一遍，反而节省了开发成本，同时对于新人的加入，由于服务职责单一，让新人进来一个一个服务地熟悉，好过于一个大而全的项目。 微服务基础架构 处理流程 浏览器发起下单请求到负载均衡(LB) 负载均衡派发请求到API gateway API gateway查询服务发现，找到user服务和store服务，将用户信息和商品ID发送给store服务， store服务查询MySQL，找到商品信息，生成订单信息，将请求发送给payment服务 payment服务根据用户指定的支付渠道，向第三方服务发起请求，存储订单信息，并返回订单状态 store服务 -&gt; API gateway -&gt; LB LB返回数据到浏览器 微服务技术 Recommended Posts(Driven byHexo Recommended Posts plugin)Anatomy of envoy proxy: the architecture of envoy and how it worksIstio Version Control On Kubernetes再见，micro: 迁移go-micro到纯gRPC框架微服务实践四: 配置管理","link":"/microservice/microservice-in-action-1/"},{"title":"微服务实践三: 服务编排","text":"物理机部署传统发布流程（以Java spring boot为例） 编译jar包 分发到服务器A,B,C 服务启动，监听到指定端口 配置负载均衡到已启动服务端口 服务发布成功 关于服务更新，为了实现滚动更新，可以让LB绑定的服务逐渐更新 传统更新流程 编译jar包 分发到服务器A,B,C 将服务器A从LB上解绑，更新服务器A上的服务 启动服务，通过健康检查和QA之后，将服务器A绑定到LB上 继续更新服务器B和C 服务完全更新成功 拓容流程 新增机器节点 启动jar包 将新节点注册到LB上 特点 单机端口有限，同一个服务如果在同一个服务器更新，需要不同的端口 动态更新LB 拓容成本高 服务化部署（这里以kubernetes为例）k8s发布流程 构建docker镜像 创建deployment和service，可以限制服务的CPU、Memory等资源，k8s寻找空闲节点启动服务 更新iptables将物理机上指定端口路由到VIP(虚拟服务IP) 绑定物理机端口到LB k8s更新流程 构建docker镜像 更新deployment和service，k8s更新某个pod 轮流更新pod，直到所有pod更新完成 k8s拓容 寻找空闲节点启动服务，直到达到指定数量 特点 几乎无物理端口限制（k8s需要物理端口作为转发，默认为30000+，数量有限） 服务间通信，可以使用serviceName或者服务的VIP进行访问，内网访问更方便 虚拟化物理机资源，隔离物理资源的细节，资源控制如拓容、服务资源限制方便 Kubernetes vs Docker swarm 稳定性上，k8s上基于iptables的网络路由比docker swarm的网络更加稳定 配置性上，k8s比docker swarm要复杂，swarm采用manager-worker架构，由manager调度worker，docker 1.12以上对于swarm原生支持，方便启动集群，不过k8s在新版本之后也越来越易于配置 管理系统上，swarm比k8s的UI界面更友好，操作性更强 微服务架构下的应用 外部访问可以暴露gateway到LB上，外部通过访问LB进行访问 使用k8s或者swarm，服务间通信可以使用serviceName进行访问，也可以利用容器的IP，使用服务注册进行服务查询 自动拓容，当检测到服务的CPU和内存利用率升高，通过水平拓展，增加服务节点；服务压力减少后，逐渐减少服务节点数量 Recommended Posts(Driven byHexo Recommended Posts plugin)Anatomy of envoy proxy: the architecture of envoy and how it worksIstio Version Control On Kubernetes再见，micro: 迁移go-micro到纯gRPC框架微服务实践四: 配置管理","link":"/microservice/microservice-in-action-3/"},{"title":"Mahout入门-1","text":"Mahout是什么？Mahout是用来进行机器学习和推荐算法的一个Java框架。 一个简单的基于用户的协同过滤的示例读取用户对电影评分的数据集，为ID为2的用户推荐3部电影。 12345678910111213141516171819202122232425262728293031323334353637383940package com.mahout;import org.apache.mahout.cf.taste.common.TasteException;import org.apache.mahout.cf.taste.impl.model.file.FileDataModel;import org.apache.mahout.cf.taste.impl.neighborhood.ThresholdUserNeighborhood;import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender;import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity;import org.apache.mahout.cf.taste.model.DataModel;import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood;import org.apache.mahout.cf.taste.recommender.RecommendedItem;import org.apache.mahout.cf.taste.recommender.UserBasedRecommender;import org.apache.mahout.cf.taste.similarity.UserSimilarity;import java.io.File;import java.io.IOException;import java.util.List;public class Main { public static void main(String[] args) { try { DataModel model = new FileDataModel(new File(&quot;resources/dataset.csv&quot;)); UserSimilarity similarity = new PearsonCorrelationSimilarity(model); UserNeighborhood neighborhood = new ThresholdUserNeighborhood(0.10, similarity, model); UserBasedRecommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity); List&lt;RecommendedItem&gt; recommendations = recommender.recommend(2, 3); for (RecommendedItem recommendation : recommendations) { System.out.println(recommendation); } } catch (IOException e) { e.printStackTrace(); } catch (TasteException e) { e.printStackTrace(); } }} 实践后感好久没有碰Java系列，今天又是看了Mahout的示例，看起来很简单，结果自己试了下，发现要调整IDE，配置Maven，包括数据集文件路径的设置都要注意。 引用 官网示例","link":"/programming/mahout-tutorial/"},{"title":"微服务实践四: 配置管理","text":"配置涵盖程序运行的环境，程序依赖的基础资源地址，程序的行为等。 需求 根据环境读取配置 方便更新配置 基础数据格式为list，map等，常见的配置格式是JSON，YAML，XML 配置部署（docker环境）跟随镜像打包最直接的方式也就是将配置文件也打包到docker镜像，配置即代码，更新配置需要构建新的镜像 优点 配置更新随代码库有版本记录和修改记录 缺点 每次更新配置需要进行git commit - git push - docker build - docker push - docker pull这么繁杂的流程（如下图），紧急情况下过于缓慢 利用文件挂载方式利用docker volume的方式，将配置文件挂载到容器里指定的目录，程序通过访问文件系统读取配置 优点 对程序的侵入性小，很多程序都是读取文件系统，不改变它们的习惯 容器更新不依赖于配置，容器每次重启都可以更新配置。k8s的ConfigMap就是这个方式 缺点 需要为所有容器挂载该卷，配置更新需要更新到所有容器 配置服务从配置服务如Etcd，Consul中读取配置，即配置集中管理，通过API的方式读取（如下图） 优点 配置更新和读取通过API 多Etcd实例，配置服务高可用，同时保证一致性 容器更新不依赖于配置，容器每次重启重新拉取配置 配置的管理更安全，不是所有开发人员都可以更新 缺点 一般需要改变程序行为，老程序往往不支持这些配置服务，需要通过改代码的方式注入配置，修改起来会有成本 总结尽量将配置与程序解耦 老项目更新起来成本高的情况下进项优先使用文件挂载，其次是镜像打包，除非这个项目稳定到你基本不需要更新配置。 新项目因为配置的频繁更新，上配置服务是很有必要的，除非你想为了更新配置文件跑一遍CI Recommended Posts(Driven byHexo Recommended Posts plugin)Anatomy of envoy proxy: the architecture of envoy and how it worksIstio Version Control On Kubernetes再见，micro: 迁移go-micro到纯gRPC框架etcd生产环境实践","link":"/microservice/microservice-in-action-4/"},{"title":"微服务实践二: 服务容错与降级","text":"保证系统能稳定地运行在生产环境是第一要务，就算是服务质量下降，只要仍在工作，那就是万幸。 常见服务问题 服务超时依赖的第三方服务因为某种不可抗力超时了？数据库慢查询拖垮了整个数据库？ 服务错误某个服务挂了？ 服务负载高突然陡增的访问量？ 解决方法 限时针对服务超时，可以通过超时控制保证接口的返回，可以通过设置超时时间为1s，尽快返回结果，因为大多数情况下，接口超时一方面影响用户体验，一方面可能是由于后端依赖出现了问题，如负载过高，机器故障等。某个互联网公司曾经说，当系统故障时，fail fast。 fallback有些情况下，即使服务出错，对用户而言，也希望是透明的，无感的，设置一些fallback，做一些服务降级，保证用户的体验，即使这个服务实际上是挂掉的，返回内容是空的或者是旧的，在此故障期间，程序员能赶紧修复，对用户几乎没有造成不良体验。 电路熔断这里的电路熔断是对于后端服务的保护，当错误、超时、负载上升到一定的高度，那么负载再高下去，对后端来说肯定是无法承受，好像和电路熔断一样，这三个因素超过了阈值，客户端就可以停止对后端服务的调用，这个保护的措施，帮助了运维人员能迅速通过增加机器和优化系统，帮助系统度过难关。 工具Hystrix能保护客户端，服务降级，它的dashboard上有一句标语，defend your app，确实，当后端程序能对异常，超时，错误等进行处理，那么客户端能获得的数据能更加稳定统一，同时它也是对后端服务的保护，hystrix有上述的电路熔断机制和用户可以自定义fallback，对服务限时等功能。 hystrix运行流程可见How it works 以构建一个对内部RPC调用的HystrixCommand为例: 构建一个HystrixCommand用于RPC调用，设置超时时间为1s，fallback为返回空数据 如果缓存打开，结果优先从缓存中获取 如果电路被熔断，尝试fallback 如果并发量超过限制，尝试fallback 不然，运行实际的RPC调用，如果调用失败或者超时，尝试fallback 根据实际情况设置hystrix的超时时间，fallback，并发量都可以根据需要封装的指令进行设置，可以说非常灵活，根据自己的具体业务进行合适的设置，能优化用户体验。 例如：文章列表API依赖的服务超时，可以通过服务降级拉取缓存中的旧数据进行返回，虽然即时性稍逊，但是起码用户能读到几分钟前的文章，在此期间，赶紧修复问题。 Recommended Posts(Driven byHexo Recommended Posts plugin)Anatomy of envoy proxy: the architecture of envoy and how it worksIstio Version Control On Kubernetes再见，micro: 迁移go-micro到纯gRPC框架微服务实践四: 配置管理","link":"/microservice/microservice-in-action-2/"},{"title":"Mit分布式系统课程-Lab2-PartB","text":"前两天删了在github上的6.824, 因为课程里说了不要把源码放在公共的地方, 之前没删一直心有歉意, 还好进度没这么快, 应该对别人也不会造成影响. 言归正传, 这星期空余时间一直在看Lab2的Part B, 今天因为身体不适, 请假一天在家, 开始实现思路, 并记录一下自己遇到的一些大小问题. 目标实现基于viewservice(Lab1实现的视图服务器, 用来记录主副服务器的地址)的键值服务器, 分别为primary(主服务器),Backup(备份服务器), 客户端通过viewservice获得primary的地址, 发送键值的存取请求, primary在响应请求的同时, 将数据备份到backup服务器. 实现完成后, 通过所有的unit test. 架构 viewservice 存储并提供primary以及backup的信息 接受primary, backup的定时ping请求, 更新或保持两个服务器的位置 提供查询当前服务器状态 primary 接受client的请求 定期向viewservice报备自己的状态 在backup服务器更换时负责将自身全部数据同步过去 backup 作为primary的备份存储 定期向viewservice报备自己的状态 在primary挂了之后, 提升为primary, 负责client的请求 client 向primary发送数据存储,查询的请求 若primary返回”错误的服务器”时, 询问viewservice, 更新primary 遇到的挑战 实现时有些golang的特性不是特别了解, 一边打开golang的官方文档, 一边poc. 决定在什么时候需要容错? 是否可恢复? 在反复重试时, 是否要定最大重试次数, 以及失败的处理. 单元测试当我运行go test来验证逻辑的时候, 实在不得不赞叹老师们详细的测试用例, 有几个case让我挠头. TestAtMostOnce 模拟服务器返回报文丢失的情况, 利用puthash-get验证, puthash是put操作的一种变体, 它要存储的值是先前的值和当前值的hash结果, 所以这个地方引入了存储状态, 我通过每次请求要求客户端发送一个unique id来记录此次操作, 尽管报文丢失, 客户端重试时还是能用这个id找回上次通信的内容. TestConcurrentSame 验证primary服务器只有在backup也处理成功的情况才算成功. TestRepeatedCrash 模拟存储服务器间断崩溃的情况, 就是要保证客户端的每个请求都有重试的可能, primary和backup的通信过程中, backup随时可能崩溃, 这个测试使用put-get验证. TestRepeatedCrashUnreliable 模拟存储服务器间断崩溃的同时, 网络也不稳定, 服务器的返回报文会丢失. 这个case的难点在它是使用puthash-get来验证的, 就是说, 你要在服务器不时崩溃的情况下保证以前的值不仅在primary服务器上存在, 也要在backup服务器上保存, 以免primary下一秒挂了. TestPartition1 模拟primary已经过期的情况下, 向他发送的请求, 能被它拒绝. POCbackup服务器更换时, primary需要同步当前数据库所有数据到backup, 我不想用foreach来每条数据进行同步, 所以想既然rpc能传递这么多类型的参数, 那直接将map传递过去不就不用那么麻烦了?所以进行了一次poc. server.go1234567891011121314151617181920212223242526272829303132333435363738394041424344import &quot;net&quot;import &quot;net/rpc&quot;import &quot;os&quot;import &quot;fmt&quot;type Server struct { db map[string]string}func (server *Server) Sync(database map[string]string, reply *int) error { fmt.Println(&quot;Start syncing...&quot;) server.db = database for k, v := range server.db { fmt.Println(k, &quot; =&gt; &quot;, v) } *reply = 1 fmt.Println(&quot;Finish syncing...&quot;) return nil}func main() { server := new(Server) server.db = make(map[string]string) rpcs := rpc.NewServer() rpcs.Register(server) me := &quot;/var/tmp/test1122&quot; os.Remove(me) l, e := net.Listen(&quot;unix&quot;, me) if e != nil { fmt.Println(&quot;listen error&quot;) } for true { conn, e := l.Accept() if e != nil { fmt.Println(&quot;accept error&quot;) conn.Close() return } go func() { rpcs.ServeConn(conn) }() }} client.go1234567891011121314151617181920212223package mainimport &quot;net/rpc&quot;import &quot;fmt&quot;func main() { me := &quot;/var/tmp/test1122&quot; c, e := rpc.Dial(&quot;unix&quot;, me) if e != nil { fmt.Println(&quot;dial error&quot;) return } defer c.Close() db := make(map[string]string) db[&quot;name&quot;] = &quot;srg&quot; db[&quot;age&quot;] = &quot;25&quot; reply := 1 e = c.Call(&quot;Server.Sync&quot;, db, &amp;reply) if e != nil { fmt.Println(&quot;sync error&quot;) return }} 开两个terminal, 在$GOPATH目录先运行server.go, 再运行client.go, 最后server.go运行后的输出, 看来是可以的 12345$ go run server.go Start syncing...name =&gt; srgage =&gt; 25Finish syncing... 在我把这页的hint也看完的时候, 发现里面提到这个问题, 建议直接把这个map作为参数传递. Recommended Posts(Driven byHexo Recommended Posts plugin)Practice datacenter failover in production分布式系统频次限制实现","link":"/programming/mit-distributed-system-01/"},{"title":"MySQL支持emoji","text":"方案1: 应用层支持MySQL默认的数据库编码是utf8，对于emoji文字是不能直接存储的，要想存储emoji，有许多库支持对emoji的转换，例如将😊转存为:smile:，放在数据库里，在从数据库里取出来的时候，再转换为😊，这是应用层的方案。 方案2: 存储层支持MySQL编码设置为utf8mb4，直接支持emoji，不需要应用层的改动。 注意事项:MySQL索引的长度不能大于767字节, utf8 1个字存3byte, utf8mb4存4byte，相对应VARCHAR的字节数分别是255和191，所以要把索引字段的长度限制到191。 12ALTER TABLE &lt;table1&gt; MODIFY COLUMN &lt;col1_indexed&gt; VARCHAR(191);ALTER TABLE &lt;table1&gt; CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; 参考资料 How to support full Unicode in MySQL databases","link":"/operation/mysql-utf8mb4-encode/"},{"title":"PHP数组的内存布局","text":"内存布局它的内存结构，目前的实现方案是分配一块连续的内存区间，用来存储hash元数据和具体数据。连续的内存，上面部分是存储hash值到值地址的映射，而下面部分就是值的存储区域，如下图（摘自php-src/Zend/zend_types.h）： 1234567891011121314/* * HashTable Data Layout * ===================== * * +=============================+ * | HT_HASH(ht, ht-&gt;nTableMask) | * | ... | * | HT_HASH(ht, -1) | * +-----------------------------+ * ht-&gt;arData ---&gt; | Bucket[0] | * | ... | * | Bucket[ht-&gt;nTableSize-1] | * +=============================+ */ 新建数组初始化一块内存，大小为HT_SIZE(ht) = HT_HASH_SIZE(ht) + HT_DATA_SIZE(ht)，如果你查看zend_types.h，能看到这三个宏定义： 12345678#define HT_SIZE(ht) \\ (HT_HASH_SIZE(ht) + HT_DATA_SIZE(ht))#define HT_DATA_SIZE(ht) \\ ((size_t)(ht)-&gt;nTableSize * sizeof(Bucket)) #define HT_HASH_SIZE(ht) \\ (((size_t)(uint32_t)-(int32_t)(ht)-&gt;nTableMask) * sizeof(uint32_t)) 这里要知道nTableMask是什么，从字面意思是掩码，它的类型是uint32_t，值是(uint32_t)(-nTableSize)，nTableSize最小为2，那么此时nTableMask为4294967294，表示成二进制，11111111 11111111 11111111 11111110，这个值在之后计算对应hash所在的下标值会用到。下面就是这块内存最开始的状态，假设数组大小为2，nNumUsed即已有元素为0，则下面是初始状态： 123456789101112/* * HashTable Data Layout * ===================== * ht-&gt;nNumUsed = 0 * +=============================+ * 0 | | * 1 | | * +-----------------------------+ * ht-&gt;arData ---&gt; | | * | | * +=============================+ */ 插入key的hash值为12345678的元素E事实上对于这块内存，我们仅仅是根据arData这个指针去使用，而它是Bucket* 类型，上半部分的类型是uint32_t* ，怎么在不超出上半部分边界的情况下进行索引，这里用到了刚才提到的nTableMask，下标为(int32_t)(HASH | nTableMask)，计算出为-2，那么((uint32_t*)arData)[-2]所指向的就是下标为0处的内存地址，通过nTableMask保证不会越界。E元素首先根据nNumUsed找到存储自身的位置，即ht->arData[ht->nNumUsed]，并且修改hash元数据部分，0位置的值指向存储区域，如下所示： 123456789101112/* * HashTable Data Layout * ===================== * ht-&gt;nNumUsed = 1 * +=============================+ * 0 | 0 | * 1 | | * +-----------------------------+ * ht-&gt;arData ---&gt; | E | next-&gt;HT_INVALID_IDX * | | * +=============================+ */ 插入key的hash值为12345678的元素F使用链表法解决冲突。 123456789101112/* * HashTable Data Layout * ===================== * ht-&gt;nNumUsed = 2 * +=============================+ * 0 | 1*sizeof(Bucket) | * 1 | | * +-----------------------------+ * ht-&gt;arData ---&gt; | E | next-&gt;HT_INVALID_IDX * | F | next-&gt;0 * +=============================+ */ 哈希拓容，rehash扩大数组大小，将数据转存到新数组。 123456789101112131415/* HashTable Data Layout * ===================== * ht-&gt;nNumUsed = 2 * +=============================+ * 0 | 1*sizeof(Bucket) | * 1 | | * 2 | | * 3 | | * +-----------------------------+ * ht-&gt;arData ---&gt; | E | next-&gt;HT_INVALID_IDX * | F | next-&gt;0 * | | * | | * +=============================+ */Recommended Posts(Driven byHexo Recommended Posts plugin)xhprof安装记录PHP7特性概览(一)PHP7数组实现(画图版)","link":"/programming/php-hashtable/"},{"title":"以Kubernetes sidecar方式部署Nginx: 提供更好的Web性能","text":"Web server’s gzipWeb服务开启数据压缩，有利于节省带宽。服务器根据客户端请求头所带的Accept-Encoding判断是否需要对返回数据进行压缩，通常支持的压缩格式是gzip。 应用gzip or Nginx gzip开发人员可以选择在Web framework中开发一些middleware来实现Gzip，也可以选择使用Nginx gzip，将所有gzip放在nginx中完成。 放在nginx中实现的优势是nginx中gzip性能优秀，能很大程度地减少gzip带来的消耗，像Golang中系统自带库中实现的gzip性能上相比nginx就差很多，并且需要使用对象池进行优化，避免每次创建gzip对象带来的性能损耗，在CPU和内存上占用较大。 使用Nginx gzip替代应用gzip如果使用Nginx实现的gzip，那么部署的时候可以有几种方案。 集中式nginx集群nginx集中部署，通过配置反向代理服务各种应用，优势是部署方便，集中管理。劣势是更新路由也是牵一发动全身，并且需要及时拓容。 每个实例搭配nginx原本对外暴露的应用现在通过nginx代理，1:1的方式部署，不用担心拓容的问题。需要解决的就是如何保证它们打包部署。 Sidecar in Kubernetes这里讨论Kubernetes中部署Web服务的情况，遇到刚才的方案二，可以在Kubernetes中找到非常匹配的部署方法。 Kubernetes中最小部署单位称为Pod，Pod中可以部署1个以上的功能紧密联系的容器，并且它们共享网络、磁盘，也就是它们能通过localhost:port访问到彼此，那以上的情况nginx作为gzip功能可以说和后端应用是紧密结合，所以可以以sidecar的形式部署。 Nginx配置如果你的应用监听在8080端口，nginx监听在8090，可以如下配置 /etc/nginx/site.conf123456789101112131415161718192021user nginx;worker_processes 1;events { worker_connections 1024;}error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;http { include /etc/nginx/mime.types; default_type application/octet-stream; keepalive_timeout 65; gzip on; gzip_min_length 256; gzip_types application/javascript application/json text/css text/plain; include /etc/nginx/conf.d/*.conf;} /etc/nginx/conf.d/site.conf12345678server { listen 8090; location / { proxy_pass http://127.0.0.1:8088/; proxy_set_header Host $http_host; }} 参考 Use of pods Nginx gzip HTTP Accept-Encoding Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesImplement zero downtime HTTP service rollout on Kubernetes","link":"/operation/nginx-kubernetes-sidecar-for-better-performance/"},{"title":"Practice datacenter failover in production","text":"Distributed system is like human body, it will have issues and break. There’s a theory that we feed it with issues deliberately and constantly, the body will be more and more stable and robust. It’s the same to system, put some issues to datacenters and let them failover automatically. Multiple data centersCompanies use data center redundancy to implement service’s high availability, there will be multiple data centers existing with 3 main deployments: Disaster RecoveryYou will have your live traffic served in the primary data center, meanwhile disaster recovery data center is a backup to recover when the primary is down. Usually, it doesn’t allow you to run normal operations in the disaster recovery one. Hot StandbyThe primary data center is taking traffic, the hot standby data center is almost equivalent to primary but doesn’t take traffic. You can switch to hot standby anytime the primary data center is down. Live traffic handlingThere are multiple data centers and they’re taking traffic simultaneously. What is DC failover?When dc ( data center ) failure happens, the most emergent thing is to use the backup dc to replace the primary one and ensures the business keeps running, so the technical team will failover the data center to backup one. Why do we need to do failover often?In the deployments mentioned before, there will be one or more data centers serving user, once the primary one is down the backup one needs to take over as soon as possible, but it’s not often that the primary data center is down, as the time flies, likely, backup datacenter cannot replace the primary or is hard to replace.To keep the backup dc up-to-date, we should do dc failover often regardless manually or automatically. PreparationDefine the impactService is always for customers, although failover is a long-term project that will improve the service’s availability, it’s better not to interrupt the customer’s experience all of a sudden, so according to different company’s considerations, they need to define the impact they can undertake. Define the scopeAfter the company decides to practice data center failover in production, there will be a lot of questions to answer, “What’s the scale of this failover?”, “Is it global or partial?”, “What consequences can I bear or how much budget do we have?”, “who should participate in”…Such questions can define the scope of the failover, how many business lines should participate in it, how many teams, how many people will join. Define the goalAlso, define the goal clearly, it’s a long term project that ensures there will be always multiple available data centers online, this failover is the first time and will repeat very soon, finally, it’ll be continuous and automatic like chaos engineering. Team as unitEvery team involved should take care of the servers, services… they need to failover when the failover day comes, the team is the minimal unit. Deadline“Deadline is the first productivity”, since the failover may not seem to be important to everyone, teams may not put them to the priority, so the deadline is a clear signal it will happen sooner or later. From an SRE’s perspectiveAs an SRE, I play a vital role in this failover and will execute the operations. You should have a full list of your services. Get your operation documentation ready, for example, the operation commands and monitoring dashboard addresses. Mini failovers can be done gradually before the entire one. When the day comesA clear agendaA clear agenda is a precondition that everything is under control even if something unexpected happens. Take the unexpected into consideration and make the agenda more flexible. Instant communicationFor the team, the progress must be understood by every team member and their clients.Also, it’s necessary to put everything unexpected into a global channel and everyone is aware of it. Roles we playThe coordinator is the one who’s responsible for connecting all the team players, (s)he’s responsible for recording the matters happening including success and failure, other team players should report what they’ve done and seen to the coordinator. After the failoverAfter the failover, teams involved in this failover should look back and check what’s the good part and what needs to be improved.Against the weakness, we can make some plans and put them into the backlog, we’ll be more confident facing the next failover.I will spend more time on chaos engineering which is the continuous accidents injection to production and will bring production more resilience. Reference Multiple data center redundancy Principles of Chaos Engineering Recommended Posts(Driven byHexo Recommended Posts plugin)Use Traffic Control to Simulate Network Chaos in Bare metal & Kubernetes分布式系统频次限制实现Mit分布式系统课程-Lab2-PartB","link":"/operation/practice-datacenter-failover-in-production/"},{"title":"PHP7特性概览(一)","text":"了解PHP7的一些特性，搭建PHP7源码编译环境，并运行官网这些新特性的代码。 在64位平台支持64位integer在64位平台支持64位integer，长度为2^64-1字符串。 更详细查看 抽象语法树抽象语法树是语法分析之后产物，忽略了语法细节，是解释前端和后端的中间媒介。新增抽象语法树，解耦语法分析和编译，简化开发维护，并为以后新增一些特性，如添加抽象语法树编译hook，深入到语言级别实现功能。 更详细查看 闭包this绑定新增Closure::call，优化Closure::bindTo(JavaScript中bind，apply也是一样的应用场景)。 12345&lt;?phpclass Foo { private $x = 3; }$foo = new Foo;$foobar = function () { var_dump($this-&gt;x); };$foobar-&gt;call($foo); // prints int(3) 2-3行新建Foo的对象，第4行创建了一个foobar的闭包，第5行调用闭包的call方法，将闭包体中的$this动态绑定到$foo并执行。 同时官网上进行性能测试，Closure::call的性能优于Closure::bindTo。 更详细查看Closure::call 简化isset的语法糖从使用者角度来说，比较贴心的一个语法糖，减少了不必要的重复代码，使用情景如： 12345&lt;?php// PHP 5.5.14$username = isset($_GET['username']) ? $_GET['username'] : 'nobody';// PHP 7$username = $_GET['username'] ?? 'nobody'; 在服务器端想获取$_GET中的变量时，若是PHP5语法，需要使用?:操作符，每次要重写一遍$_GET['username']，而在PHP7就可以使用这个贴心的语法糖，省略这个重复的表达式。 更详细查看isset_ternary yield from允许Generator方法代理Traversable的对象和数组的操作。这个语法允许把yield语句分解成更小的概念单元，正如利用分解类方法简化面向对象代码。例子： 12345678910111213141516171819&lt;?phpfunction g1() { yield 2; yield 3; yield 4;} function g2() { yield 1; yield from g1(); yield 5;} $g = g2();foreach ($g as $yielded) { print($yielded);}// output:// 12345 yield from后能跟随Generator，Array，或Traversable的对象。 更详细查看generator delegation 匿名类123456&lt;?phpclass Foo {} $child = new class extends Foo {}; var_dump($child instanceof Foo); // true 更详细查看anonymous class 标量类型声明123456789101112&lt;?phpdeclare(strict_types=1);function add(int $a, int $b): int { return $a + $b;}var_dump(add(1, 2)); // int(3)// floats are truncated by defaultvar_dump(add(1.5, 2.5)); // int(3) //strings convert if there's a number partvar_dump(add(&quot;1&quot;, &quot;2&quot;)); // int(3) 更详细查看 返回值类型声明1234567891011121314&lt;?phpfunction get_config(): array { return [1,2,3];}var_dump(get_config());function &amp;get_arr(array &amp;$arr): array { return $arr;}$arr = [1,2,3];$arr1 = get_arr($arr);$arr[] = 4;// $arr1[] = 4;var_dump($arr1 === $arr); 更详细查看return_types 3路比较一个语法糖，用来简化比较操作符，常应用于需要使用比较函数的排序，消除用户自己写比较函数可能出现的错误。分为3种情况，大于(1)，等于(0)，小于(-1)。 %}1234567&lt;?phpfunction order_func($a, $b) { return $a &lt;=&gt; $b;}echo order_func(2, 2); // 0echo order_func(3, 2); // 1echo order_func(1, 2); // -1 导入包的缩写12345678910111213141516171819202122232425&lt;?phpimport shorthandCurrent use syntax: use Symfony\\Component\\Console\\Helper\\Table;use Symfony\\Component\\Console\\Input\\ArrayInput;use Symfony\\Component\\Console\\Output\\NullOutput;use Symfony\\Component\\Console\\Question\\Question;use Symfony\\Component\\Console\\Input\\InputInterface;use Symfony\\Component\\Console\\Output\\OutputInterface;use Symfony\\Component\\Console\\Question\\ChoiceQuestion as Choice;use Symfony\\Component\\Console\\Question\\ConfirmationQuestion; // Proposed group use syntax: use Symfony\\Component\\Console\\{ Helper\\Table, Input\\ArrayInput, Input\\InputInterface, Output\\NullOutput, Output\\OutputInterface, Question\\Question, Question\\ChoiceQuestion as Choice, Question\\ConfirmationQuestion,}; 更详细查看 Recommended Posts(Driven byHexo Recommended Posts plugin)xhprof安装记录PHP7数组实现(画图版)PHP数组的内存布局","link":"/programming/php7-new-features-1/"},{"title":"PHP7数组实现(画图版)","text":"主要介绍一下PHP7对于数组的实现。 预备知识PHP的数据类型 zend_longphp中的long类型, 在64位机上是64位有符号整数, 不然是32位符号整数。 1234567891011// zend_long.h/* This is the heart of the whole int64 enablement in zval. */#if defined(__x86_64__) || defined(__LP64__) || defined(_LP64) || defined(_WIN64)# define ZEND_ENABLE_ZVAL_LONG64 1#endif#ifdef ZEND_ENABLE_ZVAL_LONG64typedef int64_t zend_long;#elsetypedef int32_t zend_long;#endif double双精度浮点数 zend_refcounted引用计数类型, 记录引用次数, 以及u用于存储元素类型信息type字段(如is_string, is_array, is_object等), 标明对象是否调用过free, destructor函数的flags, 记录GC root number (or 0) and color的gc_info字段(详情见php的zend_gc.h及zend_gc.c). 12345678910111213// zend_types.hstruct _zend_refcounted { uint32_t refcount; /* reference counter 32-bit */ union { struct { ZEND_ENDIAN_LOHI_3( zend_uchar type, zend_uchar flags, /* used for strings &amp; objects */ uint16_t gc_info) /* keeps GC root number (or 0) and color */ } v; uint32_t type_info; } u;}; zend_string字符串类型, 带有引用计数, 和string的hash值h, 避免每次需要计算, 字段长度len, 以及val用来索引字符串, 这里tricky地避免了2次malloc内存. 12345678// zend_types.htypedef struct _zend_string zend_string;struct _zend_string { zend_refcounted gc; zend_ulong h; /* hash value */ size_t len; char val[1];}; zend_object123456789typedef struct _zend_object zend_object;struct _zend_object { zend_refcounted gc; uint32_t handle; // TODO: may be removed ??? zend_class_entry *ce; const zend_object_handlers *handlers; HashTable *properties; zval properties_table[1];}; zend_resource123456struct _zend_resource { zend_refcounted gc; int handle; // TODO: may be removed ??? int type; void *ptr;}; zend_reference1234struct _zend_reference { zend_refcounted gc; zval val;}; zend_array详细见下文具体实现. 1234567891011121314151617181920212223242526// zend_types.htypedef struct _zend_array zend_array;typedef struct _zend_array HashTable;struct _zend_array { zend_refcounted gc; union { struct { ZEND_ENDIAN_LOHI_4( zend_uchar flags, zend_uchar nApplyCount, zend_uchar nIteratorsCount, zend_uchar reserve) } v; uint32_t flags; } u; uint32_t nTableMask; Bucket *arData; uint32_t nNumUsed; uint32_t nNumOfElements; uint32_t nTableSize; uint32_t nInternalPointer; zend_long nNextFreeElement; dtor_func_t pDestructor;}; 位运算给出一个上限Max, 利用位运算求出指定N的表达式-(Max-(N % Max))的值. 1234uint32_t max = 2;uint32_t mask = (uint32_t)-max; // 二进制表示: 11111111 11111111 11111111 11111110uint32_t n = 12345678;int32_t answser = (int32_t)(n | mask); // -2 散列表散列表原理维基百科 局部性原理空间局部性, 时间局部性 具体实现初始化12&lt;?php$arr = array(); arData指针指向的内存, 是真实数据的起始地址, 在邻近的低址内存是存储hash值到真实数据地址的映射表, 这个映射表是uint32_t类型的数组, 大小相同于nTableSize, 这样最好情况下, 每个hash值都能映射一个真实数据. 插入插入key为$key1, $key1.hash为0, 值为10的元素 12&lt;?php$arr[$key1] = 10; 上文提到的位运算就是应用在插入元素场景, 由于arData指向的是真实数据的起始地址, 而索引信息(即存储hash值到真实数据的映射)处于arData更低地址, 那么要更新索引信息, 就需要计算出-(nTableSize-(nHash % nTableSize)), nHash就是键的hash值, 例如向大小为2的数组, 插入hash值为0的元素, 那么索引到hash值为0的区域就是((uint32_t*)arData)-(2-(0%2)), 如图, 将hash值为0的数据偏移0*sizeof(Bucket)存储到了((uint32_t*)arData)-2. 哈希冲突插入key为$key2($key2 != $key1), $key2.hash为0, 值为20的元素, 造成哈希冲突 12&lt;?php$arr[$key2] = 20; 数组拓容插入key为$key3, $key3.hash为1, 值为30的元素, 造成数组的load factor过高, 触发拓容 12&lt;?php$arr[$key3] = 30; 删除删除key为$key2的元素 12&lt;?phpdelete $arr[$key2]; 遍历123456789&lt;?phpforeach ($arr as $v) { print $v . &quot;\\n&quot;;}// output// 10// 30 直接遍历arData, 最大边界为arData+nNumUsed, 跳过被UNDEF的元素. 与历史版本比较改进123456789101112131415161718// zend_hash.htypedef struct _hashtable { uint nTableSize; uint nTableMask; uint nNumOfElements; ulong nNextFreeElement; Bucket *pInternalPointer; /* Used for element traversal */ Bucket *pListHead; Bucket *pListTail; Bucket **arBuckets; dtor_func_t pDestructor; zend_bool persistent; unsigned char nApplyCount; zend_bool bApplyProtection;#if ZEND_DEBUG int inconsistent;#endif} HashTable; 连续的内存, 更高的效率通过简单地观察数据结构, 可以发现5.3.23版本是使用Bucket **arBuckets分配一块二维Bucket数组存放键值, 与7.0的预分配连续内存的做法不同, 每次需要插入元素都要申请一块sizeof(Bucket)的内存, 更容易造成内存碎片, 以及低效率; 更小的数据结构, 更优美的实现此外, 废弃了Bucket *pListHead以及Bucket *pListTail这两个头尾指针, 本来是为了实现数组特性, 实现正反序遍历等功能, 而7.0既然已经是连续的一块内存, 那么直接从Bucket *arData下标0处, 到达边界arData+nNumUsed就可以实现这个功能. 良好的局部性遍历数组有更好的局部性, 相较于5.3.23的链表遍历, 使得遍历时cache能更准确地加载数据, 拥有更好的时间空间局部性. 可以说这个7.0版本对PHP数组的优化是非常成功的, 除此之外, 对于其他的数据结构7.0也是有”瘦身”优化, 对于整个效率和内存占用有比较明显的改善. Recommended Posts(Driven byHexo Recommended Posts plugin)xhprof安装记录PHP7特性概览(一)PHP数组的内存布局","link":"/programming/php-hashtable2/"},{"title":"Build a production-ready web application Part 1: Python Basics","text":"This article helps the software engineers who have Web development experience in other languages to migrate to Python Web development quickly, it contains several parts like basics, web development part, etc. ContextI’m a developer who used Golang in the recent 4 years, I’m getting used to Golang and its framework, because of my team’s preference, we chose Python 3 as the new project’s language, I wanted to write Python code as well as possible in a short time. Because the core of Web development is the same no matter what language it is, I just need to migrate my Web knowledge to Python 3.8+. This is the checklist of all the essential knowledge I sorted out in the last month. Python BasicsFirst, let’s recap the most basic Python syntax. (PS: I’ll mix different languages’ syntax when I switch languages very often, the common case is that I wrote for i in items in Golang) Entrypoint12if __name__ == '__main__': print('hello world') If-else statement1234567a = 1if a is None: print('Nothing')elif a == 1: print('Bingo')else: print('bye') Common data structuresIt contains array, set, dictionary, tuple. 123456789101112131415161718192021222324252627# arrayarr = [1,2,3]if 1 in arr: print('1 is in the arr')# setvisited = set()visited.add(1)if 1 in visted: print('1 is in visited')# dict, it will throw error when the key is not foundword_mapping = dict()word_mapping['Peace'] = 'Love'print(word_mapping['Peace']) # Outputs: Love# defaultdict, it will return default value when the key is not foundfrom collections import defaultdictdef def_value(): return &quot;Undefined&quot;word_mapping = defaultdict(def_value)print(word_mapping['no existing key']) # Outputs: Undefined# tuplecountries = (&quot;China&quot;, &quot;Netherlands&quot;, &quot;Spain&quot;)print(countries[0]) # Outputs: China IterationTo iterate an array, list, set, dictionary, you can always use for … in , also it allows you to create an iterable object which contains the iterable object returned by __iter__ and the way to fetch next object by __next__ . 123456789101112131415161718192021222324252627282930# iterate arrarr = [1, 2, 3]for k in arr: print(k)# iterate dictword_mapping = {&quot;Peace&quot;: &quot;Love&quot;, &quot;Love&quot;: &quot;Peace&quot;}for k in word_mapping: print(word_mapping[k])# customized iterable objectsclass MyLanguages: def __init__(self): self.languages = [&quot;Python&quot;, &quot;Golang&quot;, &quot;Java&quot;] def __iter__(self): self.index = 0 return self def __next__(self): if self.index &gt;= len(self.languages): raise StopIteration res = self.languages[self.index] self.index += 1 return reslanguages = MyLanguages()for language in languages: print(language) FunctionIt contains traditional function and anonymous function like the other languages. Traditional function is the one with def keyword, name of function and function body. 123# sum_two accepts two int parameters and return the sum of them.def sum_two(num1: int, num2: int) -&gt; int: return num1 + num2 We also want variable parameters sometimes, we can add * before the parameter and the parameter will accept multiple parameters, type type of args is tuple. 12345678# sum_all accepts multiple integer parameters and sum them up.def sum_all(*args: int) -&gt; int: sum = 0 for arg in args: sum += arg return sumsum_all(1, 2, 3) We want to accept variable name parameters, we can add ** before the parameter and the parameter will receive key=value pairs. 1234567def print_vars(**kwargs): for k in kwargs: print(k, &quot;=&gt;&quot;, kwargs[k])print_vars(name=&quot;songrgg&quot;, hobby='cooking')# name =&gt; songrgg# hobby =&gt; cooking Note that the **kwargs parameters should be at the end of the parameter list, otherwise it’s not straightforward for the Python interpreter to parse the parameters afterwards. For anonymous functions, we use lambda as the keyword, it can accept multiple parameters but only one expression in the body. 12sum_two = lambda num1, num2: num1 + num2print(sum_two(1, 2)) # Output: 3 It’s super useful when you’re using some map, filter, sort, etc and it accepts a lambda function. 12345678910111213141516# mapnumbers1 = [1, 2, 3]numbers2 = [4, 5, 6] result = map(lambda x, y: x + y, numbers1, numbers2)print(list(result)) # Output: [5, 7, 9]# filternumber_list = range(-5, 5)less_than_zero = list(filter(lambda x: x &lt; 0, number_list))print(less_than_zero) # Output: [-5, -4, -3, -2, -1]# sort based on the 1-index value asca = [(1, 2), (4, 1), (9, 10), (13, -3)]a.sort(key=lambda x: x[1])print(a) # Output: [(13, -3), (4, 1), (1, 2), (9, 10)] Class definitionThe class definition starts with class keyword, the class contains some functions by convention, like the constructor function __init__ , method call __call__ which allows you to call the class instance like a function. The self parameter appears for those instance functions, it’s the this keyword in other languages, a pointer to the instance itself, it’s always as the first parameter. 1234567891011class Sample: def __init__(self): print('Sample init') def __call__(self): print('Sample is called implicitly by __call__')sample = Sample()# shorthand for sample.__call__()sample() There are also class functions whose scope is the class, the first parameter is cls instance, they will be introduced in the decorators section. DecoratorsA decorator is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. 12345678910111213def echo_first(f): &quot;&quot;&quot; Nothing special, echo first before the method is called &quot;&quot;&quot; def new_f(*args, **kwargs): print('echo') f(*args, **kwargs) return new_fclass Test: @echo_first def method(self): print('method called')Test().method() In this sample, it’s a simple function decorator that print echo first before the method is called. There are some common used decorators we can use, @staticmethod prevents the method access to the class or instance, @classmethod limits the access to the class itself, instance_method has the biggest scope. 12345678910111213class Test: @staticmethod def static_method(): print(&quot;I don't have access to the class or instance&quot;) @classmethod def class_method(cls): print(&quot;I only have access to the class&quot;) def instance_method(self): self.value = 1 Test2.cls_value = 2 print(&quot;I have access to both class and instance&quot;) There is @property decorator to call the setter/getter/deleter of the property in a native way, 12345678910111213141516171819202122class PropertyTest: @property def x(self): &quot;&quot;&quot;I am the 'x' property.&quot;&quot;&quot; print('getter called') return self._x @x.setter def x(self, value): print('setter called') self._x = value @x.deleter def x(self): del self._xpt = PropertyTest()pt.x = 34 # setter calledprint(pt.x)# Output: getter called# 4del pt.x Error handling123456789101112try: …except (ValueError, ZeroDivisionError): …else: # no exceptions raisedfinally: # cleanup code # Raising exceptions if x &lt; 1: raise ValueError(&quot;…&quot;) Concurrency Coroutines are very similar to threads. However, coroutines are cooperatively multitasked, whereas threads are typically preemptively multitasked. Coroutines provide concurrency but not parallelism. The advantages of coroutines over threads are that they may be used in a hard-realtime context (switching between coroutines need not involve any system calls or any blocking calls whatsoever), there is no need for synchronization primitives such as mutexes, semaphores, etc. in order to guard critical sections, and there is no need for support from the operating system. The Python doc gives several examples of how to define coroutines and execute them, 1234567891011121314151617import asyncioimport timeasync def say_after(delay, what): await asyncio.sleep(delay) print(what)async def main(): print(f&quot;started at {time.strftime('%X')}&quot;) await say_after(1, 'hello') await say_after(2, 'world') print(f&quot;finished at {time.strftime('%X')}&quot;)asyncio.run(main())# it will take around 3 seconds for they're running in sequence They can’t be executed as the normal methods, if you call main() directly, it returns a coroutine object, to execute it you need pass it to asyncio.run. To run the coroutines concurrently, you can wrap the coroutine to task by asyncio.create_task , 12345678async def main(): task1 = asyncio.create_task(say_after(2, &quot;hello&quot;)) task2 = asyncio.create_task(async_method(2, &quot;world&quot;)) await task1 await task2asyncio.run(main())# it will take around 2 seconds for they're running concurrently It will run for around 2 seconds instead of 4 seconds. Awaitables We say that an object is an awaitable object if it can be used in an [await](https://docs.python.org/3/reference/expressions.html#await) expression. Coroutines: the async def functions Tasks: used to schedule coroutines concurrently, created by asyncio.create_task Futures: A Future is a special low-level awaitable object that represents an eventual result of an asynchronous operation. EndNext Artitlce contains the toolkit of setting up the Web application using FastAPI framework, also it will contain the code format, linter, docker image, etc.","link":"/programming/python-quickstart-1-basics/"},{"title":"Raspberry 3安装docker","text":"SD卡制作准备一张4GB或者以上的micro sd卡，下载系统镜像，例如raspbian-jessie-lite.img，并使用刷机工具，如Pi filler将其写入sd卡，当然也可以使用命令行的dd，或者其它工具，要注意的是sd卡必须是fat32格式的文件系统，不然树莓派是无法识别的。 开机启动先把所有的线材都连上之后，最后再通电，应该很顺利能看到启动的系统。 设置WIFI1234echo 'network={ ssid=&quot;your-ssid&quot; psk=&quot;your-psk&quot;}' | sudo tee -a /etc/wpa_supplicant/wpa_supplicant.conf 如果你的WIFI设置没有启动 1sudo ifconfig wlan0 up 设置apt国内镜像阿里云的镜像还是很快的。 1echo 'deb http://mirrors.aliyun.com/raspbian/raspbian/ wheezy main non-free contrib\\ndeb-src http://mirrors.aliyun.com/raspbian/raspbian/ wheezy main non-free contrib' | sudo tee /etc/apt/sources.list.d/hypriot.list docker安装1234567ssh pi@raspberrypi.localsudo apt-get install -y apt-transport-httpswget -q https://packagecloud.io/gpg.key -O - | sudo apt-key add -echo 'deb https://packagecloud.io/Hypriot/Schatzkiste/debian/ wheezy main' | sudo tee /etc/apt/sources.list.d/hypriot.listsudo apt-get updatesudo apt-get install -y docker-hypriotsudo systemctl enable docker 以非root权限运行docker 1sudo usermod -aG docker `whoami` 如果没有起作用，需要重新登录 参考Run Docker on a Raspberry Pi 3 with onboard WiFi Recommended Posts(Driven byHexo Recommended Posts plugin)how to build the smallest docker image as fast as you canHow to install Spinnaker on CentOS 7","link":"/life/raspberry3-docker/"},{"title":"分布式系统频次限制实现","text":"频次限制（rate limiting）是Web系统比较常见的功能，防止用户频繁访问接口，导致系统负载增加而影响服务的质量。这里来介绍在分布式系统中实现一个共享的频次限制服务，且保证接口低时延高访问量。 系统要求 针对线上的功能，实现对指定对象有访问频次的限制 支持多个客户端访问 低延迟 承受较大的访问量 易于拓展 流程 设置服务频次限制，如针对某个消耗资源很高的API设置1QPS的访问限制，并为单一用户生成访问该API的唯一key，如userA_APIX就是表示userA访问APIX的情况 根据指定key，服务向ratelimiter询问是否允许此次访问 基于令牌桶算法的伪代码12345678910111213141516171819202122232425262728293031// 初始换状态rate := 1 // 设置频率为1opsneeded := 1 // 设置访问1次API需要的令牌数量为1key := &quot;userA_APIX&quot; // userA访问APIX的场景tokens := 0 // 当前可用令牌数lastTime := time.Now() // 上次令牌更新时间// 时间过去1stime.Sleep(time.Second)// userA请求访问APIXnowTime := time.Now()// 计算这段时间新生成的令牌数量newTokens := (nowTime-lastTime)*rate// 判断是否允许访问allow := (newTokens+tokens)&gt;needed// 更新数据if allow { tokens = newTokens+tokens-needed lastTime = nowTime // 响应用户操作} else { tokens = tokens+newTokens lastTime = nowTime // 提示用户操作超过限制} 设计思路 频次限制应该统一存储webserver是任意可拓展个数的服务，一开始，我将ratelimiter的存储放在了各自服务中，导致明明我限制的是整个集群这个API的访问单人不可超过1QPS，如果N个服务，实际上是NQPS。所以这个频次限制需要集中存储，统一整个系统的访问频次。 频次限制本质是一个服务我试着把存储挪到了redis，并且使用了一些redis的WATCH命令实现了CAS(Compare And Set)，redis里存了key，tokens，lastTime，算法逻辑仍然放在client端（也就是那些web server）。经过压测，QPS比较低，主要是因为CAS的重试率随着并发量上升不断升高，况且重试过程不断访问redis增加了网络开销，于是考虑如何将逻辑和数据放到一起。 控制成本重新开发一个频限服务，需要考虑多节点数据同步，请求转发，failover等功能，我希望以最小的开支实现这个系统。 实现最终选择了redis的lua脚本实现了频限功能，既统一了存储，又可以利用redis cluster实现了可拓展的频限服务，以最小的成本实现功能。我司技术栈是Golang，所以实现了Golang的简单库，逻辑是初始化redis client并将lua脚本上传，在判断是否超过频率限制的时候，使用EVALSHA命令执行。由于逻辑都在redis端，所以客户端实际上代码极少，只要注意redis挂了不影响服务的响应，并且redis重启后保证lua脚本上传等。 lua脚本是来自Stripe公司的一位工程师： 1234567891011121314151617181920212223242526local tokens_key = KEYS[1]local timestamp_key = KEYS[2]local rate = tonumber(ARGV[1])local capacity = tonumber(ARGV[2])local now = tonumber(ARGV[3])local requested = tonumber(ARGV[4])local fill_time = capacity/ratelocal ttl = math.floor(fill_time*2)local last_tokens = tonumber(redis.call(&quot;get&quot;, tokens_key))if last_tokens == nil then last_tokens = capacityendlocal last_refreshed = tonumber(redis.call(&quot;get&quot;, timestamp_key))if last_refreshed == nil then last_refreshed = 0endlocal delta = math.max(0, now-last_refreshed)local filled_tokens = math.min(capacity, last_tokens+(delta*rate))local allowed = filled_tokens &gt;= requestedlocal new_tokens = filled_tokensif allowed then new_tokens = filled_tokens - requestedendredis.call(&quot;setex&quot;, tokens_key, ttl, new_tokens)redis.call(&quot;setex&quot;, timestamp_key, ttl, now)return { allowed, new_tokens } 通过SCRIPT LOAD上传该脚本获得hash，使用EVALSHA &lt;hash&gt; 2 userA_APIX_token userA_APIX_ts 1 1 &lt;now_time&gt; 1调用该脚本，根据返回的allowed决定此次操作是否进行。 benchmark经过测试，MacBook Pro (Retina, 13-inch, Late 2013), CPU 2.4 GHz Intel Core i5, Memory 8 GB 1600 MHz DDR3上能达到**1w+**QPS。 项目地址ratelimiter的golang实现，目前该版本暂时支持redis client，为了拓展性，可以将其升级为redis cluster，满足大并发系统的需求。 Recommended Posts(Driven byHexo Recommended Posts plugin)Practice datacenter failover in productionMit分布式系统课程-Lab2-PartB","link":"/programming/rate-limiter-for-distributed-system/"},{"title":"Spark on Hive实现APP渠道分析","text":"背景最近在做APP投放渠道分析，就是Android应用投放到应用市场，所谓渠道就是huawei,xiaomi,yingyongbao之类，运营人员根据数据分析渠道的下载安装情况、各个渠道的投放效果。 需求完成一个Android渠道分析的展示面板，包含以下指标： APP总新增激活数量 按渠道划分的新增数量 各渠道的新增变化走势图（以小时为单位） 品牌占比 操作系统占比 架构 数据APP激活时上报给服务端数据，Flume处理数据并将数据发送到Kafka。 数据格式客户端上报对JSON格式更加友好，所以这里选择使用JSON格式，格式定义一方面需要考虑客户端的开发成本，一方面需要考虑日后的拓展性，所以最直接的方法是统一固定的字段，将根据事件所变化的内容放到拓展字段里去，拓展字段是Map类型，可以支持各种拓展的形式。 数据内容以此次渠道分析为例，客户端需要上传客户端的渠道、APP版本、设备标志符、设备型号等信息，更详细的如Geo信息，如果想获得更好的数据展示效果，可以上传，但在此场景可以不需要，这些是主动上报的部分。还有一部分内容是需要在服务端获取的，例如设备IP，为了之后的地理展示，可以使用MaxMind公司的IP与城市对应的数据库进行地理解析。 数据处理flume接受到客户端的数据之后，需要对数据进行解析JSON，并且获得用户IP、分析Geo Location做一些轻量级的处理，因为这个部分是在前端flume做的，这个部分的flume重点是逻辑要轻，重要的是吞吐量高和延迟低。 接下来前端flume把处理完的数据按照事件名发送到Kafka同名的topic中，后端flume消费Kafka并将消息转存到Hdfs中。 数据持久化将数据从Hdfs持久化到Hive，一方面是更节省空间，一方面是更有利于Spark进行查询。 这里持久化到Hive的方法，可以有几种： Flume直接读取Kafka的数据并存储到Hive，这是由Flume的Hive Sink实现的，数据持久化到Hive Transaction表，是Hive 0.13引入的，支持ACID。 Flume读取数据到Hdfs，支持配置文件路径，可以根据时间来划分存储路径，之后可以定期使用Hive加载数据，将数据存储到Hive中去。 第一种方法，相对来说配置简单，省去了中间一步转储的过程。第二种方法，相对繁琐，但是之后会有一个好处。 下面是设备注册表device_registration的schema device_registration1234567891011121314151617181920CREATE TABLE `bi.device_registration`( `app_id` string, `uid` bigint, `time` bigint, `ip` string, `device_id` string, `app_version` string, `os_name` string, `os_version` string, `device_brand` string, `device_model` string, `ua_name` string, `ua_version` string, `channel` string, `ts` timestamp, `lon` double, `lat` double, `country` string, `city` string) 这里要提一下，为什么有了time这个unix时间戳，还需要ts这个timestamp类型的时间，实际上是为了之后的查询工具Superset需要使用来实现按时间查询。 计算这里的计算每小时运行一次，从Hive中读取过去一个小时的设备激活原数据，与device_registration进行比对，将未出现在device_registration中的设备信息加入。 这里可以使用JOIN来实现，Spark数据表之间的Join方式有多种，inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti 这里场景适合使用left_anti，因为是取不在这个集合的设备信息。 这里还要注意，因为时间区间内的数据有可能会有重复，所以需要取时间较早的那条，这里用到了groupByKey和reduceGroups，具体是 1234567deviceInfos.as[Device] .groupByKey(_.device_id) .reduceGroups((x, y) =&gt; if (x.time &lt; y.time) x else y) .map[Device]((x: (String, Device)) =&gt; x._2) .write.mode(SaveMode.Append) .format(&quot;hive&quot;) .saveAsTable(&quot;bi.device_registration&quot;) 查询查询引擎使用Presto，配置Hive Connector，实时搜索数据。刚才上面提到数据持久化Hive中的两种方法，其中支持事务的表不可用于Presto的查询，因为Hive Transaction表的数据格式未被Presto支持(详见Implement Hive ACID table support)。 直接查询通过上一步的device_registration表，我们可以通过时间维度进行查询。 直接查询就是查询raw data，数据更加丰富。这里以“按渠道划分的新增数量”为例， 12345678SELECT &quot;channel&quot; AS &quot;channel&quot;, date_trunc('hour', CAST(ts AS TIMESTAMP)) AS &quot;__timestamp&quot;, COUNT(*) AS &quot;count&quot;FROM &quot;bi&quot;.&quot;device_registration&quot;WHERE &quot;ts&quot; &gt;= from_iso8601_timestamp('2018-02-03T16:29:29') AND &quot;ts&quot; &lt;= from_iso8601_timestamp('2018-02-05T08:29:29')GROUP BY &quot;channel&quot;, date_trunc('hour', CAST(ts AS TIMESTAMP)) 预先计算预先计算就是将图表展示需要的数据结果提前计算存储到数据库，查询的时候直接就可以从结果表中查询。 预先计算是用空间换时间，损失一些灵活性，不如查询raw data时可以自由定制查询。每次新增查询对于预先计算来说都需要新增加计算逻辑。随着数据量的增大，预先计算不可避免。 优化查询速度运行一段时间后，查询速度明显变慢，查看Hdfs上的hive目录发现数据表内的小文件多达几百上千，这是由于Spark处理完数据并写入Hive会产生非常多的bucket，与数据条数成正比，写入成本低却增加了读数据的成本，这样当数据查询时，由于Hdfs上的小文件非常之多，I/O花费很大，导致整体查询速度下降迅速，这里想办法将文件进行合并，减少文件数量。 通过使用Hive执行compaction，方法比较取巧，就是在每次计算完数据之后，运行Hive脚本，通过复制数据库，Hive会自动将文件数量压缩： 123CREATE TABLE bi.device_registration_compact AS SELECT * from bi.device_registration;DROP TABLE bi.device_registration;ALTER TABLE bi.device_registration_compact RENAME TO bi.device_registration; 将Hdfs上的文件数量降低到个位数，查询也在秒级完成，这种方法只适用于数据量不大的情况，目前记录条数在1M以内的查询速度在秒级，之后依然会考虑使用其它方案改良。 展示Superset是Airbnb开源的图表展示工具，不仅支持很多后端查询引擎，并且有许多成熟的图表展示，更可贵的是拥有用户权限管理。 Recommended Posts(Driven byHexo Recommended Posts plugin)Hive on spark实践Hive环境搭建(Ubuntu)","link":"/programming/spark-on-hive-app-channel-analytics/"},{"title":"Spring boot实现数据库读写分离","text":"背景数据库配置主从之后，如何在代码层面实现读写分离？ 用户自定义设置数据库路由Spring boot提供了AbstractRoutingDataSource根据用户定义的规则选择当前的数据库，这样我们可以在执行查询之前，设置读取从库，在执行完成后，恢复到主库。 实现可动态路由的数据源，在每次数据库查询操作前执行 ReadWriteSplitRoutingDataSource.java123456789101112import org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource;/** * @author songrgg * @since 1.0 */public class ReadWriteSplitRoutingDataSource extends AbstractRoutingDataSource { @Override protected Object determineCurrentLookupKey() { return DbContextHolder.getDbType(); }} 线程私有路由配置，用于ReadWriteSplitRoutingDataSource动态读取配置 DbContextHolder.java123456789101112131415161718192021222324252627/** * @author songrgg * @since 1.0 */public class DbContextHolder { public enum DbType { MASTER, SLAVE } private static final ThreadLocal&lt;DbType&gt; contextHolder = new ThreadLocal&lt;&gt;(); public static void setDbType(DbType dbType) { if(dbType == null){ throw new NullPointerException(); } contextHolder.set(dbType); } public static DbType getDbType() { return contextHolder.get() == null ? DbType.MASTER : contextHolder.get(); } public static void clearDbType() { contextHolder.remove(); }} AOP优化代码利用AOP将设置数据库的操作从代码中抽离，这里的粒度控制在方法级别，所以利用注解的形式标注这个方法涉及的数据库事务只读，走从库。 只读注解，用于标注方法的数据库操作只走从库。 ReadOnlyConnection.java1234567891011121314151617package com.wallstreetcn.hatano.config;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * Indicates the database operations is bound to the slave database. * AOP interceptor will set the database to the slave with this interface. * @author songrgg * @since 1.0 */@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)public @interface ReadOnlyConnection {} ReadOnlyConnectionInterceptor.java1234567891011121314151617181920212223242526272829303132333435363738import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.core.Ordered;import org.springframework.stereotype.Component;/** * Intercept the database operations, bind database to read-only database as this annotation * is applied. * @author songrgg * @since 1.0 */@Aspect@Componentpublic class ReadOnlyConnectionInterceptor implements Ordered { private static final Logger logger = LoggerFactory.getLogger(ReadOnlyConnectionInterceptor.class); @Around(&quot;@annotation(readOnlyConnection)&quot;) public Object proceed(ProceedingJoinPoint proceedingJoinPoint, ReadOnlyConnection readOnlyConnection) throws Throwable { try { logger.info(&quot;set database connection to read only&quot;); DbContextHolder.setDbType(DbContextHolder.DbType.SLAVE); Object result = proceedingJoinPoint.proceed(); return result; } finally { DbContextHolder.clearDbType(); logger.info(&quot;restore database connection&quot;); } } @Override public int getOrder() { return 0; }} UserService.java1234@ReadOnlyConnectionpublic List&lt;User&gt; getUsers(Integer page, Integer limit) { return repository.findAll(new PageRequest(page, limit));} 配置Druid数据库连接池build.gradle1compile(&quot;com.alibaba:druid:1.0.18&quot;) groovy依赖注入配置dataSource为可路由数据源 context.groovy123456789101112131415161718192021222324import com.alibaba.druid.pool.DruidDataSourceimport DbContextHolderimport ReadWriteSplitRoutingDataSource** SOME INITIALIZED CODE LOAD PROPERTIES **def dataSourceMaster = new DruidDataSource()dataSourceMaster.url = properties.get('datasource.master.url')println(&quot;master set to &quot; + dataSourceMaster.url)dataSourceMaster.username = properties.get('datasource.master.username')dataSourceMaster.password = properties.get('datasource.master.password')def dataSourceSlave = new DruidDataSource()dataSourceSlave.url = properties.get('datasource.slave.url')println(&quot;slave set to &quot; + dataSourceSlave.url)dataSourceSlave.username = properties.get('datasource.slave.username')dataSourceSlave.password = properties.get('datasource.slave.password') beans { dataSource(ReadWriteSplitRoutingDataSource) { bean -&gt; targetDataSources = [ (DbContextHolder.DbType.MASTER): dataSourceMaster, (DbContextHolder.DbType.SLAVE): dataSourceSlave ] }} 参考资料 Dynamic DataSource Routing with Spring @Transactional Alibaba Druid Recommended Posts(Driven byHexo Recommended Posts plugin)How to host Swagger documentation using yaml/json configuration files?Spring data redis的一个bug解决Jedis数据读取乱码问题","link":"/programming/spring-boot-database-read-write-split/"},{"title":"Spring data redis的一个bug","text":"前两天上线了一个新功能，导致线上业务的缓存总是无法更新，报错也是非常奇怪，redis.clients.jedis.exceptions.JedisConnectionException: Unknown reply: 5，google到的原因是Spring data redis中的scan操作，它获取了redis的连接之后，在操作没有完全结束之前就把redis连接放回连接池，其它线程从连接池里复用该连接时，会导致数据读取的错误。 bug修复官方维护人员在 DefaultHashOperations.java 里使用新建redis连接，并等cursor关闭的时候才关闭connection的方式解决该bug。 解决办法 提升spring data redis版本到1.8，尚未release，可以拉取最新代码，编译成jar包，并配置在local repository 换个实现方案，尝试用其它数据结构，如hset等 引用JedisConnection.java bug地址 Recommended Posts(Driven byHexo Recommended Posts plugin)How to host Swagger documentation using yaml/json configuration files?Spring boot实现数据库读写分离解决Jedis数据读取乱码问题","link":"/programming/spring-data-redis-issuse/"},{"title":"Use Traffic Control to Simulate Network Chaos in Bare metal &amp; Kubernetes","text":"This article explains how to use traffic control to simulate network chaos in Bare metal &amp; Kubernetes, the network chaos can test the resilience of your service under a specified network condition such as packet loss, latency increase, etc. What’s Linux traffic control?Linux traffic control consists of shaping, scheduling, policing, dropping the traffic, it can be used to network administration, for example, rate limit the user traffic, setup the traffic priority. How does traffic control work?tc is the user-space utility program used to configure the Linux kernel packet scheduler, it supports rich functionality for the users to setup the traffic control. Let’s start from a network chaos sample, suppose we have a news website which consists of a bunch of microservices and the news detail API from news service (which is used to render the news page) depends on the comment API from comment service, we want to test the application’s resilience when the comment API is down. We want to use tc to block the egress traffic to the comment service, we’ll run it on the nodes of news service, the command will be 123tc qdisc add dev eth0 root handle 1:0 priotc qdisc add dev eth0 parent 1:3 netem loss 100%tc filter add dev eth0 parent 1:0 protocol ip prio 3 u32 match ip dst &lt;comment-service-ip&gt; flowid 1:3 To explain these commands, there are several components needed to be clarified, qdiscqdisc is short for ‘queueing discipline’ and it is elementary to understanding traffic control. Whenever the kernel needs to send a packet to an interface, it isenqueuedto the qdisc configured for that interface. Immediately afterwards, the kernel tries to get as many packets as possible from the qdisc, for giving them to the network adaptor driver. classSome qdiscs can contain classes, which contain further qdiscs - traffic may then be enqueued in any of the inner qdiscs, which are within the classes. When the kernel tries to dequeue a packet from such a classful qdisc it can come from any of the classes. A qdisc may for example prioritize certain kinds of traffic by trying to dequeue from certain classes before others. filterA filter is used by a classful qdisc to determine in which class a packet will be enqueued. Whenever traffic arrives at a class with subclasses, it needs to be classified. Various methods may be employed to do so, one of these are the filters. All filters attached to the class are called, until one of them returns with a verdict. If no verdict was made, other criteria may be available. This differs per qdisc. The above commands can be interpreted as the tree below, IDEach node in the tree has a unique ID, it consists of two parts, “{major}:{minor}”.For the qdisc, its major part must be unique in the whole tree, its minor part is usually 0 and can be omitted in the tc command.For the class, its ID’s major part is the same as its parent major; its minor part should not be the same as its siblings. Root qdiscThe first command tc qdisc add dev eth0 root handle 1:0 prio set priority qdisc for the root device eth0, as the name says, the packets are sent in prioritized order. Its ID is 1:0 which must be unique and 1:0 is reserved for the root qdisc. 3 classesThe 3 classes are created by the prio qdisc by default, the packets are “routed” to one of them based on the priority. Their IDs are decided by the parent ID, in this case, major part must be the same as its parent 1, minor part is from 1 to 3. Leaf qdiscsThe first 2 leaf qdiscs from left are fifo qdiscs which is set for prio qdisc by default. The third one is specified by the command and it overrides the default fifo qdisc, it uses netem qdisc to create 100% packet loss on class 1:3, so any packet sent to this class will be abandoned. FilterThere’s only 1 filter in the tree, it routed the packets whose destination IP is comment service to the class 1:3, it will then abandon 100% of them. As a result, these 3 commands will help create 100% packet loss from news service to comment service as network chaos. How does tc cooperate with Docker?Suppose I want to run the network chaos for a docker container like above, we have 2 ways: Run tc within the target containerYou can run tc within the docker container with NET_ADMIN capability, otherwise you’ll face Operation not permitted 1docker run -it --rm --cap-add=NET_ADMIN &lt;service-image&gt; &lt;start-command&gt; One more point is that the target container must have tc command, then you can use docker exec to modify the traffic control within the container. However, containers are not always started with NET_ADMIN, so this method is limited in many cases. Run tc on the host machineAnother way to modify it is to do it on the host machine where the container runs. As we know, the container’s network is isolated from the host machine by Linux namespace, so we can enter the container’s network namespace and modify the traffic control. Here’s the workflow: First, we should get the docker container’s ID. 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7d357060fe32 ubuntu:14:04 &quot;/bin/bash&quot; 5 weeks ago Up 5 weeks adoring_engelbart Second, extract the PID of the container. 12$ docker inspect 7d357060fe32 | jq '.[].State.Pid'14902 Finally, modify the traffic control on the host. 12345# add prio qdisc to the root qdisc of the container$ sudo nsenter -t 14902 -n tc qdisc add dev eth0 root handle 1:0 prio# qdisc exists!$ sudo nsenter -t 14902 -n tc qdisc show dev eth0qdisc prio 1: root refcnt 2 bands 3 priomap 1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1 How does tc cooperate with Kubernetes?For Kubernetes, the mechanism is the same as docker one, the difference is that target containers are distributed in the cluster now, so you need to run the query first to locate the target containers and then run the tc (as we mentioned above) on the host machines where the target containers are running. There are open source chaos testing solutions if you’re interested, like Litmus, Chaos Mesh, etc. ConclusionTraffic control is a powerful tool to manipulate the packets and it’s not a frequently used tool for most of us don’t need to administrate the network :) But since chaos testing is more and more adopted by the big companies and different chaos scenarios occur, it’s a good time to understand the technical details for more confidence on the tool. Reference Traffic control Manual Linux Advanced Routing &amp; Traffic Control HOWTO Linux Network Trac Control | Implementation Overview Traffic control Visualisation Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesImplement zero downtime HTTP service rollout on KubernetesPractice datacenter failover in production","link":"/operation/use-traffic-control-simulate-network-chaos/"},{"title":"使用Groovy进行Spring依赖注入","text":"为什么选择Groovy?传统的依赖注入是XML，对我而言，可读性太差，太不美观，强烈地想换一个方式进行依赖注入，Groovy作为XML的替代方案，在Spring4之后被引入，是基于JVM的一门方言，具有较强的可读性，写更少的更易懂的脚本，完成同样的功能。 基于Spring boot框架我选择了Spring boot框架作为MVC，看重了它的惯例大于配置，不用繁琐地配置。 以下代码是伪代码，只是大概说明使用的方式 Application.java12345678910@SpringBootApplicationclass Application { public static void main(String[] args) { Object[] sources = new Object[] { Application.class, new ClassPathResource(&quot;application_context.groovy&quot;) }; SpringApplication.run(sources, args); }} 在Application启动的时候读入该脚本，解析出错会抛出异常。 src/main/resources/application_context.groovy12345beans { userService(UserMockService) { parameter = &quot;你能在这初始化参数&quot; }} 在groovy配置文件里能声明要注入的beans，可以配置初始化时的参数值，这里就是声明一个userService，它的实现是UserMockService。 UserController.java123456789@Controllerclass UserController { @Autowired private UserService userService; public User getUserAction(Long userId) { return userService.findByUserId(userId); }} User.java123456class User { String name; public User(String name) { this.name = name; }} UserService.java123interface UserService { User findByUserId(Long userId);} UserMockService.java1234567891011class UserMockService implements UserService { private String parameter; public String getParameter() { return parameter; } public void setParameter(String parameter) { this.parameter = parameter; } public User findByUserId(Long userId) { return new User(&quot;mockUser&quot;); }} 参考资料 Spring4支持Groovy依赖注入 Spring boot Groovy依赖注入示例代码 &lt; fuck&gt;XML&lt; /fuck&gt;","link":"/programming/spring-ioc-groovy/"},{"title":"微信本地测试","text":"项目里要测试微信公众号, 但又不愿意忍受每次改下代码就提交到版本库, 再发布到生产这个琐碎的流程, 所以必须有一个本地能测试的方法, 到知乎上找了一个方法(引用知乎操晓峰的回答). 原理是HTTP请求重定向, 将微信中要访问的url转发到你设置的代理服务器, 代理服务器转发到你的开发服务器. 有趣的是, 在公司一直没有完全成功, 总有一个页面是刷不出来, 查看Charles发现传输速度极慢, 真想吐槽公司的网络. 回家后重试了一次, 完全成功. Recommended Posts(Driven byHexo Recommended Posts plugin)微信分享常见问题","link":"/programming/wechat-local-debug/"},{"title":"Generate monitoring dashboards &amp; alertings using Grafana API","text":"Grafana has been adopted as a common monitoring dashboard by more and more companies, in many cases, when operators need to create dashboard repeatedly they either choose to use template variables or create dashboards one by one. I think it’s very useful to leverage the Grafana API to generate the monitoring dashboards automatically from template. My thought on dashboard automationThere’s a common use case, if you need to create a lot of similar dashboard (like system stats) for different services, the dashboard skeleton may always be the same, you can use template variables to make the difference. However, you can not create alertings on the graphs which use template variables, so in these situations you have to create graphs without variables. So we can create a template dashboard first with variables in Grafana, then use scripts to read from the template and render them with variables we specify, finally call Grafana APIs to create/update dashboards. Listen to what dashboards we need to create/update, collect metadata first.For example, if it’s for server stats, we need to get the server info from cloud provider or other infrastructure. Trigger the change to the dashboard Create the template of dashboard with template variables, it’s okay, we’ll replace the template later. Render the template with the metadata we collected. Add the graphs using CRUD APIs. MetadataMetadata is what you need to generate dashboards, basically it contains: How many dashboards you want to create? What are each dashboard’s variables?If you use public cloud providers, cloud APIs can be used otherwise you have your own infrastructure management APIs. Template dashboardGrafana Variables is a good functionality for monitor a great number of servers or application, defining the template dashboard with variables is good for rendering because you can not only view the effect on the template but also the variable is easy to replace for the format is $VARIABLE or [[VARIABLE]], string replacement is enough for this. Grafana dashboard CRUD APIGrafana provides enough HTTP APIs to do this, once you create the template dashboard it already has the graphs and alertings, so Grafana dashboard API is enough for most cases. For authentication, you can create a service account for automating the dashboards and use API token to call APIs. The automation scripts can maintain a relationship between the dashboard UID and dashboard name and you can also do the change comparison before updating the dashboards.Delete the dashboards when some of them are deprecated. Run your automationIt’s okay to run automation scripts as cronjobs. Reference Grafana dashboard API Grafana alerting API is only used to get alerts, if you need to modify the alerts using dashboard API Grafana API token Grafana SDK Grafana dashboard reference Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesHow does Prometheus query work? - Part 1, Step, Query and RangeHow to check and monitor SSL certificates expiration with Telegraf","link":"/operation/use-grafana-api-generate-dashboards/"},{"title":"微信分享常见问题","text":"调试前端打开debug模式，浏览器打开时在console里会打印微信分享初始化信息，在微信里会以alert形式出现。 常见问题 签名的时候注意大小写 签名时候用的url需要和当前位置的url保持一致 微信二次分享失败 微信在第二次分享的链接里增加了后缀，如?from=singlemessage&amp;isappinstalled=1，这里需要转义 12345678910111213$.ajax({ url: 'https://domain/wechatshare/sign?url=' + encodeURIComponent(window.location.href.split('#')[0]), type: 'get', success: function (data) { data = JSON.parse(data) data.jsApiList = [ 'onMenuShareTimeline', 'onMenuShareAppMessage','onMenuShareQQ' ]; wx.config(data); }, error: function (xhr, textStatus) { }}) Recommended Posts(Driven byHexo Recommended Posts plugin)微信本地测试","link":"/programming/wechat-share-problem/"},{"title":"笔试面试带来什么？","text":"最早的笔试是大二下半学期的时候支付宝的校园招聘，那时候自己刚看完csapp，终于给了自己一个基本计算机概念，没那么不知所措了。。 被marine鼓励了一下，参加了这个笔试，这笔试考的是linux使用的一些基础知识，比如指令，shell脚本的一些编写，还有一些IQ问题、过了几天，alipay让我去面试，人生的第一次面试就这么诞生了，犹记得在那晚等着电话的我，单曲循环曹方姐姐的门这首歌，在微冷的租的小房间里突然响起铃声，激动地拿起手机，听到的是hr姐姐的悦耳的声音，就算在现在我始终觉得这声音是听过的hr的最好听的了、 面试那天，傻乎乎地给了妈妈和奶奶打了电话，也算分享我当时激动的心情！面试的前奏是紧张的，可是反而在看见那个面试的哥哥的时候，我没那么紧张了，可能是因为他胖胖的，没那么严肃，给我一点放松的心情，他问了我什么知识，比较懂，我说我看过csapp，所以计算机的基础知识，比较扎实，我了个擦，他直接问我中断有哪几种，我当时直接懵了，我没有刻意地去记忆这些东西，他突然发问，我就呆了，毕竟这些知识和code的多少有关系，我当时总共学习一年，code量少，确实被开始就打击了！中断的具体种类在我之后关心操作系统的实现的时候，曾经关注过，可是我现在还是记不太住，可能对我来说，我的缓存确实比较小，不能存到不经常用的内容！像什么缺页中断，是由内存缺页引起的，我会记得比较牢、 被灭了1盏灯之后，他问了两道算法题，现在具体什么内容记不清了，反正是炸鸡了！ 人生中第一次收到回家等消息吧、、第一次回学校认真地等待着消息，印证了第一次的默拒、 大三下，暑期实习的机会我没有特别积极，原因是大三这年的精神状态极差，有一段时间处于讨厌学校的情绪中，陷入自我怪圈！干的事情确实比较少，所以在暑假的时候想自己写点代码，于是有了scheme解释器的小项目，不写不知道，写了才知道，代码功力确实不够，在写的时间里，会出现一个个在看书出现不了的问题，比如，多文件之间全局变量的引用，static修饰符作为修饰变量的可见性，出现文件互相依赖的时候，如何解决？如何使用函数指针？ 如何编写Makefile ？这些只有在实践中才会遇见的有意思的问题，慢慢被自己解决，慢慢终于自己的解释器，初见轮廓，这个暑假确实是对我有意义，督促我不能光看不练，只动嘴皮子、 今年10月份的秋季校园招聘，如火如荼地开展，我发送了许多的简历出去，几乎所有的大公司都投过，结果也比较美好，ms，google，amazon，oracle，tencent，alibaba，baidu…都给我笔试机会，这些笔试，我只过了alibaba一家，除了baidu没去，其他全都没过！baidu没去的原因，是我本身对baidu没太大热情，并且次次下沙到玉泉的公交让我有种崩溃的感觉，我非常讨厌坐车，因为下沙到玉泉要坐1.5-2小时，到了玉泉，人都傻了，记得腾讯那天是早上9点笔试，我6:30和djh出发，中途坐车时肚子突然闹了一下，非常尴尬地到处在街上找公共厕所，问了个门卫大叔，问有没有厕所，他很强硬地回了门卫哪有厕所，我真是脑子都炸了，那他们不用解决吗？ 幸好忍到了厕所的拯救、真是记忆犹新！结果考了1个小时，就出来了，甚至觉得题目太简单，自信满满的觉得妥了，结果我是想多了的节奏、ms的题目设置是不能蒙，宁愿不答也不要蒙，可是我真是自己不会的软件工程题目，自己手贱蒙了个答案上去，乱七八糟的，甚至有道题我觉得题目错了，都是乱蒙，结果确实是题目错了、真是非常缺乏自信，太慌了、 这些公司的笔试，难度正常，但是你要有良好的心理素质、我觉得我的问题是考试经历少，心理素质差！ 对自己的表现非常失望，基本上是自己放弃了这些公司的，很大原因是自信心不足，没有一往无前的决心，怨自己！！ 但是当我lose the chances的时候我才发现，我的机会越来越少，在第二波校园招聘来临的时候，我只能硬着头皮把简历投到自己连了解都不了解的公司，这些公司在我的学校里有招聘会，我去了4家做笔试，第一家简历被拒，我没想通、 第二家笔试过后被拒，自觉笔试挺好的、第三家第四家同第二家，我甚至怀疑我是不是被加入了这些公司的黑名单、、 知乎上有个人发了条message说”我baidu笔试被刷了，我觉得人生灰暗。。求开导” ，我非常蛋疼地已自己的经历来鼓励他，甚至有些自嘲，他没放在心上，以为是我的笑谈、 可是我并没有放弃，我仍在找寻着机会，marine帮我找了很多信息，让我去尝试，鼓励我说，我可以的！于是我尝试了知乎，知乎的jobs网页做的非常酷，网页上的各职位需求用该职位所需的技术来作为媒介，显示需求什么样的人，我当时就很兴奋，觉得cool！于是投了简历，知乎的哥哥很快给了回应，并且给了我两个笔试题，一道是实现server监视特定端口的用户发言请求，请求是json格式，你要做的是判断它是否是垃圾发言，1分钟内不能多余10条，不能发10条重复的发言！ 我花了几天，实现了一个简易的server和client测试程序，我很蛋疼地用c实现，代码量还是挺大的，不过确实server的一些知识，重新复习了一下，比如，套接字如何稳定地读写，因为read，write是不阻塞的，所以你必须通过检查返回值来判断是否成功的读写！ 而且read，write返回值为-1时，还需判断是否是被中断引起的、 等我交了之后，那个哥哥下午给我个电话，和我进行了一次电话面试，一开始就劈头盖脸地问我，为什么用K&amp;R写法，问我对它有多少了解，这种写法是一时兴起，我就如实说了，他反问了一句笔试觉得好玩吗？ 额。。我真的被震住了，太强势了，我当时非常怯懦地说没想清楚，我错了、 然后的电话就已他问我一些关于c的问题，和我还有别的技术兴趣等，可是我被开始K&amp;R问题弄得有点凌乱，似乎我觉得有股火在烧！是的，感觉很不爽，但是觉得自己没想清楚是有点虚，所以有种有火无处发的感觉！期间很插曲地手机没电了，关了5分钟机，我觉得炸鸡了、、 补完剩下的电话之后，我有种虚脱的感觉，和marine谈论自己对于那个哥哥对我K&amp;R写法的强烈打击，marine的话当时就给了我一道灵光！我瞬间觉得人生充满了光明！他说，”我喜欢”是一个非常好的理由，K&amp;R是c语言最早的一代的写法，那时候混沌初开，但是充满了黑客的精神，以前的开源项目都是K&amp;R写法，我这么写首先是喜欢开源文化，黑客文化，其次那么多人那么写，我想和别人不一样，我想要不一样的写法，我想做一个cool的人，程序员不就是要追求cool，不就是要炫技吗，编译器支不支持是另一回事，这表明我的一种态度！ 我非常赞同他说的话，好像说出了心声，但是既喜且悲，悲的是因为自己和人的打交道太少，自己又怯懦，不能强势地表达出自己的想法，我觉得他们不要我也是情理之中，一个不敢说出自己意见的人，怎么配说自己是程序员哪！我顿时有种自惭、不过也为我发现自己这个巨大的缺失而高兴！ 突然面试的机会又来了，以前投过ibm，前两天ibm跟我说要给我视频面试，我很高兴，又有机会来找到自己的不足！ibm的哥哥人很好，问了我一下面试的基础问题，问了我一下和团队合作时，对队友的一些看法，最后的时候还给我了建议：1. 下次面试让室友知道，别让他们光着膀子乱走！ 2. 准备自我介绍 3. 加强表达能力、 三个建议很中肯！很感谢他！ 程序水平对我反而是其次了，和人的沟通才是重点，毕竟合作是大项目的基础！！！ 生活仍在继续—改善—自信—才能获取生存的幸福感！！！ Recommended Posts(Driven byHexo Recommended Posts plugin)PHP面试+怀旧","link":"/life/what-interview-brings-to-me/"},{"title":"下一步用武的目标","text":"用武的出发点产生于我自己比较失败的工作经历的一个发泄, 之所以说是失败, 有以下一些原因: 日益增加的技术渴求和没有匹配的日常工作之间的矛盾 工作口味一直没有被满足, 对工作的几个要求也是如此, 造成不能沉下心来做事 恶性循环, 导致离自己心里的追求越来越远 可能有这样那样的原因导致你的工作并不如意, 这可能包括你的技术、业务、薪资、人际等各种期待，与真实的工作并不合拍. 所以, 对我而言, 无论英雄, 都需要有用武之地. 我们更需要的是**过来人对工作选择的经验, 以及获得更多更详细的职位内核, 包括每日所干的事, 和其它类似职位有何不同, 做一个程序员能参与的评价和比较,更有利于有诉求的程序员根据自己的需求**, 以及对职位的更深入了解, 而不是通过千篇一律的职位描述, 或者其它官方的回答去揣测一份工作, 尽量减少错误挑选工作的概率, 减少走弯路的概率. 而我希望用武能提供这样的一个平台, 给程序员更多的机会去用自己经历和心得给这些来自互联网的职位进行评论以及评分, 或者打小标签等, 当然是基于实名的负责任地评价优劣, 降低程序员踩坑的次数. 用武的下一步方向由于我是一个it从业人员, 可能对于这行的朋友的诉求更了解些, 因为自己就是这样很”作”, 所以优先收录这些互联网的职位, 比如国内的大型互联网职位发布网站, 如拉勾网, 内推网… 初步功能 收录更多的职位信息现在版本只收录了450个拉勾网上的职位信息, 是非常少的部分, 其次需要对于其它网站进行收录, 归纳各个网站的不同点, 规约一份公共的职位信息部分, 以及各自的特色. 尽量实时收录职位信息可能设置一个合适的频率去检测新增的职位. 更准确的关键词搜索现在的搜索引擎使用的是solr, 面对不完整的关键字PH, 搜索出的结果为空, 显然并不合理, 需要进行更精确的配置. github第三方登陆及用户注册既然是为了程序员的职业, 那么首先选择github作为第三方登陆模块, 为了以后大家可以使用程序员的名义负责任地进行职位的评分和评论. 更nice的前端, 职位块的显示更清晰, 显示出重点能够已更清晰简洁的排版展示出职位关键信息. 欲查看用武情况, 可访问用武","link":"/programming/yongwu-next-step/"},{"title":"xhprof安装记录","text":"选择一个工具分析PHP函数调用的资源耗用明细，以图表化的形式展现，方便优化代码。 安装xhprof1$ pecl install xhprof-beta 在php.ini引用的extension中添加extension=xhprof.so GUI这里选择了xhgui，它的原理是在需要测试性能的脚本前加上PHP的一段代码，将收集到的性能数据存储到文件或者mongodb等存储介质中去。 MongoDB1$ apt-get install mongodb 前端1234cd /var/wwwgit clone https://github.com/perftools/xhgui.gitcd xhguiphp install.php 如果不能以root身份运行，那么sudo -u www-data php install.php 安装的时候出现the requested PHP extension mongodb is missing from your system问题是平台的拓展名为mongo.so，而composer检查的是mongodb.so，只要加上--ignore-platform-reqs 如果composer问题不清楚，建议单独跑composer命令，加上-vvv打开调试模式 使用如前面说的原理是在头部添加一段PHP代码，这里通过在Nginx里配置，或者PHP ini auto_prepend_file在php.ini中添加auto_prepend_file。 12345678location ~ \\.php { include fastcgi_params; fastcgi_buffers 128 4k; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PHP_VALUE &quot;auto_prepend_file=\\&quot;/opt/htdocs/xhgui/external/header.php\\&quot;&quot;;} 配置在config目录下添加config.php配置 12345678910111213141516171819202122232425262728293031323334353637&lt;?phpreturn array( 'debug' =&gt; false, 'mode' =&gt; 'development', // Can be either mongodb or file. /* 'save.handler' =&gt; 'file', 'save.handler.filename' =&gt; dirname(__DIR__) . '/cache/' . 'xhgui.data.' . microtime(true) . '_' . substr(md5($url), 0, 6), */ 'save.handler' =&gt; 'mongodb', // Needed for file save handler. Beware of file locking. You can adujst this file path // to reduce locking problems (eg uniqid, time ...) //'save.handler.filename' =&gt; __DIR__.'/../data/xhgui_'.date('Ymd').'.dat', 'db.host' =&gt; '127.0.0.1:27017', 'db.db' =&gt; 'xhprof', // Allows you to pass additional options like replicaSet to MongoClient. // 'username', 'password' and 'db' (where the user is added) 'db.options' =&gt; array(), 'templates.path' =&gt; dirname(__DIR__) . '/src/templates', 'date.format' =&gt; 'M jS H:i:s', 'detail.count' =&gt; 6, 'page.limit' =&gt; 25, // Profile 1 in 100 requests. // You can return true to profile every request. 'profiler.enable' =&gt; function() { return rand(1, 100) === 3; }, 'profiler.simple_url' =&gt; function($url) { return preg_replace('/\\=\\d+/', '', $url); }); Recommended Posts(Driven byHexo Recommended Posts plugin)PHP7特性概览(一)PHP7数组实现(画图版)PHP数组的内存布局","link":"/programming/xhprof-install-record/"},{"title":"Implement zero downtime HTTP service rollout on Kubernetes","text":"You might have encountered some 5xx errors during http service rollout on Kubernetes and wonder how to make it more reliable without these errors, this article will first explain where this errors come from and how to fix them and implement zero downtime. Sometimes you can’t see downtime because it’s hiddenFirst, I would like to share an interesting case before we setup the zero downtime rollout, the client is accessing the service through envoy and fortunately we have x-envoy-retry-on on connect-failure and gateway-error in envoy, so the client didn’t see any 500 or 503 caused by the service rollout, envoy did all the work to “hide” these errors. We don’t spend too much time solving the real rollout issue thanks to Envoy, but it’s not a liable solution, the errors are just hidden by some retries. When the envoy configuration is not set correctly, the client will suffer from this. When does it respond 5xx?According to the Pod lifecycle, there will be two moments when the service can return 5xx (it may be 503 or 500). The moment when the old Pod is killed. The moment when the new Pod is provisioned. 500 errorWhen a new Pod is created and it receives traffic before it’s ready, it will respond 500 to the clients. In this case, we need an accurate readiness check on the Pod, only if the readiness is okay, the Pod starts taking traffic. Ensure application container is readyThe application pod has readiness check, the developer needs to implement a good readiness to ensure the application can start to serve traffic. A basic readiness can be the application HTTP server is started, you can also check the hard dependencies of the application are ready. For example, MySQL instances are connected, Redis, Cassandra, Kafka are healthy to connect. k8s-deployment.yaml123456readinessProbe: httpGet: path: /healthcheck port: 8080 # you application port initialDelaySeconds: 5 periodSeconds: 3 It means when the application is ready, it will wait for initialDelaySeconds (5 seconds in this case) and test the healthcheck API, if it’s healthy then the Pod will be marked ready and it will receive traffic. Ensure the sidecars are ready.The readiness of the sidecar container is needed and should be set up the same way as the application container.Some service mesh frameworks, like Istio, every Pod has an Envoy sidecar, all of the outbound requests will go through it and Envoy is ready when its route configuration is fetched from Istio controlplane, more importantly, it must be ready before the application. If your application needs to initialize resources by calling envoy, you might ensure envoy is ready before the application. 503 errorWhen the pod is killed it may still in the load balancer pool for a short time then it will still receive some requests, or it’s processing some in-flight requests which reached the pod before the pod is killed. To solve this, we can roughly let the pod wait until it is removed from the LB pool and it finishes all (or most) of the in-flight requests, then it’s safe to shutdown. k8s-deployment.yaml1234567lifecycle: preStop: exec: command: - /bin/sh - '-c' - sleep 15; This means before the application exits, it will wait for 15 seconds, after that Kubernetes will send SIGTERM to the container. During this 15 seconds, it will have time to handle the in-flight requests instead of returning 503. 15 second is only a generic duration which we consider it’s enough for the application to finish all the requests, it could be shorter or longer according to the reality. More advanced case is your application needs to implement some graceful shutdown, for example, recycle the resources, close the MySQL connections or other tasks, this is something implemented in the application code. TestingBefore we deploy the Kubernetes configuration to the production environment, we can test in the testing environment. Use loadtest tool to simulate a bunch of requests to the Kubernetes application, randomly kill the pod during the test. After the new pod is ready, check if the responses contain 500 or 503. Reference The Gotchas of Zero-Downtime Traffic /w Kubernetes Delaying shutdown to wait for pod deletion propagation Recommended Posts(Driven byHexo Recommended Posts plugin)How to set up a reasonable memory limit for Java applications in KubernetesHow to alert for Pod Restart & OOMKilled in KubernetesUse Traffic Control to Simulate Network Chaos in Bare metal & KubernetesMy first Hackathon: bring Spinnaker to my company","link":"/operation/zero-downtime-kubernetes-service-rollout/"},{"title":"Golang performance benchmark and deep into the middle code: a string operation example","text":"ContextRecently, I’m looking for efficient string manipulation in Golang, during the process of that, I used Golang benchmark to test the execution time and checked the Golang middle code to know the underlying mechanism between different solutions. The problemGiven a long string, replace the given character with the other given character in it and return the new string. For example, the string is “aaaaaa”, replace “a” with “b”, after replacement, the return string will be “bbbbbb”. All the following code can be found at Github Gist. Solution 1: string concatenation1234567891011func replaceChar1(str string, ch, replaceCh byte) string { var result string for i := 0; i &lt; len(str); i++ { if str[i] == ch { result = result + string(replaceCh) } else { result = result + string(result[i]) } } return result} Solution 2: Byte array123456789func replaceChar2(str string, ch, replaceCh byte) string { bytes := []byte(str) for i := 0; i &lt; len(str); i++ { if bytes[i] == ch { bytes[i] = replaceCh } } return string(bytes)} Solution 3: String builder12345678910111213func replaceChar3(str string, ch, replaceCh byte) string { var strBuilder strings.Builder strBuilder.Grow(len(str)) for i := 0; i &lt; len(str); i++ { if str[i] == ch { strBuilder.WriteByte(replaceCh) } else { strBuilder.WriteByte(str[i]) } } return strBuilder.String()} Performance ComparisonWith 3 solutions right now, I want to know which one is more efficient, Golang benchmark is a good way of comparing the operation efficiency of these. Add a test file with _test suffix Add functions with Benchmark as prefix like 1234567891011func BenchmarkStringReplace1(b *testing.B) { // generate a 100,000-long string n := 100_000 str := generateString(n) // reset the timer to remove the execution time of generateString(100_000) b.ResetTimer() for i := 0; i &lt; b.N; i++ { replaceChar1(str, 'a', 'b') }} Run the benchmarkThe benchmark function must run the target code b.N times. During benchmark execution, b.N is adjusted until the benchmark function lasts long enough to be timed reliably. 123456$ go test -bench=. string_op_benchmark.go string_op_benchmark_test.gogoos: darwingoarch: arm64BenchmarkStringReplace1-10 3 389948722 ns/opBenchmarkStringReplace2-10 18235 62494 ns/opBenchmarkStringReplace3-10 5534 217069 ns/op The output means that the loop BenchmarkStringReplace1-10 ran 3 times at a speed of 389948722 ns per loop. The BenchmarkStringReplace2 behaved the best and BenchmarkStringReplace1-10 (String concatenation) behaved the worst. One step furtherWhat if I’m still curious about the behind scene of the code? I want to know what replaceChar2 did and the middle code it executed. The following command will generate the middle code of the replaceChar2 function. 123$ GOSSAFUNC=replaceChar2 go build string_op_benchmark.go# command-line-argumentsdumped SSA to ./ssa.html Open the ssa.html, we can see the original code of replaceChar2 and its middle code, like bytes := []byte(str) is mapped to runtime.stringtoslicebyte, return string(bytes) is mapped to runtime.slicebytetostring If we want to make the string operation faster, we might reduce the memory copy or find the other operations we can eliminate. Reference Golang benchmark Golang design and implementation","link":"/programming/golang-performance-benchmark-middle-code/"},{"title":"How to debug Kubernetes OOMKilled when the process is not using memory directly","text":"We investigated the memory increase problem some time ago and learned a lot about JVM metrics. This happened again, we noticed several Java applications deployed in Kubernetes got the memory usage increasing gradually until it reached the memory limit, even after several times of increasing the memory limit, the usage can always hit above 90%, sometimes the container will be OOMKilled. A normal process of investigating Java memoryWe followed the way we did last time to analyze the memory usage, Some figures first: container’s memory limit (12 Gi); container’s memory usage (11 Gi) check the JVM memory usageWe checked the Java process memory usage (3 Gi) and it was way lower than the app container memory usage (11 Gi)The Java process was the main process running in the container, no other processes were consuming memory. native memory trackingWe thought NMT can help us find some native memory leak, so we enabled the native memory tracking and checked different regions, all looked normal. emmm, what do we miss? Start from the beginningWhich memory are we talking aboutKubernetes will kill the container when it runs out of its memory limit, the metrics it uses are container_memory_working_set_bytes &amp; container_memory_rss , the container will be killed if either of them exceeds the memory limit. What’s in itAccording to the metric collector cadvisor, container_memory_rss : The amount of anonymous and swap cache memory (includes transparent hugepages). working_set_bytes: The amount of working set memory, this includes recently accessed memory, dirty memory, and kernel memory. Working set is &lt;= “usage”. Units: Bytes.cadvisor’s code: working_set_bytes = usage_in_bytes - memoryStat.inactive_file cadvisor fetches this data from the cgroup memory stats in each container’s /sys/fs/cgroup/memory folder, the lwn.net explains this data well. 1234567891011121314memory.usage_in_bytes # show current memory(RSS+Cache) usage.memory.memsw.usage_in_bytes # show current memory+Swap usagememory.limit_in_bytes # set/show limit of memory usagememory.memsw.limit_in_bytes # set/show limit of memory+Swap usagememory.failcnt # show the number of memory usage hits limitsmemory.memsw.failcnt # show the number of memory+Swap hits limitsmemory.max_usage_in_bytes # show max memory usage recordedmemory.memsw.usage_in_bytes # show max memory+Swap usage recordedmemory.soft_limit_in_bytes # set/show soft limit of memory usagememory.stat # show various statisticsmemory.use_hierarchy # set/show hierarchical account enabledmemory.force_empty # trigger forced move charge to parentmemory.swappiness # set/show swappiness parameter of vmscan... Based on this, the working_set_bytes contains the page cache and memory_rss, we went to the container and printed the memory stats. 12345678910111213141516bash-4.2$ cat /sys/fs/cgroup/memory/memory.stat cache 8815085056 # of bytes of page cache memory.rss 2360238080 # of bytes of anonymous and swap cache memory.rss_huge 0shmem 0mapped_file 540672dirty 0writeback 2162688swap 0pgpgin 6545913pgpgout 5526026pgfault 1145124816pgmajfault 0inactive_anon 0total_inactive_file 484167680... The page cache (cache) consumed almost 9 Gi memory, after excluding the total_inactive_file (~480Mi), it’s above 8 Gi. Page cache is allocated by the operating system to improve the performance of disk I/O, after some investigation, we found we had a big file written by the app without file rotation, at that moment, it reached 100Gi. We truncated that file and the page cache dropped down to tens of megabytes. A thorough check routineThis is the complete memory layout we have now, based on this, a thorough check routine will be Find a pod with the issue, get the metrics memory_usage_bytes memory_rss_bytes Check if the file cache (memory_usage_bytes - memory_rss_bytes) is high memory_usage_bytes - memory_rss_bytes is the active page cache size, if it’s above hundreds of MBs or several GBs, it means I/O is quite heavy and OS improves it by caching file. Sometimes, it’s reasonable but usually you need to check if it’s what you expect. Check if the rss is equal to memory usageIf so, check the application metrics instead, JVM metrics, Golang metrics etc.Otherwise, things are interesting again … ConclusionNow we know page cache can be an important contributor to the memory increase, therefore we need to monitor the page cache size. In Cadvisor, it’s container_memory_working_set_bytes - container_memory_rss, when the application is I/O intensive, the page cache can be high because the OS tries to improve the I/O efficiency, but for CPU intensive applications, take care of those unnecessary page cache.","link":"/operation/another-kubernetes-oom-kill-troubleshooting/"}],"tags":[{"name":"django","slug":"django","link":"/tags/django/"},{"name":"oauth2","slug":"oauth2","link":"/tags/oauth2/"},{"name":"envoy","slug":"envoy","link":"/tags/envoy/"},{"name":"service mesh","slug":"service-mesh","link":"/tags/service-mesh/"},{"name":"architecture","slug":"architecture","link":"/tags/architecture/"},{"name":"request analysis","slug":"request-analysis","link":"/tags/request-analysis/"},{"name":"microservice","slug":"microservice","link":"/tags/microservice/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"},{"name":"etcd","slug":"etcd","link":"/tags/etcd/"},{"name":"operation","slug":"operation","link":"/tags/operation/"},{"name":"configuration management","slug":"configuration-management","link":"/tags/configuration-management/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"go-micro","slug":"go-micro","link":"/tags/go-micro/"},{"name":"gRPC","slug":"gRPC","link":"/tags/gRPC/"},{"name":"migration","slug":"migration","link":"/tags/migration/"},{"name":"client performance","slug":"client-performance","link":"/tags/client-performance/"},{"name":"sre","slug":"sre","link":"/tags/sre/"},{"name":"cicd","slug":"cicd","link":"/tags/cicd/"},{"name":"spinnaker","slug":"spinnaker","link":"/tags/spinnaker/"},{"name":"docker-compose","slug":"docker-compose","link":"/tags/docker-compose/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"swagger","slug":"swagger","link":"/tags/swagger/"},{"name":"spring boot","slug":"spring-boot","link":"/tags/spring-boot/"},{"name":"prometheus","slug":"prometheus","link":"/tags/prometheus/"},{"name":"monitoring","slug":"monitoring","link":"/tags/monitoring/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"certificate","slug":"certificate","link":"/tags/certificate/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"universal link","slug":"universal-link","link":"/tags/universal-link/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"influxdb","slug":"influxdb","link":"/tags/influxdb/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"istio","slug":"istio","link":"/tags/istio/"},{"name":"leveldb","slug":"leveldb","link":"/tags/leveldb/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"namespace","slug":"namespace","link":"/tags/namespace/"},{"name":"cgroups","slug":"cgroups","link":"/tags/cgroups/"},{"name":"service orchestration","slug":"service-orchestration","link":"/tags/service-orchestration/"},{"name":"mahout","slug":"mahout","link":"/tags/mahout/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"fault tolerance","slug":"fault-tolerance","link":"/tags/fault-tolerance/"},{"name":"service degrade","slug":"service-degrade","link":"/tags/service-degrade/"},{"name":"distributed system","slug":"distributed-system","link":"/tags/distributed-system/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"php","slug":"php","link":"/tags/php/"},{"name":"sidecar","slug":"sidecar","link":"/tags/sidecar/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"data center failover","slug":"data-center-failover","link":"/tags/data-center-failover/"},{"name":"resilience","slug":"resilience","link":"/tags/resilience/"},{"name":"chaos engineering","slug":"chaos-engineering","link":"/tags/chaos-engineering/"},{"name":"data structure","slug":"data-structure","link":"/tags/data-structure/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"raspberry","slug":"raspberry","link":"/tags/raspberry/"},{"name":"ratelimiter","slug":"ratelimiter","link":"/tags/ratelimiter/"},{"name":"app channel analytics","slug":"app-channel-analytics","link":"/tags/app-channel-analytics/"},{"name":"database read write split","slug":"database-read-write-split","link":"/tags/database-read-write-split/"},{"name":"traffic control","slug":"traffic-control","link":"/tags/traffic-control/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"groovy","slug":"groovy","link":"/tags/groovy/"},{"name":"dependency injection","slug":"dependency-injection","link":"/tags/dependency-injection/"},{"name":"wechat","slug":"wechat","link":"/tags/wechat/"},{"name":"grafana","slug":"grafana","link":"/tags/grafana/"},{"name":"automation","slug":"automation","link":"/tags/automation/"},{"name":"alerting","slug":"alerting","link":"/tags/alerting/"},{"name":"xhprof","slug":"xhprof","link":"/tags/xhprof/"},{"name":"rollout","slug":"rollout","link":"/tags/rollout/"}],"categories":[{"name":"programming","slug":"programming","link":"/categories/programming/"},{"name":"operation","slug":"operation","link":"/categories/operation/"},{"name":"architecture","slug":"architecture","link":"/categories/architecture/"},{"name":"life","slug":"life","link":"/categories/life/"},{"name":"microservice","slug":"microservice","link":"/categories/microservice/"}],"pages":[{"title":"About Me","text":"whoami1234567891011121314151617---Songrong: An amatuer cook who lives by programmingFrom: ChinaJob: Site Reliability EngineerSRE: CI/CD Kubernetes Chao EngineeringHobbies: Eating Cooking Watching movies","link":"/about/index.html"}]}